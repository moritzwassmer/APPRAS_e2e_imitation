{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from torch.utils.data import DataLoader, WeightedRandomSampler\n",
    "from torch.utils.data.dataloader import default_collate\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchsummary import summary\n",
    "import torch\n",
    "import pickle\n",
    "\n",
    "\n",
    "# Append the project dir to path\n",
    "sys.path.append(os.path.join(\"..\", \"..\", \"..\"))\n",
    "from data_pipeline.utils import train_test_split, create_metadata_df, get_sample_weights_of_dataset, measurements_to_df\n",
    "from data_pipeline.dataset_xy import CARLADatasetXY\n",
    "from data_pipeline.dataset_xy_opt import CARLADatasetXYOpt\n",
    "from data_pipeline.data_sampler import BranchPerCommandSampler\n",
    "from data_pipeline.data_preprocessing import preprocessing\n",
    "from models.resnet_baseline.architectures_v3 import Resnet_Baseline_V3, Resnet_Baseline_V3_Dropout\n",
    "from models.resnet_baseline.architectures_v5 import Resnet_Baseline_V5\n",
    "from models.resnet_lidar.lidar_v1 import Resnet_Lidar_V1, Resnet_Lidar_V1_Dropout, Resnet_Lidar_V1_Dropout_2\n",
    "from models.model_trainer import ModelTrainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choose training settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set data balancing options (if both false, then no balancing is applied)\n",
    "use_balance_by_loss_weighting = False\n",
    "use_balance_by_over_under_sampling = False\n",
    "\n",
    "assert not use_balance_by_loss_weighting or not use_balance_by_over_under_sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train additionally on the noisy data\n",
    "use_data_noisy = True\n",
    "\n",
    "path_data_noisy = None\n",
    "if use_data_noisy:\n",
    "    path_data_noisy = os.path.join(\"..\", \"..\", \"..\", \"data\", \"Noise-Dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_data = os.path.join(\"..\", \"..\", \"..\", \"data\", \"data\")\n",
    "\n",
    "config_xy = {\"used_inputs\": [\"rgb\", \"lidar_bev\", \"measurements\"], \n",
    "        \"used_measurements\": [\"speed\", \"steer\", \"throttle\", \"brake\", \"command\"],\n",
    "        \"y\": [\"brake\", \"steer\", \"throttle\"],\n",
    "        \"seq_len\": 1\n",
    "        }\n",
    "\n",
    "# config_xy = {\"used_inputs\": [\"rgb\", \"measurements\"], \n",
    "#         \"used_measurements\": [\"speed\", \"waypoints\", \"command\"],\n",
    "#         \"y\": [\"waypoints\"],\n",
    "#         \"seq_len\": 1\n",
    "#         }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'mps' if torch.has_mps else 'cpu')\n",
    "batch_size = 64\n",
    "\n",
    "# Create df_meta \n",
    "df_meta_data = create_metadata_df(path_data, config_xy[\"used_inputs\"])\n",
    "df_meta_data_noisy = None\n",
    "if use_data_noisy:\n",
    "    df_meta_data_noisy = create_metadata_df(path_data_noisy, config_xy[\"used_inputs\"])\n",
    "\n",
    "# Train/test split\n",
    "train_test_config = {\n",
    "    \"train\": ['Town00', 'Town01', 'Town02', 'Town03', 'Town04', 'Town05', 'Town07', 'Town08', 'Town09', 'Town10'],\n",
    "    \"test\": ['Town06']\n",
    "}\n",
    "df_meta_data_train, df_meta_data_test_1, df_meta_data_test_2 = train_test_split(df_meta_data, towns_intersect=train_test_config, df_meta_data_noisy=df_meta_data_noisy)\n",
    "\n",
    "# Decrease train/test size for quick test run\n",
    "# df_meta_data_train = df_meta_data_train.head(5 * batch_size)\n",
    "# df_meta_data_test_1 = df_meta_data_test_1.head(5 * batch_size)\n",
    "# df_meta_data_test_2 = df_meta_data_test_2.head(5 * batch_size)\n",
    "\n",
    "# Create Dataset & DataLoader\n",
    "dataset_train = CARLADatasetXY(root_dir=path_data, df_meta_data=df_meta_data_train, config=config_xy)\n",
    "dataset_test_1 = CARLADatasetXY(root_dir=path_data, df_meta_data=df_meta_data_test_1, config=config_xy)\n",
    "dataset_test_2 = CARLADatasetXY(root_dir=path_data, df_meta_data=df_meta_data_test_2, config=config_xy)\n",
    "\n",
    "# dataset_train = CARLADatasetXYOpt(df_meta_data_train)\n",
    "# dataset_test_1 = CARLADatasetXYOpt(df_meta_data_test_1)\n",
    "# dataset_test_2 = CARLADatasetXYOpt(df_meta_data_test_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate sample weights to be passed to ModelTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_sample_weights(sample_weights):\n",
    "    with open('sample_weights.pickle', 'wb') as handle:\n",
    "        pickle.dump(sample_weights, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "def load_sample_weights():\n",
    "    with open('sample_weights.pickle', 'rb') as handle:\n",
    "        b = pickle.load(handle)\n",
    "    return b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_weights = None\n",
    "if use_balance_by_loss_weighting or use_balance_by_over_under_sampling:\n",
    "    # Dictionary that saves all weights to all y variables \n",
    "    sample_weights = get_sample_weights_of_dataset(dataset_train, num_bins=10, multilabel_option=use_balance_by_over_under_sampling) # TODO: Hacky False to try prob balacing only on steer\n",
    "    # sample_weights = load_sample_weights()\n",
    "    print(sample_weights.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Hacky False to try prob balacing only on steer\n",
    "# sample_weights = {\"multilabel\": sample_weights[\"steer\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Hacky cmd based sample weights\n",
    "#cmd_counts_serd = df_meas_train[\"command\"].value_counts().sort_index()\n",
    "#sample_weights = {\"multilabel\": np.array([1 / cmd_counts_ser.iloc[item-1] for item in df_meas_train[\"command\"].values])}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_meas_train = measurements_to_df(df_meta_data_train)\n",
    "# df_meas_train[\"probs\"] = sample_weights[\"multilabel\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weighted_random_sampler = None\n",
    "shuffle = True\n",
    "if use_balance_by_over_under_sampling:\n",
    "    weighted_random_sampler = WeightedRandomSampler(weights=sample_weights[\"multilabel\"], num_samples=dataset_train.__len__(), replacement=True)\n",
    "    shuffle = False\n",
    "\n",
    "dataloader_train = DataLoader(dataset_train, batch_size=batch_size, num_workers=0, shuffle=shuffle, sampler=weighted_random_sampler)\n",
    "dataloader_test_1 = DataLoader(dataset_test_1, batch_size=batch_size, num_workers=0, shuffle=False, )\n",
    "dataloader_test_2 = DataLoader(dataset_test_2, batch_size=batch_size, num_workers=0, shuffle=False, )\n",
    "\n",
    "# Attempt to directly initialize tensors on device in the DataLoader\n",
    "# collate_fn=lambda x: tuple(x_.to(device) for x_ in default_collate(x))\n",
    "# collate_fn=lambda x: list(map(lambda x: x.to(device), default_collate(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dataloader_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dataloader_test_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x, y, idx in dataloader_test_1:\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create ModelTrainer & run it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = Resnet_Lidar_V1_Dropout(0.25)\n",
    "# model = Resnet_Baseline_V3_Dropout(0.25)\n",
    "# model = Resnet_Baseline_V3()\n",
    "model = Resnet_Lidar_V1_Dropout_2()\n",
    "if not use_balance_by_loss_weighting:\n",
    "    sample_weights = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sum_object = summary(model, [(3, 160, 960), (7, ), (1,)], 64) # (3, 88, 244)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Must be ordered alphabetically (i.e. the same like sample_weights keys)\n",
    "loss_fns_dict = {\"brake\": nn.L1Loss(reduction='none'), \"steer\": nn.L1Loss(reduction='none'), \"throttle\": nn.L1Loss(reduction='none')}\n",
    "loss_fn_weights = {\"brake\": 0.05, \"steer\": 0.45, \"throttle\": 0.5}\n",
    "# loss_fns_dict = {\"waypoints\": nn.L1Loss(reduction='none')}\n",
    "# loss_fn_weights = {\"waypoints\": 1}\n",
    "\n",
    "model_trainer = ModelTrainer(\n",
    "    model=model,\n",
    "    optimizer=optim.Adam(model.parameters(), lr=0.0001),\n",
    "    loss_fns=loss_fns_dict,\n",
    "    loss_fn_weights=loss_fn_weights,\n",
    "    n_epochs=10,\n",
    "    dataloader_train=dataloader_train,\n",
    "    dataloader_test=dataloader_test_2,\n",
    "    sample_weights=sample_weights,\n",
    "    preprocessing=preprocessing,\n",
    "    upload_tensorboard=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_trainer.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_trainer.df_performance_stats[[\"val_brake_loss\",\t\"val_steer_loss\",\t\"val_throttle_loss\"]].mean(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_trainer.df_performance_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_trainer.df_performance_stats.to_csv(\"test.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Investigating model predictions (errors)\n",
    "To be moved in an extra module at some time ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Baseline_V3()\n",
    "model.load_state_dict(torch.load(\"baseline_v3_7_hours.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model_trainer.model.to(torch.device(\"cpu\"))\n",
    "torch.save(model.state_dict(), \"resnet_baseline_v3_4_10_epochs_loss_balanced_noisy_2.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_trainer = ModelTrainer(\n",
    "    model=model,\n",
    "    optimizer=optim.Adam(model.parameters(), lr=0.0001),\n",
    "    loss_fn=nn.L1Loss(),\n",
    "    n_epochs=10,\n",
    "    dataloader_train=dataloader_train,\n",
    "    dataloader_test=dataloader_test,\n",
    "    preprocessing=preprocessing,\n",
    "    upload_tensorboard=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true_list, y_pred_list = model_trainer.get_dataset_predictions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_true = pd.DataFrame(np.transpose(y_true_list), columns=dataset_test.y)\n",
    "df_pred = pd.DataFrame(np.transpose(y_pred_list), columns=dataset_test.y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pred.hist(bins=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_residuals = df_true - df_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_residuals.hist(bins=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment which operators/function can be executed on device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum([torch.tensor([1, 2], device=torch.device(\"mps\")), torch.tensor([1, 2], device=torch.device(\"mps\"))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing could be done on GPU but not on MPS (Apple)\n",
    "preprocessing[\"rgb\"](torch.rand((3, 160, 960), device=torch.device(\"mps\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.tensor([1, 2], device=torch.device(\"mps\")).device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dict = {\"t1\": torch.tensor([1, 2, 3])}\n",
    "# [test_dict[key].to(torch.device(\"mps\")) for key in test_dict]\n",
    "for key in test_dict:\n",
    "    test_dict[key] = test_dict[key].to(torch.device(\"mps\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_values = torch.tensor([0, 1, 2, 3, 4], device=torch.device(\"mps\"))\n",
    "torch_IDX = torch.tensor([0, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_values[torch_IDX]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dirs = os.listdir(\"runs\")\n",
    "dirs_creation_time = [os.path.getctime(os.path.join(\"runs\", dir)) for dir in dirs]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[el[0] for el in sorted(zip(dirs, dirs_creation_time), key=lambda x: x[1])][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.path.getctime('runs/Feb03_15-22-49_MBPvonJulian2.fritz.box')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "bc80d0638afb8ec7c43f4b834002a598fcddbd6e8bf5db40ad8cba47e68e6a97"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
