{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from torch.utils.data import DataLoader, WeightedRandomSampler\n",
    "from torch.utils.data.dataloader import default_collate\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchsummary import summary\n",
    "import torch\n",
    "import pickle\n",
    "\n",
    "\n",
    "# Append the project dir to path\n",
    "from data_pipeline.utils import train_test_split, create_metadata_df, get_sample_weights_of_dataset, measurements_to_df\n",
    "from data_pipeline.dataset_xy import CARLADatasetXY\n",
    "from data_pipeline.dataset_xy_opt import CARLADatasetXYOpt\n",
    "from data_pipeline.data_sampler import BranchPerCommandSampler\n",
    "from data_pipeline.data_preprocessing import preprocessing\n",
    "from models.resnet_baseline.architectures_v3 import Resnet_Baseline_V3, Resnet_Baseline_V3_Dropout\n",
    "from models.resnet_baseline.architectures_v5 import Resnet_Baseline_V5\n",
    "from models.resnet_lidar.lidar_v1 import Resnet_Lidar_V1, Resnet_Lidar_V1_Dropout, Resnet_Lidar_V1_Dropout_2\n",
    "from models.model_trainer import ModelTrainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choose training settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set data balancing options (if both false, then no balancing is applied)\n",
    "use_balance_by_loss_weighting = False\n",
    "use_balance_by_over_under_sampling = False\n",
    "\n",
    "assert not use_balance_by_loss_weighting or not use_balance_by_over_under_sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train additionally on the noisy data\n",
    "use_data_noisy = True\n",
    "\n",
    "path_data_noisy = None\n",
    "if use_data_noisy:\n",
    "    path_data_noisy = os.path.join(\"D:\\\\\", \"data\", \"Noise\")\n",
    "    #D:\\data\\all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_data = os.path.join(\"D:\\\\\", \"data\", \"all\")\n",
    "\n",
    "config_xy = {\"used_inputs\": [\"rgb\", \"measurements\"], \n",
    "        \"used_measurements\": [\"speed\", \"steer\", \"throttle\", \"brake\", \"command\"],\n",
    "        \"y\": [\"brake\", \"steer\", \"throttle\"],\n",
    "        \"seq_len\": 1\n",
    "        }\n",
    "\n",
    "# config_xy = {\"used_inputs\": [\"rgb\", \"measurements\"], \n",
    "#         \"used_measurements\": [\"speed\", \"waypoints\", \"command\"],\n",
    "#         \"y\": [\"waypoints\"],\n",
    "#         \"seq_len\": 1\n",
    "#         }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'mps' if torch.has_mps else 'cpu')\n",
    "batch_size = 64\n",
    "\n",
    "# Create df_meta \n",
    "df_meta_data = create_metadata_df(path_data, config_xy[\"used_inputs\"])\n",
    "df_meta_data_noisy = None\n",
    "if use_data_noisy:\n",
    "    df_meta_data_noisy = create_metadata_df(path_data_noisy, config_xy[\"used_inputs\"])\n",
    "\n",
    "# Train/test split\n",
    "train_test_config = {\n",
    "    \"train\": ['Town00', 'Town01', 'Town02', 'Town03', 'Town04', 'Town05', 'Town07', 'Town08', 'Town09', 'Town10'],\n",
    "    \"test\": ['Town06']\n",
    "}\n",
    "df_meta_data_train, df_meta_data_test_1, df_meta_data_test_2 = train_test_split(df_meta_data, towns_intersect=train_test_config, df_meta_data_noisy=df_meta_data_noisy)\n",
    "\n",
    "# Decrease train/test size for quick test run\n",
    "#df_meta_data_train = df_meta_data_train.head(5 * batch_size)\n",
    "#df_meta_data_test_1 = df_meta_data_test_1.head(5 * batch_size)\n",
    "#df_meta_data_test_2 = df_meta_data_test_2.head(5 * batch_size)\n",
    "\n",
    "# Create Dataset & DataLoader\n",
    "dataset_train = CARLADatasetXY(root_dir=path_data, df_meta_data=df_meta_data_train, config=config_xy)\n",
    "dataset_test_1 = CARLADatasetXY(root_dir=path_data, df_meta_data=df_meta_data_test_1, config=config_xy)\n",
    "dataset_test_2 = CARLADatasetXY(root_dir=path_data, df_meta_data=df_meta_data_test_2, config=config_xy)\n",
    "\n",
    "# dataset_train = CARLADatasetXYOpt(df_meta_data_train)\n",
    "# dataset_test_1 = CARLADatasetXYOpt(df_meta_data_test_1)\n",
    "# dataset_test_2 = CARLADatasetXYOpt(df_meta_data_test_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate sample weights to be passed to ModelTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_sample_weights(sample_weights):\n",
    "    with open('sample_weights.pickle', 'wb') as handle:\n",
    "        pickle.dump(sample_weights, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "def load_sample_weights():\n",
    "    with open('sample_weights.pickle', 'rb') as handle:\n",
    "        b = pickle.load(handle)\n",
    "    return b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_weights = None\n",
    "if use_balance_by_loss_weighting or use_balance_by_over_under_sampling:\n",
    "    # Dictionary that saves all weights to all y variables \n",
    "    sample_weights = get_sample_weights_of_dataset(dataset_train, num_bins=10, multilabel_option=use_balance_by_over_under_sampling) # TODO: Hacky False to try prob balacing only on steer\n",
    "    # sample_weights = load_sample_weights()\n",
    "    print(sample_weights.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Hacky False to try prob balacing only on steer\n",
    "# sample_weights = {\"multilabel\": sample_weights[\"steer\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Hacky cmd based sample weights\n",
    "#cmd_counts_serd = df_meas_train[\"command\"].value_counts().sort_index()\n",
    "#sample_weights = {\"multilabel\": np.array([1 / cmd_counts_ser.iloc[item-1] for item in df_meas_train[\"command\"].values])}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_meas_train = measurements_to_df(df_meta_data_train)\n",
    "# df_meas_train[\"probs\"] = sample_weights[\"multilabel\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "weighted_random_sampler = None\n",
    "shuffle = True\n",
    "if use_balance_by_over_under_sampling:\n",
    "    weighted_random_sampler = WeightedRandomSampler(weights=sample_weights[\"multilabel\"], num_samples=dataset_train.__len__(), replacement=True)\n",
    "    shuffle = False\n",
    "\n",
    "dataloader_train = DataLoader(dataset_train, batch_size=batch_size, num_workers=0, shuffle=shuffle, sampler=weighted_random_sampler)\n",
    "dataloader_test_1 = DataLoader(dataset_test_1, batch_size=batch_size, num_workers=0, shuffle=False, )\n",
    "dataloader_test_2 = DataLoader(dataset_test_2, batch_size=batch_size, num_workers=0, shuffle=False, )\n",
    "\n",
    "# Attempt to directly initialize tensors on device in the DataLoader\n",
    "# collate_fn=lambda x: tuple(x_.to(device) for x_ in default_collate(x))\n",
    "# collate_fn=lambda x: list(map(lambda x: x.to(device), default_collate(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3367"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataloader_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "271"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataloader_test_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x, y, idx in dataloader_test_1:\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create ModelTrainer & run it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = Resnet_Lidar_V1_Dropout(0.25)\n",
    "# model = Resnet_Baseline_V3_Dropout(0.25)\n",
    "# model = Resnet_Baseline_V3()\n",
    "model = Resnet_Baseline_V3_Dropout(0.25)\n",
    "if not use_balance_by_loss_weighting:\n",
    "    sample_weights = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sum_object = summary(model, [(3, 160, 960), (7, ), (1,)], 64) # (3, 88, 244)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model will be trained on: cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Must be ordered alphabetically (i.e. the same like sample_weights keys)\n",
    "loss_fns_dict = {\"brake\": nn.L1Loss(reduction='none'), \"steer\": nn.L1Loss(reduction='none'), \"throttle\": nn.L1Loss(reduction='none')}\n",
    "loss_fn_weights = {\"brake\": 0.05, \"steer\": 0.45, \"throttle\": 0.5}\n",
    "# loss_fns_dict = {\"waypoints\": nn.L1Loss(reduction='none')}\n",
    "# loss_fn_weights = {\"waypoints\": 1}\n",
    "\n",
    "model_trainer = ModelTrainer(\n",
    "    model=model,\n",
    "    optimizer=optim.Adam(model.parameters(), lr=0.0001),\n",
    "    loss_fns=loss_fns_dict,\n",
    "    loss_fn_weights=loss_fn_weights,\n",
    "    n_epochs=20,\n",
    "    dataloader_train=dataloader_train,\n",
    "    dataloader_test=dataloader_test_2,\n",
    "    sample_weights=sample_weights,\n",
    "    preprocessing=preprocessing,\n",
    "    upload_tensorboard=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "\n",
      "Epoch [1/20], Step [0/3367], Loss: 0.0116\n",
      "Epoch [1/20], Step [200/3367], Loss: 0.0211\n",
      "Epoch [1/20], Step [400/3367], Loss: 0.0247\n",
      "Epoch [1/20], Step [600/3367], Loss: 0.0273\n",
      "Epoch [1/20], Step [800/3367], Loss: 0.0236\n",
      "Epoch [1/20], Step [1000/3367], Loss: 0.0243\n",
      "Epoch [1/20], Step [1200/3367], Loss: 0.0418\n",
      "Epoch [1/20], Step [1400/3367], Loss: 0.0138\n",
      "Epoch [1/20], Step [1600/3367], Loss: 0.0127\n",
      "Epoch [1/20], Step [1800/3367], Loss: 0.0088\n",
      "Epoch [1/20], Step [2000/3367], Loss: 0.0145\n",
      "Epoch [1/20], Step [2200/3367], Loss: 0.0221\n",
      "Epoch [1/20], Step [2400/3367], Loss: 0.0216\n",
      "Epoch [1/20], Step [2600/3367], Loss: 0.0209\n",
      "Epoch [1/20], Step [2800/3367], Loss: 0.0186\n",
      "Epoch [1/20], Step [3000/3367], Loss: 0.0257\n",
      "Epoch [1/20], Step [3200/3367], Loss: 0.0341\n",
      "\n",
      "Train Loss Individual: [0.0012 0.0065 0.0152]  Train Loss Total: 0.0229\n",
      "Val Loss Individual: [0.0027 0.029  0.0328]  Val Loss Total: 0.0644\n",
      "\n",
      "Epoch took:  2:38:55\n",
      "Epoch 2\n",
      "\n",
      "Epoch [2/20], Step [0/3367], Loss: 0.0175\n",
      "Epoch [2/20], Step [200/3367], Loss: 0.0293\n",
      "Epoch [2/20], Step [400/3367], Loss: 0.0247\n",
      "Epoch [2/20], Step [600/3367], Loss: 0.0406\n",
      "Epoch [2/20], Step [800/3367], Loss: 0.0131\n",
      "Epoch [2/20], Step [1000/3367], Loss: 0.0113\n",
      "Epoch [2/20], Step [1200/3367], Loss: 0.0119\n",
      "Epoch [2/20], Step [1400/3367], Loss: 0.0260\n",
      "Epoch [2/20], Step [1600/3367], Loss: 0.0192\n",
      "Epoch [2/20], Step [1800/3367], Loss: 0.0158\n",
      "Epoch [2/20], Step [2000/3367], Loss: 0.0169\n",
      "Epoch [2/20], Step [2200/3367], Loss: 0.0179\n",
      "Epoch [2/20], Step [2400/3367], Loss: 0.0164\n",
      "Epoch [2/20], Step [2600/3367], Loss: 0.0205\n",
      "Epoch [2/20], Step [2800/3367], Loss: 0.0241\n",
      "Epoch [2/20], Step [3000/3367], Loss: 0.0148\n",
      "Epoch [2/20], Step [3200/3367], Loss: 0.0225\n",
      "\n",
      "Train Loss Individual: [0.0011 0.0063 0.0146]  Train Loss Total: 0.0221\n",
      "Val Loss Individual: [0.0023 0.0292 0.0294]  Val Loss Total: 0.0609\n",
      "\n",
      "Epoch took:  2:42:50\n",
      "Epoch 3\n",
      "\n",
      "Epoch [3/20], Step [0/3367], Loss: 0.0204\n",
      "Epoch [3/20], Step [200/3367], Loss: 0.0218\n",
      "Epoch [3/20], Step [400/3367], Loss: 0.0211\n",
      "Epoch [3/20], Step [600/3367], Loss: 0.0087\n",
      "Epoch [3/20], Step [800/3367], Loss: 0.0192\n",
      "Epoch [3/20], Step [1000/3367], Loss: 0.0117\n",
      "Epoch [3/20], Step [1200/3367], Loss: 0.0178\n",
      "Epoch [3/20], Step [1400/3367], Loss: 0.0355\n",
      "Epoch [3/20], Step [1600/3367], Loss: 0.0359\n",
      "Epoch [3/20], Step [1800/3367], Loss: 0.0082\n",
      "Epoch [3/20], Step [2000/3367], Loss: 0.0119\n",
      "Epoch [3/20], Step [2200/3367], Loss: 0.0287\n",
      "Epoch [3/20], Step [2400/3367], Loss: 0.0139\n",
      "Epoch [3/20], Step [2600/3367], Loss: 0.0384\n",
      "Epoch [3/20], Step [2800/3367], Loss: 0.0122\n",
      "Epoch [3/20], Step [3000/3367], Loss: 0.0146\n",
      "Epoch [3/20], Step [3200/3367], Loss: 0.0224\n",
      "\n",
      "Train Loss Individual: [0.0011 0.0062 0.0144]  Train Loss Total: 0.0218\n",
      "Val Loss Individual: [0.0028 0.0286 0.0315]  Val Loss Total: 0.0629\n",
      "\n",
      "Epoch took:  2:41:43\n",
      "Epoch 4\n",
      "\n",
      "Epoch [4/20], Step [0/3367], Loss: 0.0244\n",
      "Epoch [4/20], Step [200/3367], Loss: 0.0134\n",
      "Epoch [4/20], Step [400/3367], Loss: 0.0211\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_12840\\941696418.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel_trainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\OneDrive\\UNI\\Master\\WS22\\APP-RAS\\Programming\\models\\model_trainer.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    107\u001b[0m                 \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    108\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 109\u001b[1;33m                 \u001b[0mrunning_loss_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mrunning_loss\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mloss_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mrunning_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss_\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrunning_loss_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    110\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    111\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mbatch_idx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mprint_every\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\OneDrive\\UNI\\Master\\WS22\\APP-RAS\\Programming\\models\\model_trainer.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    107\u001b[0m                 \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    108\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 109\u001b[1;33m                 \u001b[0mrunning_loss_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mrunning_loss\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mloss_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mrunning_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss_\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrunning_loss_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    110\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    111\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mbatch_idx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mprint_every\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model_trainer.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_trainer.df_performance_stats[[\"val_brake_loss\",\"val_steer_loss\",\"val_throttle_loss\"]].mean(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_trainer.df_performance_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_trainer.df_performance_stats.to_csv(\"test.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Investigating model predictions (errors)\n",
    "To be moved in an extra module at some time ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Baseline_V3()\n",
    "model.load_state_dict(torch.load(\"baseline_v3_7_hours.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model_trainer.model.to(torch.device(\"cpu\"))\n",
    "torch.save(model.state_dict(), \"resnet_baseline_v3_4_10_epochs_loss_balanced_noisy_2.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_trainer = ModelTrainer(\n",
    "    model=model,\n",
    "    optimizer=optim.Adam(model.parameters(), lr=0.0001),\n",
    "    loss_fn=nn.L1Loss(),\n",
    "    n_epochs=10,\n",
    "    dataloader_train=dataloader_train,\n",
    "    dataloader_test=dataloader_test,\n",
    "    preprocessing=preprocessing,\n",
    "    upload_tensorboard=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true_list, y_pred_list = model_trainer.get_dataset_predictions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_true = pd.DataFrame(np.transpose(y_true_list), columns=dataset_test.y)\n",
    "df_pred = pd.DataFrame(np.transpose(y_pred_list), columns=dataset_test.y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pred.hist(bins=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_residuals = df_true - df_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_residuals.hist(bins=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment which operators/function can be executed on device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum([torch.tensor([1, 2], device=torch.device(\"mps\")), torch.tensor([1, 2], device=torch.device(\"mps\"))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing could be done on GPU but not on MPS (Apple)\n",
    "preprocessing[\"rgb\"](torch.rand((3, 160, 960), device=torch.device(\"mps\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.tensor([1, 2], device=torch.device(\"mps\")).device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dict = {\"t1\": torch.tensor([1, 2, 3])}\n",
    "# [test_dict[key].to(torch.device(\"mps\")) for key in test_dict]\n",
    "for key in test_dict:\n",
    "    test_dict[key] = test_dict[key].to(torch.device(\"mps\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_values = torch.tensor([0, 1, 2, 3, 4], device=torch.device(\"mps\"))\n",
    "torch_IDX = torch.tensor([0, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_values[torch_IDX]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dirs = os.listdir(\"runs\")\n",
    "dirs_creation_time = [os.path.getctime(os.path.join(\"runs\", dir)) for dir in dirs]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[el[0] for el in sorted(zip(dirs, dirs_creation_time), key=lambda x: x[1])][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.path.getctime('runs/Feb03_15-22-49_MBPvonJulian2.fritz.box')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "bc80d0638afb8ec7c43f4b834002a598fcddbd6e8bf5db40ad8cba47e68e6a97"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
