{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from torch.utils.data import DataLoader, WeightedRandomSampler\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch\n",
    "\n",
    "\n",
    "# Append the project dir to path\n",
    "sys.path.append(os.path.join(\"..\", \"..\"))\n",
    "from data_pipeline.utils import train_test_split, create_metadata_df, get_sample_weights_of_dataset\n",
    "from data_pipeline.dataset_xy import CARLADatasetXY\n",
    "\n",
    "from data_pipeline.data_preprocessing import preprocessing\n",
    "from models.resnet_rgb.architectures_v3 import Resnet_Baseline_V3, Resnet_Baseline_V3_Dropout\n",
    "from models.resnet_lidar.lidar_v1 import Resnet_Lidar_V1, Resnet_Lidar_V1_Dropout, Resnet_Lidar_V1_Dropout_2\n",
    "from models.model_trainer import ModelTrainer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choose settings"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choose data balancing options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If both false, no balancing is applied\n",
    "use_balance_by_loss_weighting = False\n",
    "use_balance_by_over_under_sampling = False\n",
    "\n",
    "assert not use_balance_by_loss_weighting or not use_balance_by_over_under_sampling"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choose if additional noisy data shall be used for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_data_noisy = True\n",
    "\n",
    "\n",
    "path_data_noisy = None\n",
    "if use_data_noisy:\n",
    "    path_data_noisy = os.path.join(\"..\", \"..\", \"data\", \"Noise-Dataset\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choose model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Any model from models/resnet_rgb or models/resnet_lidar can be chosen\n",
    "model = Resnet_Baseline_V3_Dropout(0.25)\n",
    "model = Resnet_Lidar_V1_Dropout_2()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choose training settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose loss functions to use. Must be ordered alphabetically (i.e. the same like sample_weights keys)\n",
    "loss_fns_dict = {\"brake\": nn.L1Loss(reduction='none'), \"steer\": nn.L1Loss(reduction='none'), \"throttle\": nn.L1Loss(reduction='none')}\n",
    "# Choose loss functions weighting factors. Must be ordered alphabetically (i.e. the same like sample_weights keys)\n",
    "loss_fn_weights = {\"brake\": 0.05, \"steer\": 0.45, \"throttle\": 0.5}\n",
    "\n",
    "# Choose optimizer used to minimize the loss function\n",
    "optimizer=optim.Adam(model.parameters(), lr=0.0001)\n",
    "\n",
    "# Choose number epochs\n",
    "n_epochs=20"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_data = os.path.join(\"..\", \"..\", \"data\", \"data\")\n",
    "\n",
    "if str(type(model)).__contains__(\"Lidar\"):\n",
    "        config_xy = {\"used_inputs\": [\"rgb\", \"lidar_bev\", \"measurements\"], \n",
    "                \"used_measurements\": [\"speed\", \"steer\", \"throttle\", \"brake\", \"command\"],\n",
    "                \"y\": [\"brake\", \"steer\", \"throttle\"],\n",
    "                \"seq_len\": 1\n",
    "                }\n",
    "else:\n",
    "        config_xy = {\"used_inputs\": [\"rgb\", \"measurements\"], \n",
    "                \"used_measurements\": [\"speed\", \"steer\", \"throttle\", \"brake\", \"command\"],\n",
    "                \"y\": [\"brake\", \"steer\", \"throttle\"],\n",
    "                \"seq_len\": 1\n",
    "                }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'mps' if torch.has_mps else 'cpu')\n",
    "batch_size = 64\n",
    "\n",
    "# Create df_meta \n",
    "df_meta_data = create_metadata_df(path_data, config_xy[\"used_inputs\"])\n",
    "df_meta_data_noisy = None\n",
    "if use_data_noisy:\n",
    "    df_meta_data_noisy = create_metadata_df(path_data_noisy, config_xy[\"used_inputs\"])\n",
    "\n",
    "# Train/test split\n",
    "train_test_config = {\n",
    "    \"train\": ['Town00', 'Town01', 'Town02', 'Town03', 'Town04', 'Town05', 'Town07', 'Town08', 'Town09', 'Town10'],\n",
    "    \"test\": ['Town06']\n",
    "}\n",
    "df_meta_data_train, df_meta_data_test_1, df_meta_data_test_2 = train_test_split(df_meta_data, towns_intersect=train_test_config, df_meta_data_noisy=df_meta_data_noisy)\n",
    "\n",
    "# Create Dataset & DataLoader\n",
    "dataset_train = CARLADatasetXY(df_meta_data=df_meta_data_train, config=config_xy)\n",
    "dataset_test_1 = CARLADatasetXY(df_meta_data=df_meta_data_test_1, config=config_xy)\n",
    "dataset_test_2 = CARLADatasetXY(df_meta_data=df_meta_data_test_2, config=config_xy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "total = len(dataset_train) + len(dataset_test_1) + len(dataset_test_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1338574766702883"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset_test_2)/total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Town01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Town01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Town01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Town01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Town01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17301</th>\n",
       "      <td>Town04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17302</th>\n",
       "      <td>Town04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17303</th>\n",
       "      <td>Town04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17304</th>\n",
       "      <td>Town04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17305</th>\n",
       "      <td>Town04</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>17306 rows Ã— 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            0\n",
       "0      Town01\n",
       "1      Town01\n",
       "2      Town01\n",
       "3      Town01\n",
       "4      Town01\n",
       "...       ...\n",
       "17301  Town04\n",
       "17302  Town04\n",
       "17303  Town04\n",
       "17304  Town04\n",
       "17305  Town04\n",
       "\n",
       "[17306 rows x 1 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_meta_data_test_1[\"dir\"].str.extract(\"(Town\\d\\d)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate sample weights to be passed to ModelTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_weights = None\n",
    "if use_balance_by_loss_weighting or use_balance_by_over_under_sampling:\n",
    "    # Dictionary that saves all weights to all y variables \n",
    "    sample_weights = get_sample_weights_of_dataset(dataset_train, num_bins=10, multilabel_option=use_balance_by_over_under_sampling) \n",
    "    # sample_weights = load_sample_weights()\n",
    "    print(sample_weights.keys())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "weighted_random_sampler = None\n",
    "shuffle = True\n",
    "if use_balance_by_over_under_sampling:\n",
    "    weighted_random_sampler = WeightedRandomSampler(weights=sample_weights[\"multilabel\"], num_samples=dataset_train.__len__(), replacement=True)\n",
    "    shuffle = False\n",
    "\n",
    "dataloader_train = DataLoader(dataset_train, batch_size=batch_size, num_workers=0, shuffle=shuffle, sampler=weighted_random_sampler)\n",
    "dataloader_test_1 = DataLoader(dataset_test_1, batch_size=batch_size, num_workers=0, shuffle=False, )\n",
    "dataloader_test_2 = DataLoader(dataset_test_2, batch_size=batch_size, num_workers=0, shuffle=False, )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create ModelTrainer & run it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not use_balance_by_loss_weighting:\n",
    "    sample_weights = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model will be trained on: mps\n"
     ]
    }
   ],
   "source": [
    "model_trainer = ModelTrainer(\n",
    "    model=model,\n",
    "    optimizer=optimizer,\n",
    "    loss_fns=loss_fns_dict,\n",
    "    loss_fn_weights=loss_fn_weights,\n",
    "    n_epochs=n_epochs,\n",
    "    dataloader_train=dataloader_train,\n",
    "    dataloader_test=dataloader_test_2,\n",
    "    sample_weights=sample_weights,\n",
    "    preprocessing=preprocessing,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/julianvonklitzing/miniforge3/envs/carla/lib/python3.8/site-packages/torch/autograd/__init__.py:197: UserWarning: The operator 'aten::sgn.out' is not currently supported on the MPS backend and will fall back to run on the CPU. This may have performance implications. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/mps/MPSFallback.mm:11.)\n",
      "  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20], Step [0/3367], Loss: 0.2712\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model_trainer\u001b[39m.\u001b[39;49mrun()\n",
      "File \u001b[0;32m~/Documents/GitHub/end2endappras/models/resnet_baseline/notebooks/../../../models/model_trainer.py:121\u001b[0m, in \u001b[0;36mModelTrainer.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mTRAIN \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    119\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mTRAIN:\n\u001b[1;32m    120\u001b[0m     \u001b[39m# Work through batches\u001b[39;00m\n\u001b[0;32m--> 121\u001b[0m     \u001b[39mfor\u001b[39;00m batch_idx, (X, Y_true, IDX) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataloader_train):\n\u001b[1;32m    122\u001b[0m         start_forward \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[1;32m    123\u001b[0m         \u001b[39m# In this step dicts are transformed to lists with same order\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/carla/lib/python3.8/site-packages/torch/utils/data/dataloader.py:628\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    625\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    626\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    627\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 628\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    629\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    630\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    631\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    632\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/miniforge3/envs/carla/lib/python3.8/site-packages/torch/utils/data/dataloader.py:671\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    669\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    670\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 671\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    672\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[1;32m    673\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/miniforge3/envs/carla/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py:58\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     56\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     57\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 58\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     59\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     60\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/miniforge3/envs/carla/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py:58\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     56\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     57\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 58\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     59\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     60\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/Documents/GitHub/end2endappras/models/resnet_baseline/notebooks/../../../data_pipeline/dataset_xy.py:76\u001b[0m, in \u001b[0;36mCARLADatasetXY.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[39mfor\u001b[39;00m data_point_idx \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(idx_lagged, idx \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m):\n\u001b[1;32m     75\u001b[0m     file_path \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_file_path_from_df(input_idx, data_point_idx)\n\u001b[0;32m---> 76\u001b[0m     data_t \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mload_data_from_path(file_path)\n\u001b[1;32m     77\u001b[0m     data[idx_array] \u001b[39m=\u001b[39m data_t\n\u001b[1;32m     78\u001b[0m     idx_array \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "File \u001b[0;32m~/Documents/GitHub/end2endappras/models/resnet_baseline/notebooks/../../../data_pipeline/dataset_xy.py:130\u001b[0m, in \u001b[0;36mCARLADatasetXY.load_data_from_path\u001b[0;34m(self, path)\u001b[0m\n\u001b[1;32m    128\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    129\u001b[0m \u001b[39mif\u001b[39;00m file_format \u001b[39min\u001b[39;00m [\u001b[39m\"\u001b[39m\u001b[39m.png\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m.jpg\u001b[39m\u001b[39m\"\u001b[39m]:\n\u001b[0;32m--> 130\u001b[0m     data \u001b[39m=\u001b[39m cv2\u001b[39m.\u001b[39;49mimread(path)\n\u001b[1;32m    131\u001b[0m     data \u001b[39m=\u001b[39m cv2\u001b[39m.\u001b[39mcvtColor(data, cv2\u001b[39m.\u001b[39mCOLOR_BGR2RGB)\n\u001b[1;32m    132\u001b[0m     data \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mtranspose(data, (\u001b[39m2\u001b[39m, \u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m))\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model_trainer.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "carla",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "bc80d0638afb8ec7c43f4b834002a598fcddbd6e8bf5db40ad8cba47e68e6a97"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
