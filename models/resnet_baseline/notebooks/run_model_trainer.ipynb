{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/julianvonklitzing/miniforge3/envs/carla/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "import os\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchsummary import summary\n",
    "sys.path.append(\"../../../data_pipeline\")\n",
    "sys.path.append(\"../..\")\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "from utils import train_test_split, create_metadata_df\n",
    "from dataset_xy import CARLADatasetXY\n",
    "from dataset_xy_opt import CARLADatasetXYOpt\n",
    "from data_preprocessing import preprocessing\n",
    "\n",
    "from baseline_v1 import Baseline_V1\n",
    "from baseline_v2 import Baseline_V2\n",
    "\n",
    "from model_trainer import ModelTrainer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Dataset and DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_data = os.path.join(\"..\", \"..\", \"..\", \"data\", \"data\")\n",
    "# path_data = os.path.join(\"..\", \"..\", \"..\", \"data\", \"data_prep\")\n",
    "\n",
    "config_xy = {\"used_inputs\": [\"rgb\", \"measurements\"], \n",
    "        \"used_measurements\": [\"speed\", \"steer\", \"throttle\", \"brake\", \"command\"],\n",
    "        \"y\": [\"brake\", \"steer\", \"throttle\"],\n",
    "        \"seq_len\": 1\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "\n",
    "# Create df_meta \n",
    "df_meta_data = create_metadata_df(path_data, config_xy[\"used_inputs\"])\n",
    "# df_meta_data_train, df_meta_data_test = train_test_split(df_meta_data, towns={\"train\": [\"Town04\"], \"test\": [\"Town05\"]}) # \"Town06\"\n",
    "# Make test set the 15% of the size of the train set\n",
    "# df_meta_data_train = df_meta_data_train.head(5 * batch_size)\n",
    "df_meta_data_train = df_meta_data # test whole set for nan values\n",
    "df_meta_data_test = df_meta_data_test.sample(n=int(df_meta_data_train.shape[0] * 0.15), random_state=3)\n",
    "\n",
    "# Create Dataset & DataLoader\n",
    "dataset_train = CARLADatasetXY(root_dir=path_data, df_meta_data=df_meta_data_train, config=config_xy)\n",
    "dataset_test = CARLADatasetXY(root_dir=path_data, df_meta_data=df_meta_data_test, config=config_xy)\n",
    "# dataset_train = CARLADatasetXYOpt(df_meta_data_train)\n",
    "# dataset_test = CARLADatasetXYOpt(df_meta_data_test)\n",
    "\n",
    "dataloader_train = DataLoader(dataset_train, batch_size=batch_size, num_workers=0, shuffle=True)\n",
    "dataloader_test = DataLoader(dataset_test, batch_size=batch_size, num_workers=0, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataloader_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataloader_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create ModelTrainer & run it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/julianvonklitzing/miniforge3/envs/carla/lib/python3.8/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/Users/julianvonklitzing/miniforge3/envs/carla/lib/python3.8/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "model=Baseline_V1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see fix for multiple inputs: https://github.com/sksq96/pytorch-summary/issues/90\n",
    "# summary(model, input_size=[(3, 160, 960), (7,), (1,)], batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model will be trained on: mps\n"
     ]
    }
   ],
   "source": [
    "model_trainer = ModelTrainer(\n",
    "    model=model,\n",
    "    optimizer=optim.Adam(model.parameters(), lr=0.0001),\n",
    "    loss_fn=nn.L1Loss(),\n",
    "    n_epochs=1,\n",
    "    dataloader_train=dataloader_train,\n",
    "    dataloader_test=dataloader_test,\n",
    "    preprocessing=preprocessing,\n",
    "    upload_tensorboard=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Input tensor should be a float tensor. Got torch.uint8.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model_trainer\u001b[39m.\u001b[39;49mrun()\n",
      "File \u001b[0;32m~/Documents/GitHub/end2endappras/models/resnet_baseline/notebooks/../../model_trainer.py:74\u001b[0m, in \u001b[0;36mModelTrainer.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[39mfor\u001b[39;00m batch_idx, (X_true, y_true) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataloader_train):\n\u001b[1;32m     72\u001b[0m     \u001b[39m# forward\u001b[39;00m\n\u001b[1;32m     73\u001b[0m     start_forward \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[0;32m---> 74\u001b[0m     loss_list \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward_pass(X_true, y_true, writer, epoch)\n\u001b[1;32m     75\u001b[0m     loss \u001b[39m=\u001b[39m \u001b[39msum\u001b[39m(loss_list) \u001b[39m/\u001b[39m \u001b[39m3\u001b[39m\n\u001b[1;32m     76\u001b[0m     times_forward\u001b[39m.\u001b[39mappend(time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m start_forward)\n",
      "File \u001b[0;32m~/Documents/GitHub/end2endappras/models/resnet_baseline/notebooks/../../model_trainer.py:130\u001b[0m, in \u001b[0;36mModelTrainer.forward_pass\u001b[0;34m(self, X_true, Y_true, writer, epoch)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward_pass\u001b[39m(\u001b[39mself\u001b[39m, X_true, Y_true, writer, epoch):\n\u001b[1;32m    128\u001b[0m     \u001b[39m# (optional) do preprocessing (squeezing is currently done in the models forward function)\u001b[39;00m\n\u001b[1;32m    129\u001b[0m     X_true[\u001b[39m\"\u001b[39m\u001b[39mrgb\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39msqueeze(X_true[\u001b[39m\"\u001b[39m\u001b[39mrgb\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[0;32m--> 130\u001b[0m     X_true \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpreprocessing[key](X_true[key])\u001b[39m.\u001b[39mfloat() \u001b[39mfor\u001b[39;00m key \u001b[39min\u001b[39;00m X_true]\n\u001b[1;32m    131\u001b[0m     Y_true \u001b[39m=\u001b[39m [Y_true[key]\u001b[39m.\u001b[39mfloat() \u001b[39mfor\u001b[39;00m key \u001b[39min\u001b[39;00m Y_true]\n\u001b[1;32m    133\u001b[0m     \u001b[39m# move to cuda\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/GitHub/end2endappras/models/resnet_baseline/notebooks/../../model_trainer.py:130\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward_pass\u001b[39m(\u001b[39mself\u001b[39m, X_true, Y_true, writer, epoch):\n\u001b[1;32m    128\u001b[0m     \u001b[39m# (optional) do preprocessing (squeezing is currently done in the models forward function)\u001b[39;00m\n\u001b[1;32m    129\u001b[0m     X_true[\u001b[39m\"\u001b[39m\u001b[39mrgb\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39msqueeze(X_true[\u001b[39m\"\u001b[39m\u001b[39mrgb\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[0;32m--> 130\u001b[0m     X_true \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpreprocessing[key](X_true[key])\u001b[39m.\u001b[39mfloat() \u001b[39mfor\u001b[39;00m key \u001b[39min\u001b[39;00m X_true]\n\u001b[1;32m    131\u001b[0m     Y_true \u001b[39m=\u001b[39m [Y_true[key]\u001b[39m.\u001b[39mfloat() \u001b[39mfor\u001b[39;00m key \u001b[39min\u001b[39;00m Y_true]\n\u001b[1;32m    133\u001b[0m     \u001b[39m# move to cuda\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/GitHub/end2endappras/models/resnet_baseline/notebooks/../../../data_pipeline/data_preprocessing.py:10\u001b[0m, in \u001b[0;36mprep_rgb\u001b[0;34m(X_rgb)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mprep_rgb\u001b[39m(X_rgb):\n\u001b[0;32m---> 10\u001b[0m     \u001b[39mreturn\u001b[39;00m transforms\u001b[39m.\u001b[39;49mCompose([\n\u001b[1;32m     11\u001b[0m     \u001b[39m# transforms.Resize(256),\u001b[39;49;00m\n\u001b[1;32m     12\u001b[0m     \u001b[39m# transforms.CenterCrop(224),\u001b[39;49;00m\n\u001b[1;32m     13\u001b[0m     \u001b[39m# transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\u001b[39;49;00m\n\u001b[1;32m     14\u001b[0m     \u001b[39m# transforms.Normalize(mean=[62.4933, 73.9556, 81.5393], std=[55.3234, 54.6214, 58.7628]),\u001b[39;49;00m\n\u001b[1;32m     15\u001b[0m     transforms\u001b[39m.\u001b[39;49mNormalize(mean\u001b[39m=\u001b[39;49m[\u001b[39m79.6657\u001b[39;49m, \u001b[39m81.5673\u001b[39;49m, \u001b[39m105.6161\u001b[39;49m], std\u001b[39m=\u001b[39;49m[\u001b[39m66.8309\u001b[39;49m, \u001b[39m60.1001\u001b[39;49m, \u001b[39m66.2220\u001b[39;49m]),\n\u001b[1;32m     16\u001b[0m     ])(X_rgb)\n",
      "File \u001b[0;32m~/miniforge3/envs/carla/lib/python3.8/site-packages/torchvision/transforms/transforms.py:95\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, img):\n\u001b[1;32m     94\u001b[0m     \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransforms:\n\u001b[0;32m---> 95\u001b[0m         img \u001b[39m=\u001b[39m t(img)\n\u001b[1;32m     96\u001b[0m     \u001b[39mreturn\u001b[39;00m img\n",
      "File \u001b[0;32m~/miniforge3/envs/carla/lib/python3.8/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniforge3/envs/carla/lib/python3.8/site-packages/torchvision/transforms/transforms.py:270\u001b[0m, in \u001b[0;36mNormalize.forward\u001b[0;34m(self, tensor)\u001b[0m\n\u001b[1;32m    262\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, tensor: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m    263\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[39m        tensor (Tensor): Tensor image to be normalized.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    268\u001b[0m \u001b[39m        Tensor: Normalized Tensor image.\u001b[39;00m\n\u001b[1;32m    269\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 270\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mnormalize(tensor, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmean, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstd, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minplace)\n",
      "File \u001b[0;32m~/miniforge3/envs/carla/lib/python3.8/site-packages/torchvision/transforms/functional.py:360\u001b[0m, in \u001b[0;36mnormalize\u001b[0;34m(tensor, mean, std, inplace)\u001b[0m\n\u001b[1;32m    357\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(tensor, torch\u001b[39m.\u001b[39mTensor):\n\u001b[1;32m    358\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mimg should be Tensor Image. Got \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(tensor)\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 360\u001b[0m \u001b[39mreturn\u001b[39;00m F_t\u001b[39m.\u001b[39;49mnormalize(tensor, mean\u001b[39m=\u001b[39;49mmean, std\u001b[39m=\u001b[39;49mstd, inplace\u001b[39m=\u001b[39;49minplace)\n",
      "File \u001b[0;32m~/miniforge3/envs/carla/lib/python3.8/site-packages/torchvision/transforms/functional_tensor.py:921\u001b[0m, in \u001b[0;36mnormalize\u001b[0;34m(tensor, mean, std, inplace)\u001b[0m\n\u001b[1;32m    918\u001b[0m _assert_image_tensor(tensor)\n\u001b[1;32m    920\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m tensor\u001b[39m.\u001b[39mis_floating_point():\n\u001b[0;32m--> 921\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mInput tensor should be a float tensor. Got \u001b[39m\u001b[39m{\u001b[39;00mtensor\u001b[39m.\u001b[39mdtype\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    923\u001b[0m \u001b[39mif\u001b[39;00m tensor\u001b[39m.\u001b[39mndim \u001b[39m<\u001b[39m \u001b[39m3\u001b[39m:\n\u001b[1;32m    924\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    925\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mExpected tensor to be a tensor image of size (..., C, H, W). Got tensor.size() = \u001b[39m\u001b[39m{\u001b[39;00mtensor\u001b[39m.\u001b[39msize()\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    926\u001b[0m     )\n",
      "\u001b[0;31mTypeError\u001b[0m: Input tensor should be a float tensor. Got torch.uint8."
     ]
    }
   ],
   "source": [
    "model_trainer.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_trainer.df_performance_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "carla",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "bc80d0638afb8ec7c43f4b834002a598fcddbd6e8bf5db40ad8cba47e68e6a97"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
