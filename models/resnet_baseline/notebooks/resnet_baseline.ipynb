{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3c4750c0",
   "metadata": {},
   "source": [
    "# Model ResNet\n",
    "\n",
    "https://www.pluralsight.com/guides/introduction-to-resnet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7e8eff3",
   "metadata": {},
   "source": [
    "#### TODOS\n",
    "1. DONE Debugging, does output make sense?\n",
    "    1. Resize images\n",
    "    2. preprocessing fixes\n",
    "    5. replace scaling by proper function\n",
    "2. try on leaderboard\n",
    "3. Include Odometry and fuse into heads\n",
    "    - Speed\n",
    "    - Location\n",
    "4. navigation\n",
    "5. controller\n",
    "6. Evaluation on Test set, Modularization\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96a254eb",
   "metadata": {},
   "source": [
    "## Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2969a3fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL STUFF\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "#import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import *\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "\n",
    "# GENERAL STUFF\n",
    "import time\n",
    "import copy\n",
    "import os\n",
    "import sys\n",
    "#sys.path.insert(1, 'C:\\\\Users\\\\morit\\\\OneDrive\\\\UNI\\\\Master\\\\WS22\\\\APP-RAS\\\\Programming\\\\data_pipeline') # TODO\n",
    "\n",
    "# DATA ENGINEERING\n",
    "from data_pipeline.dataset_xy import CARLADatasetXY\n",
    "from data_pipeline.dataset import CARLADataset\n",
    "from data_pipeline.utils import train_test_split, create_metadata_df, measurements_to_df, render_example_video_from_folder_name\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ce60a27",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f4066654",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyResnet(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        # ResNet Architecture with pretrained weights, also bigger resnets available\n",
    "        self.net = torchvision.models.resnet18(weights=True)\n",
    "        num_ftrs = self.net.fc.in_features\n",
    "\n",
    "        # Top layer of ResNet which you can modify. We choose Identity to use it as Input for all the heads\n",
    "        self.net.fc = nn.Sequential(\n",
    "            nn.Identity(),\n",
    "            #nn.Dropout(p=1, inplace=False)\n",
    "        )\n",
    "        \n",
    "        # Input Layer fuer cmd, spd\n",
    "        self.cmd_input = nn.Sequential(\n",
    "            nn.Linear(7, 128),\n",
    "            nn.Tanh() #nn.LeakyReLU() # TODO\n",
    "            #nn.Dropout(p=0.5, inplace=False)\n",
    "        )\n",
    "        \n",
    "        self.spd_input = nn.Sequential(\n",
    "            nn.Linear(1, 128),\n",
    "            nn.Tanh(), #nn.LeakyReLU() # TODO\n",
    "            #nn.Dropout(p=0.5, inplace=False)\n",
    "        )\n",
    "        \n",
    "        # MLP\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(num_ftrs+128+128, num_ftrs+128+128),\n",
    "            nn.Tanh(), #nn.LeakyReLU()\n",
    "            #nn.Dropout(p=0.5, inplace=False),\n",
    "            nn.Linear(num_ftrs+128+128, num_ftrs+128+128),\n",
    "            nn.Tanh()#, #nn.LeakyReLU()\n",
    "            #nn.Dropout(p=0.5, inplace=False)\n",
    "        )\n",
    "        \n",
    "        # Regression Heads for Throttle, Brake and Steering\n",
    "        self.thr_head = nn.Sequential(\n",
    "            nn.Linear(num_ftrs+128+128, 1),\n",
    "            nn.Sigmoid() # [0,1] Range Output\n",
    "            \n",
    "        )\n",
    "        \n",
    "        self.brk_head = nn.Sequential(\n",
    "            nn.Linear(num_ftrs+128+128, 1),\n",
    "            nn.Sigmoid() # [0,1] Range Output\n",
    "            \n",
    "        )\n",
    "        \n",
    "        self.str_head = nn.Sequential(\n",
    "            nn.Linear(num_ftrs+128+128, 1),\n",
    "            nn.Tanh() # [-1,1] Range Output\n",
    "            \n",
    "        )\n",
    "\n",
    "    # Forward Pass of the Model\n",
    "    def forward(self, rgb, cmd, spd):\n",
    "        rgb = self.net(rgb) # BRG\n",
    "        #rgb = self.net.fc(rgb)\n",
    "        cmd = self.cmd_input(cmd)\n",
    "        spd = self.spd_input(spd)\n",
    "        \n",
    "        x = torch.cat((rgb, cmd, spd),1)\n",
    "        x = self.mlp(x)\n",
    "        \n",
    "        \n",
    "        #return self.thr_head(x), self.str_head(x), self.brk_head(x) # 3 Outputs since we have 3 Heads\n",
    "        return self.brk_head(x), self.str_head(x), self.thr_head(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beefd313",
   "metadata": {},
   "source": [
    "## Data Loaders, Data Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2a3c0c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#path_ege_data = os.path.join(\"..\", \"..\", \"data\", \"Dataset Ege\")\n",
    "train_path = \"D:\\\\data\\\\Train\"\n",
    "test_path = \"D:\\\\data\\\\Test\"\n",
    "\n",
    "config = {\"used_inputs\": [\"rgb\",\"measurements\"], \n",
    "        \"used_measurements\": [\"speed\", \"steer\", \"throttle\", \"brake\", \"command\"],\n",
    "        \"seq_len\": 1\n",
    "        }\n",
    "\n",
    "train_meta= create_metadata_df(train_path, config[\"used_inputs\"])\n",
    "test_meta = create_metadata_df(test_path, config[\"used_inputs\"])\n",
    "\n",
    "train_dataset = CARLADataset(root_dir=train_path, df_meta_data=train_meta, config=config)\n",
    "test_dataset = CARLADataset(root_dir=test_path, df_meta_data=test_meta, config=config)\n",
    "\n",
    "#weighted_sampler = WeightedSampler(dataset=train_dataset)\n",
    "\n",
    "batch_size = 64\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ccca4e9",
   "metadata": {},
   "source": [
    "### NOISE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "285bb06b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#path_ege_data = os.path.join(\"..\", \"..\", \"data\", \"Dataset Ege\")\n",
    "train_path = \"D:\\\\data\\\\Noise\"\n",
    "test_path = \"D:\\\\data\\\\Test\"\n",
    "\n",
    "config = {\"used_inputs\": [\"rgb\",\"measurements\"], \n",
    "        \"used_measurements\": [\"speed\", \"steer\", \"throttle\", \"brake\", \"command\"],\n",
    "        \"seq_len\": 1\n",
    "        }\n",
    "\n",
    "train_meta= create_metadata_df(train_path, config[\"used_inputs\"])\n",
    "test_meta = create_metadata_df(test_path, config[\"used_inputs\"])\n",
    "\n",
    "train_dataset = CARLADataset(root_dir=train_path,df_meta_data=train_meta, config=config)\n",
    "test_dataset = CARLADataset(root_dir=test_path,df_meta_data=test_meta, config=config)\n",
    "\n",
    "#weighted_sampler = WeightedSampler(dataset=train_dataset)\n",
    "\n",
    "batch_size = 64\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d3485634",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3722"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aa630256",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "382"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ebfb6bf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "augumentations = torch.nn.ModuleList([\n",
    "        transforms.GaussianBlur(9),\n",
    "        transforms.ColorJitter(brightness=1.0, contrast=0.5, saturation=1, hue=0.1),\n",
    "        transforms.RandomErasing()\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f873601b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#mean = torch.tensor([79.6657, 81.5673, 105.6161]) BGR\n",
    "#std = torch.tensor([66.8309, 60.1001, 66.2220])\n",
    "\n",
    "mean = torch.tensor([105.6161, 81.5673, 79.6657]) # RGB\n",
    "std = torch.tensor([66.2220, 60.1001, 66.8309])\n",
    "\n",
    "\n",
    "transform_norm = transforms.Compose([\n",
    "    transforms.Normalize(mean, std),\n",
    "    transforms.Resize([224,224])\n",
    "])\n",
    "\n",
    "transform_augument = transforms.RandomApply(augumentations)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd1c77c4",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f57432c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\morit\\anaconda3\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MyResnet(\n",
       "  (net): ResNet(\n",
       "    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu): ReLU(inplace=True)\n",
       "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "    (layer1): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (layer2): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (layer3): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (layer4): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "    (fc): Sequential(\n",
       "      (0): Identity()\n",
       "    )\n",
       "  )\n",
       "  (cmd_input): Sequential(\n",
       "    (0): Linear(in_features=7, out_features=128, bias=True)\n",
       "    (1): Tanh()\n",
       "  )\n",
       "  (spd_input): Sequential(\n",
       "    (0): Linear(in_features=1, out_features=128, bias=True)\n",
       "    (1): Tanh()\n",
       "  )\n",
       "  (mlp): Sequential(\n",
       "    (0): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (1): Tanh()\n",
       "    (2): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (3): Tanh()\n",
       "  )\n",
       "  (thr_head): Sequential(\n",
       "    (0): Linear(in_features=768, out_features=1, bias=True)\n",
       "    (1): Sigmoid()\n",
       "  )\n",
       "  (brk_head): Sequential(\n",
       "    (0): Linear(in_features=768, out_features=1, bias=True)\n",
       "    (1): Sigmoid()\n",
       "  )\n",
       "  (str_head): Sequential(\n",
       "    (0): Linear(in_features=768, out_features=1, bias=True)\n",
       "    (1): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialise Model (GPU or CPU)\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "net = MyResnet().cuda() if device else net\n",
    "net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "505b9074",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_cuda_if_possible(data):\n",
    "    return data.to(device) if device else data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "263843ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_pass(data, augument = True):\n",
    "    # further preprocessing\n",
    "    X_rgb = transform_norm(torch.squeeze(data[\"rgb\"])).float()\n",
    "    if False:#augument:\n",
    "        X_rgb = transform_augument(X_rgb)\n",
    "    labels = data[\"command\"]\n",
    "    labels = torch.where(labels == -1, torch.tensor(0), labels).to(torch.int64) # Replace by -1 by 0\n",
    "    # Convert the labels to a one hot encoded tensor\n",
    "    one_hot = torch.nn.functional.one_hot(labels, num_classes=7)\n",
    "    X_cmd = torch.squeeze(one_hot).float()\n",
    "    X_spd = ((data[\"speed\"]-speed_mean)/speed_std).float()\n",
    "    \n",
    "    Y_throttle = data[\"throttle\"].float()\n",
    "    Y_steer = data[\"steer\"].float()\n",
    "    Y_brake = data[\"brake\"].float()\n",
    "\n",
    "    # move to GPU\n",
    "    X_rgb = to_cuda_if_possible(X_rgb)\n",
    "    X_cmd = to_cuda_if_possible(X_cmd)\n",
    "    X_spd = to_cuda_if_possible(X_spd)\n",
    "    \n",
    "    Y_throttle = to_cuda_if_possible(Y_throttle)\n",
    "    Y_steer = to_cuda_if_possible(Y_steer)\n",
    "    Y_brake = to_cuda_if_possible(Y_brake)\n",
    "\n",
    "    # compute outputs\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    Y_hat = net(X_rgb, X_cmd, X_spd)\n",
    "    Y_hat_throttle = to_cuda_if_possible(Y_hat[2])\n",
    "    Y_hat_steer = to_cuda_if_possible(Y_hat[1])\n",
    "    Y_hat_brake = to_cuda_if_possible(Y_hat[0])\n",
    "\n",
    "    # get labels from data\n",
    "    Y_throttle = to_cuda_if_possible(data[\"throttle\"].float())\n",
    "    Y_steer = to_cuda_if_possible(data[\"steer\"].float())\n",
    "    Y_brake = to_cuda_if_possible(data[\"brake\"].float())\n",
    "\n",
    "    # Calculate Loss\n",
    "    loss_throttle = 0.5*criterion(Y_hat_throttle, Y_throttle)\n",
    "    loss_steer = 0.45*criterion(Y_hat_steer, Y_steer)\n",
    "    loss_brake = 0.05*criterion(Y_hat_brake, Y_brake)\n",
    "    loss = sum([loss_throttle, loss_steer, loss_brake])\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "992a3786",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss and Optimizer\n",
    "criterion = nn.L1Loss() ##nn.MSELoss() ##  # Easy to interpret #\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.0001) #,weight_decay=1e-5 #optim.SGD(net.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f7f3d66",
   "metadata": {},
   "source": [
    "## Model Trainer Prototype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bd3eb22e",
   "metadata": {},
   "outputs": [],
   "source": [
    "speed_mean = 2.382234##2.250456762830466\n",
    "speed_std = 1.724884##0.30215840254891313"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e39590c3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "\n",
      "Epoch [1/5], Step [0/3722], Loss: 0.2604\n",
      "Epoch [1/5], Step [50/3722], Loss: 0.2041\n",
      "Epoch [1/5], Step [100/3722], Loss: 0.1967\n",
      "Epoch [1/5], Step [150/3722], Loss: 0.1623\n",
      "Epoch [1/5], Step [200/3722], Loss: 0.1738\n",
      "Epoch [1/5], Step [250/3722], Loss: 0.1920\n",
      "Epoch [1/5], Step [300/3722], Loss: 0.1332\n",
      "Epoch [1/5], Step [350/3722], Loss: 0.1651\n",
      "Epoch [1/5], Step [400/3722], Loss: 0.1749\n",
      "Epoch [1/5], Step [450/3722], Loss: 0.1793\n",
      "Epoch [1/5], Step [500/3722], Loss: 0.1505\n",
      "Epoch [1/5], Step [550/3722], Loss: 0.1713\n",
      "Epoch [1/5], Step [600/3722], Loss: 0.1537\n",
      "Epoch [1/5], Step [650/3722], Loss: 0.1796\n",
      "Epoch [1/5], Step [700/3722], Loss: 0.1440\n",
      "Epoch [1/5], Step [750/3722], Loss: 0.1786\n",
      "Epoch [1/5], Step [800/3722], Loss: 0.1747\n",
      "Epoch [1/5], Step [850/3722], Loss: 0.1380\n",
      "Epoch [1/5], Step [900/3722], Loss: 0.1403\n",
      "Epoch [1/5], Step [950/3722], Loss: 0.1558\n",
      "Epoch [1/5], Step [1000/3722], Loss: 0.1217\n",
      "Epoch [1/5], Step [1050/3722], Loss: 0.1187\n",
      "Epoch [1/5], Step [1100/3722], Loss: 0.1318\n",
      "Epoch [1/5], Step [1150/3722], Loss: 0.1322\n",
      "Epoch [1/5], Step [1200/3722], Loss: 0.1461\n",
      "Epoch [1/5], Step [1250/3722], Loss: 0.1480\n",
      "Epoch [1/5], Step [1300/3722], Loss: 0.1154\n",
      "Epoch [1/5], Step [1350/3722], Loss: 0.1741\n",
      "Epoch [1/5], Step [1400/3722], Loss: 0.1593\n",
      "Epoch [1/5], Step [1450/3722], Loss: 0.1431\n",
      "Epoch [1/5], Step [1500/3722], Loss: 0.1685\n",
      "Epoch [1/5], Step [1550/3722], Loss: 0.1366\n",
      "Epoch [1/5], Step [1600/3722], Loss: 0.1330\n",
      "Epoch [1/5], Step [1650/3722], Loss: 0.1168\n",
      "Epoch [1/5], Step [1700/3722], Loss: 0.1535\n",
      "Epoch [1/5], Step [1750/3722], Loss: 0.1133\n",
      "Epoch [1/5], Step [1800/3722], Loss: 0.1145\n",
      "Epoch [1/5], Step [1850/3722], Loss: 0.1305\n",
      "Epoch [1/5], Step [1900/3722], Loss: 0.1381\n",
      "Epoch [1/5], Step [1950/3722], Loss: 0.1285\n",
      "Epoch [1/5], Step [2000/3722], Loss: 0.1178\n",
      "Epoch [1/5], Step [2050/3722], Loss: 0.1167\n",
      "Epoch [1/5], Step [2100/3722], Loss: 0.0929\n",
      "Epoch [1/5], Step [2150/3722], Loss: 0.1347\n",
      "Epoch [1/5], Step [2200/3722], Loss: 0.1117\n",
      "Epoch [1/5], Step [2250/3722], Loss: 0.1037\n",
      "Epoch [1/5], Step [2300/3722], Loss: 0.0939\n",
      "Epoch [1/5], Step [2350/3722], Loss: 0.0974\n",
      "Epoch [1/5], Step [2400/3722], Loss: 0.1212\n",
      "Epoch [1/5], Step [2450/3722], Loss: 0.1021\n",
      "Epoch [1/5], Step [2500/3722], Loss: 0.0855\n",
      "Epoch [1/5], Step [2550/3722], Loss: 0.1128\n",
      "Epoch [1/5], Step [2600/3722], Loss: 0.1038\n",
      "Epoch [1/5], Step [2650/3722], Loss: 0.1328\n",
      "Epoch [1/5], Step [2700/3722], Loss: 0.0758\n",
      "Epoch [1/5], Step [2750/3722], Loss: 0.1400\n",
      "Epoch [1/5], Step [2800/3722], Loss: 0.0987\n",
      "Epoch [1/5], Step [2850/3722], Loss: 0.1002\n",
      "Epoch [1/5], Step [2900/3722], Loss: 0.0946\n",
      "Epoch [1/5], Step [2950/3722], Loss: 0.0944\n",
      "Epoch [1/5], Step [3000/3722], Loss: 0.1043\n",
      "Epoch [1/5], Step [3050/3722], Loss: 0.0709\n",
      "Epoch [1/5], Step [3100/3722], Loss: 0.0687\n",
      "Epoch [1/5], Step [3150/3722], Loss: 0.1098\n",
      "Epoch [1/5], Step [3200/3722], Loss: 0.1116\n",
      "Epoch [1/5], Step [3250/3722], Loss: 0.0949\n",
      "Epoch [1/5], Step [3300/3722], Loss: 0.0788\n",
      "Epoch [1/5], Step [3350/3722], Loss: 0.0832\n",
      "Epoch [1/5], Step [3400/3722], Loss: 0.0902\n",
      "Epoch [1/5], Step [3450/3722], Loss: 0.0952\n",
      "Epoch [1/5], Step [3500/3722], Loss: 0.0805\n",
      "Epoch [1/5], Step [3550/3722], Loss: 0.1392\n",
      "Epoch [1/5], Step [3600/3722], Loss: 0.1052\n",
      "Epoch [1/5], Step [3650/3722], Loss: 0.0864\n",
      "Epoch [1/5], Step [3700/3722], Loss: 0.1023\n",
      "\n",
      "train-loss: 0.1302,\n",
      "Validation [1/5], Step [0/382], Loss: 0.0454\n",
      "Validation [1/5], Step [50/382], Loss: 0.0236\n",
      "Validation [1/5], Step [100/382], Loss: 0.0174\n",
      "Validation [1/5], Step [150/382], Loss: 0.0259\n",
      "Validation [1/5], Step [200/382], Loss: 0.0290\n",
      "Validation [1/5], Step [250/382], Loss: 0.0229\n",
      "Validation [1/5], Step [300/382], Loss: 0.0281\n",
      "Validation [1/5], Step [350/382], Loss: 0.0333\n",
      "validation loss: 0.0365, \n",
      "\n",
      "Improvement-Detected, save-model\n",
      "Epoch 2\n",
      "\n",
      "Epoch [2/5], Step [0/3722], Loss: 0.0951\n",
      "Epoch [2/5], Step [50/3722], Loss: 0.0799\n",
      "Epoch [2/5], Step [100/3722], Loss: 0.0916\n",
      "Epoch [2/5], Step [150/3722], Loss: 0.0629\n",
      "Epoch [2/5], Step [200/3722], Loss: 0.0755\n",
      "Epoch [2/5], Step [250/3722], Loss: 0.0863\n",
      "Epoch [2/5], Step [300/3722], Loss: 0.0866\n",
      "Epoch [2/5], Step [350/3722], Loss: 0.0690\n",
      "Epoch [2/5], Step [400/3722], Loss: 0.0898\n",
      "Epoch [2/5], Step [450/3722], Loss: 0.0620\n",
      "Epoch [2/5], Step [500/3722], Loss: 0.0902\n",
      "Epoch [2/5], Step [550/3722], Loss: 0.0764\n",
      "Epoch [2/5], Step [600/3722], Loss: 0.0926\n",
      "Epoch [2/5], Step [650/3722], Loss: 0.1152\n",
      "Epoch [2/5], Step [700/3722], Loss: 0.0788\n",
      "Epoch [2/5], Step [750/3722], Loss: 0.0773\n",
      "Epoch [2/5], Step [800/3722], Loss: 0.0746\n",
      "Epoch [2/5], Step [850/3722], Loss: 0.0549\n",
      "Epoch [2/5], Step [900/3722], Loss: 0.0877\n",
      "Epoch [2/5], Step [950/3722], Loss: 0.0678\n",
      "Epoch [2/5], Step [1000/3722], Loss: 0.0716\n",
      "Epoch [2/5], Step [1050/3722], Loss: 0.0989\n",
      "Epoch [2/5], Step [1100/3722], Loss: 0.0867\n",
      "Epoch [2/5], Step [1150/3722], Loss: 0.0891\n",
      "Epoch [2/5], Step [1200/3722], Loss: 0.0956\n",
      "Epoch [2/5], Step [1250/3722], Loss: 0.0837\n",
      "Epoch [2/5], Step [1300/3722], Loss: 0.0757\n",
      "Epoch [2/5], Step [1350/3722], Loss: 0.0808\n",
      "Epoch [2/5], Step [1400/3722], Loss: 0.0575\n",
      "Epoch [2/5], Step [1450/3722], Loss: 0.0552\n",
      "Epoch [2/5], Step [1500/3722], Loss: 0.0831\n",
      "Epoch [2/5], Step [1550/3722], Loss: 0.0802\n",
      "Epoch [2/5], Step [1600/3722], Loss: 0.0829\n",
      "Epoch [2/5], Step [1650/3722], Loss: 0.0963\n",
      "Epoch [2/5], Step [1700/3722], Loss: 0.0615\n",
      "Epoch [2/5], Step [1750/3722], Loss: 0.0785\n",
      "Epoch [2/5], Step [1800/3722], Loss: 0.0551\n",
      "Epoch [2/5], Step [1850/3722], Loss: 0.0633\n",
      "Epoch [2/5], Step [1900/3722], Loss: 0.0890\n",
      "Epoch [2/5], Step [1950/3722], Loss: 0.0738\n",
      "Epoch [2/5], Step [2000/3722], Loss: 0.0638\n",
      "Epoch [2/5], Step [2050/3722], Loss: 0.0798\n",
      "Epoch [2/5], Step [2100/3722], Loss: 0.1082\n",
      "Epoch [2/5], Step [2150/3722], Loss: 0.0750\n",
      "Epoch [2/5], Step [2200/3722], Loss: 0.0944\n",
      "Epoch [2/5], Step [2250/3722], Loss: 0.0856\n",
      "Epoch [2/5], Step [2300/3722], Loss: 0.0556\n",
      "Epoch [2/5], Step [2350/3722], Loss: 0.0916\n",
      "Epoch [2/5], Step [2400/3722], Loss: 0.0540\n",
      "Epoch [2/5], Step [2450/3722], Loss: 0.0772\n",
      "Epoch [2/5], Step [2500/3722], Loss: 0.1015\n",
      "Epoch [2/5], Step [2550/3722], Loss: 0.0704\n",
      "Epoch [2/5], Step [2600/3722], Loss: 0.1039\n",
      "Epoch [2/5], Step [2650/3722], Loss: 0.0679\n",
      "Epoch [2/5], Step [2700/3722], Loss: 0.0912\n",
      "Epoch [2/5], Step [2750/3722], Loss: 0.0844\n",
      "Epoch [2/5], Step [2800/3722], Loss: 0.1021\n",
      "Epoch [2/5], Step [2850/3722], Loss: 0.0667\n",
      "Epoch [2/5], Step [2900/3722], Loss: 0.0551\n",
      "Epoch [2/5], Step [2950/3722], Loss: 0.0669\n",
      "Epoch [2/5], Step [3000/3722], Loss: 0.0988\n",
      "Epoch [2/5], Step [3050/3722], Loss: 0.0853\n",
      "Epoch [2/5], Step [3100/3722], Loss: 0.0534\n",
      "Epoch [2/5], Step [3150/3722], Loss: 0.0948\n",
      "Epoch [2/5], Step [3200/3722], Loss: 0.0742\n",
      "Epoch [2/5], Step [3250/3722], Loss: 0.0696\n",
      "Epoch [2/5], Step [3300/3722], Loss: 0.0864\n",
      "Epoch [2/5], Step [3350/3722], Loss: 0.0572\n",
      "Epoch [2/5], Step [3400/3722], Loss: 0.0751\n",
      "Epoch [2/5], Step [3450/3722], Loss: 0.0767\n",
      "Epoch [2/5], Step [3500/3722], Loss: 0.0489\n",
      "Epoch [2/5], Step [3550/3722], Loss: 0.0757\n",
      "Epoch [2/5], Step [3600/3722], Loss: 0.0517\n",
      "Epoch [2/5], Step [3650/3722], Loss: 0.0725\n",
      "Epoch [2/5], Step [3700/3722], Loss: 0.0658\n",
      "\n",
      "train-loss: 0.1047,\n",
      "Validation [2/5], Step [0/382], Loss: 0.0256\n",
      "Validation [2/5], Step [50/382], Loss: 0.0160\n",
      "Validation [2/5], Step [100/382], Loss: 0.0150\n",
      "Validation [2/5], Step [150/382], Loss: 0.0149\n",
      "Validation [2/5], Step [200/382], Loss: 0.0237\n",
      "Validation [2/5], Step [250/382], Loss: 0.0149\n",
      "Validation [2/5], Step [300/382], Loss: 0.0200\n",
      "Validation [2/5], Step [350/382], Loss: 0.0298\n",
      "validation loss: 0.0360, \n",
      "\n",
      "Improvement-Detected, save-model\n",
      "Epoch 3\n",
      "\n",
      "Epoch [3/5], Step [0/3722], Loss: 0.0842\n",
      "Epoch [3/5], Step [50/3722], Loss: 0.0777\n",
      "Epoch [3/5], Step [100/3722], Loss: 0.0757\n",
      "Epoch [3/5], Step [150/3722], Loss: 0.0955\n",
      "Epoch [3/5], Step [200/3722], Loss: 0.0678\n",
      "Epoch [3/5], Step [250/3722], Loss: 0.0603\n",
      "Epoch [3/5], Step [300/3722], Loss: 0.0558\n",
      "Epoch [3/5], Step [350/3722], Loss: 0.0755\n",
      "Epoch [3/5], Step [400/3722], Loss: 0.0783\n",
      "Epoch [3/5], Step [450/3722], Loss: 0.0926\n",
      "Epoch [3/5], Step [500/3722], Loss: 0.0910\n",
      "Epoch [3/5], Step [550/3722], Loss: 0.0486\n",
      "Epoch [3/5], Step [600/3722], Loss: 0.0670\n",
      "Epoch [3/5], Step [650/3722], Loss: 0.0655\n",
      "Epoch [3/5], Step [700/3722], Loss: 0.0578\n",
      "Epoch [3/5], Step [750/3722], Loss: 0.0761\n",
      "Epoch [3/5], Step [800/3722], Loss: 0.0819\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/5], Step [850/3722], Loss: 0.0848\n",
      "Epoch [3/5], Step [900/3722], Loss: 0.0713\n",
      "Epoch [3/5], Step [950/3722], Loss: 0.0734\n",
      "Epoch [3/5], Step [1000/3722], Loss: 0.0680\n",
      "Epoch [3/5], Step [1050/3722], Loss: 0.0644\n",
      "Epoch [3/5], Step [1100/3722], Loss: 0.1220\n",
      "Epoch [3/5], Step [1150/3722], Loss: 0.0753\n",
      "Epoch [3/5], Step [1200/3722], Loss: 0.0751\n",
      "Epoch [3/5], Step [1250/3722], Loss: 0.0669\n",
      "Epoch [3/5], Step [1300/3722], Loss: 0.0851\n",
      "Epoch [3/5], Step [1350/3722], Loss: 0.0807\n",
      "Epoch [3/5], Step [1400/3722], Loss: 0.0828\n",
      "Epoch [3/5], Step [1450/3722], Loss: 0.0965\n",
      "Epoch [3/5], Step [1500/3722], Loss: 0.0668\n",
      "Epoch [3/5], Step [1550/3722], Loss: 0.0820\n",
      "Epoch [3/5], Step [1600/3722], Loss: 0.0858\n",
      "Epoch [3/5], Step [1650/3722], Loss: 0.0869\n",
      "Epoch [3/5], Step [1700/3722], Loss: 0.0635\n",
      "Epoch [3/5], Step [1750/3722], Loss: 0.0696\n",
      "Epoch [3/5], Step [1800/3722], Loss: 0.0624\n",
      "Epoch [3/5], Step [1850/3722], Loss: 0.0825\n",
      "Epoch [3/5], Step [1900/3722], Loss: 0.0727\n",
      "Epoch [3/5], Step [1950/3722], Loss: 0.0806\n",
      "Epoch [3/5], Step [2000/3722], Loss: 0.0709\n",
      "Epoch [3/5], Step [2050/3722], Loss: 0.0625\n",
      "Epoch [3/5], Step [2100/3722], Loss: 0.0550\n",
      "Epoch [3/5], Step [2150/3722], Loss: 0.0513\n",
      "Epoch [3/5], Step [2200/3722], Loss: 0.0684\n",
      "Epoch [3/5], Step [2250/3722], Loss: 0.0836\n",
      "Epoch [3/5], Step [2300/3722], Loss: 0.0628\n",
      "Epoch [3/5], Step [2350/3722], Loss: 0.0763\n",
      "Epoch [3/5], Step [2400/3722], Loss: 0.0655\n",
      "Epoch [3/5], Step [2450/3722], Loss: 0.0979\n",
      "Epoch [3/5], Step [2500/3722], Loss: 0.0813\n",
      "Epoch [3/5], Step [2550/3722], Loss: 0.0414\n",
      "Epoch [3/5], Step [2600/3722], Loss: 0.0734\n",
      "Epoch [3/5], Step [2650/3722], Loss: 0.0825\n",
      "Epoch [3/5], Step [2700/3722], Loss: 0.0380\n",
      "Epoch [3/5], Step [2750/3722], Loss: 0.0614\n",
      "Epoch [3/5], Step [2800/3722], Loss: 0.0937\n",
      "Epoch [3/5], Step [2850/3722], Loss: 0.0891\n",
      "Epoch [3/5], Step [2900/3722], Loss: 0.0530\n",
      "Epoch [3/5], Step [2950/3722], Loss: 0.0865\n",
      "Epoch [3/5], Step [3000/3722], Loss: 0.1039\n",
      "Epoch [3/5], Step [3050/3722], Loss: 0.0566\n",
      "Epoch [3/5], Step [3100/3722], Loss: 0.0452\n",
      "Epoch [3/5], Step [3150/3722], Loss: 0.0563\n",
      "Epoch [3/5], Step [3200/3722], Loss: 0.0717\n",
      "Epoch [3/5], Step [3250/3722], Loss: 0.0942\n",
      "Epoch [3/5], Step [3300/3722], Loss: 0.0753\n",
      "Epoch [3/5], Step [3350/3722], Loss: 0.0750\n",
      "Epoch [3/5], Step [3400/3722], Loss: 0.0593\n",
      "Epoch [3/5], Step [3450/3722], Loss: 0.0620\n",
      "Epoch [3/5], Step [3500/3722], Loss: 0.0505\n",
      "Epoch [3/5], Step [3550/3722], Loss: 0.0712\n",
      "Epoch [3/5], Step [3600/3722], Loss: 0.1023\n",
      "Epoch [3/5], Step [3650/3722], Loss: 0.0717\n",
      "Epoch [3/5], Step [3700/3722], Loss: 0.0836\n",
      "\n",
      "train-loss: 0.0935,\n",
      "Validation [3/5], Step [0/382], Loss: 0.0255\n",
      "Validation [3/5], Step [50/382], Loss: 0.0180\n",
      "Validation [3/5], Step [100/382], Loss: 0.0177\n",
      "Validation [3/5], Step [150/382], Loss: 0.0160\n",
      "Validation [3/5], Step [200/382], Loss: 0.0229\n",
      "Validation [3/5], Step [250/382], Loss: 0.0183\n",
      "Validation [3/5], Step [300/382], Loss: 0.0208\n",
      "Validation [3/5], Step [350/382], Loss: 0.0309\n",
      "validation loss: 0.0341, \n",
      "\n",
      "Improvement-Detected, save-model\n",
      "Epoch 4\n",
      "\n",
      "Epoch [4/5], Step [0/3722], Loss: 0.0798\n",
      "Epoch [4/5], Step [50/3722], Loss: 0.0646\n",
      "Epoch [4/5], Step [100/3722], Loss: 0.0515\n",
      "Epoch [4/5], Step [150/3722], Loss: 0.0650\n",
      "Epoch [4/5], Step [200/3722], Loss: 0.0575\n",
      "Epoch [4/5], Step [250/3722], Loss: 0.0522\n",
      "Epoch [4/5], Step [300/3722], Loss: 0.0719\n",
      "Epoch [4/5], Step [350/3722], Loss: 0.1018\n",
      "Epoch [4/5], Step [400/3722], Loss: 0.0700\n",
      "Epoch [4/5], Step [450/3722], Loss: 0.0615\n",
      "Epoch [4/5], Step [500/3722], Loss: 0.0872\n",
      "Epoch [4/5], Step [550/3722], Loss: 0.0725\n",
      "Epoch [4/5], Step [600/3722], Loss: 0.0549\n",
      "Epoch [4/5], Step [650/3722], Loss: 0.0884\n",
      "Epoch [4/5], Step [700/3722], Loss: 0.0576\n",
      "Epoch [4/5], Step [750/3722], Loss: 0.0336\n",
      "Epoch [4/5], Step [800/3722], Loss: 0.0509\n",
      "Epoch [4/5], Step [850/3722], Loss: 0.0765\n",
      "Epoch [4/5], Step [900/3722], Loss: 0.0443\n",
      "Epoch [4/5], Step [950/3722], Loss: 0.0527\n",
      "Epoch [4/5], Step [1000/3722], Loss: 0.0531\n",
      "Epoch [4/5], Step [1050/3722], Loss: 0.0511\n",
      "Epoch [4/5], Step [1100/3722], Loss: 0.0636\n",
      "Epoch [4/5], Step [1150/3722], Loss: 0.0794\n",
      "Epoch [4/5], Step [1200/3722], Loss: 0.0659\n",
      "Epoch [4/5], Step [1250/3722], Loss: 0.0608\n",
      "Epoch [4/5], Step [1300/3722], Loss: 0.0758\n",
      "Epoch [4/5], Step [1350/3722], Loss: 0.0531\n",
      "Epoch [4/5], Step [1400/3722], Loss: 0.0568\n",
      "Epoch [4/5], Step [1450/3722], Loss: 0.0872\n",
      "Epoch [4/5], Step [1500/3722], Loss: 0.0611\n",
      "Epoch [4/5], Step [1550/3722], Loss: 0.0649\n",
      "Epoch [4/5], Step [1600/3722], Loss: 0.0713\n",
      "Epoch [4/5], Step [1650/3722], Loss: 0.0694\n",
      "Epoch [4/5], Step [1700/3722], Loss: 0.0508\n",
      "Epoch [4/5], Step [1750/3722], Loss: 0.0629\n",
      "Epoch [4/5], Step [1800/3722], Loss: 0.0585\n",
      "Epoch [4/5], Step [1850/3722], Loss: 0.0568\n",
      "Epoch [4/5], Step [1900/3722], Loss: 0.0576\n",
      "Epoch [4/5], Step [1950/3722], Loss: 0.0654\n",
      "Epoch [4/5], Step [2000/3722], Loss: 0.0868\n",
      "Epoch [4/5], Step [2050/3722], Loss: 0.1069\n",
      "Epoch [4/5], Step [2100/3722], Loss: 0.0806\n",
      "Epoch [4/5], Step [2150/3722], Loss: 0.0788\n",
      "Epoch [4/5], Step [2200/3722], Loss: 0.0806\n",
      "Epoch [4/5], Step [2250/3722], Loss: 0.0594\n",
      "Epoch [4/5], Step [2300/3722], Loss: 0.0770\n",
      "Epoch [4/5], Step [2350/3722], Loss: 0.0382\n",
      "Epoch [4/5], Step [2400/3722], Loss: 0.0802\n",
      "Epoch [4/5], Step [2450/3722], Loss: 0.0640\n",
      "Epoch [4/5], Step [2500/3722], Loss: 0.0569\n",
      "Epoch [4/5], Step [2550/3722], Loss: 0.0377\n",
      "Epoch [4/5], Step [2600/3722], Loss: 0.0490\n",
      "Epoch [4/5], Step [2650/3722], Loss: 0.0519\n",
      "Epoch [4/5], Step [2700/3722], Loss: 0.0812\n",
      "Epoch [4/5], Step [2750/3722], Loss: 0.0580\n",
      "Epoch [4/5], Step [2800/3722], Loss: 0.0641\n",
      "Epoch [4/5], Step [2850/3722], Loss: 0.0494\n",
      "Epoch [4/5], Step [2900/3722], Loss: 0.0828\n",
      "Epoch [4/5], Step [2950/3722], Loss: 0.0772\n",
      "Epoch [4/5], Step [3000/3722], Loss: 0.0550\n",
      "Epoch [4/5], Step [3050/3722], Loss: 0.0547\n",
      "Epoch [4/5], Step [3100/3722], Loss: 0.0475\n",
      "Epoch [4/5], Step [3150/3722], Loss: 0.0631\n",
      "Epoch [4/5], Step [3200/3722], Loss: 0.0595\n",
      "Epoch [4/5], Step [3250/3722], Loss: 0.0491\n",
      "Epoch [4/5], Step [3300/3722], Loss: 0.0514\n",
      "Epoch [4/5], Step [3350/3722], Loss: 0.0670\n",
      "Epoch [4/5], Step [3400/3722], Loss: 0.0616\n",
      "Epoch [4/5], Step [3450/3722], Loss: 0.0647\n",
      "Epoch [4/5], Step [3500/3722], Loss: 0.0621\n",
      "Epoch [4/5], Step [3550/3722], Loss: 0.0617\n",
      "Epoch [4/5], Step [3600/3722], Loss: 0.0500\n",
      "Epoch [4/5], Step [3650/3722], Loss: 0.0844\n",
      "Epoch [4/5], Step [3700/3722], Loss: 0.0788\n",
      "\n",
      "train-loss: 0.0861,\n",
      "Validation [4/5], Step [0/382], Loss: 0.0345\n",
      "Validation [4/5], Step [50/382], Loss: 0.0218\n",
      "Validation [4/5], Step [100/382], Loss: 0.0205\n",
      "Validation [4/5], Step [150/382], Loss: 0.0225\n",
      "Validation [4/5], Step [200/382], Loss: 0.0270\n",
      "Validation [4/5], Step [250/382], Loss: 0.0225\n",
      "Validation [4/5], Step [300/382], Loss: 0.0221\n",
      "Validation [4/5], Step [350/382], Loss: 0.0352\n",
      "validation loss: 0.0345, \n",
      "\n",
      "Improvement-Detected, save-model\n",
      "Epoch 5\n",
      "\n",
      "Epoch [5/5], Step [0/3722], Loss: 0.0836\n",
      "Epoch [5/5], Step [50/3722], Loss: 0.0816\n",
      "Epoch [5/5], Step [100/3722], Loss: 0.0555\n",
      "Epoch [5/5], Step [150/3722], Loss: 0.0604\n",
      "Epoch [5/5], Step [200/3722], Loss: 0.0677\n",
      "Epoch [5/5], Step [250/3722], Loss: 0.0448\n",
      "Epoch [5/5], Step [300/3722], Loss: 0.0694\n",
      "Epoch [5/5], Step [350/3722], Loss: 0.0550\n",
      "Epoch [5/5], Step [400/3722], Loss: 0.0634\n",
      "Epoch [5/5], Step [450/3722], Loss: 0.0570\n",
      "Epoch [5/5], Step [500/3722], Loss: 0.0367\n",
      "Epoch [5/5], Step [550/3722], Loss: 0.0585\n",
      "Epoch [5/5], Step [600/3722], Loss: 0.0682\n",
      "Epoch [5/5], Step [650/3722], Loss: 0.0361\n",
      "Epoch [5/5], Step [700/3722], Loss: 0.0458\n",
      "Epoch [5/5], Step [750/3722], Loss: 0.0696\n",
      "Epoch [5/5], Step [800/3722], Loss: 0.0581\n",
      "Epoch [5/5], Step [850/3722], Loss: 0.0506\n",
      "Epoch [5/5], Step [900/3722], Loss: 0.0678\n",
      "Epoch [5/5], Step [950/3722], Loss: 0.0416\n",
      "Epoch [5/5], Step [1000/3722], Loss: 0.0537\n",
      "Epoch [5/5], Step [1050/3722], Loss: 0.0718\n",
      "Epoch [5/5], Step [1100/3722], Loss: 0.0529\n",
      "Epoch [5/5], Step [1150/3722], Loss: 0.0819\n",
      "Epoch [5/5], Step [1200/3722], Loss: 0.0472\n",
      "Epoch [5/5], Step [1250/3722], Loss: 0.0831\n",
      "Epoch [5/5], Step [1300/3722], Loss: 0.0434\n",
      "Epoch [5/5], Step [1350/3722], Loss: 0.0544\n",
      "Epoch [5/5], Step [1400/3722], Loss: 0.0529\n",
      "Epoch [5/5], Step [1450/3722], Loss: 0.0738\n",
      "Epoch [5/5], Step [1500/3722], Loss: 0.0626\n",
      "Epoch [5/5], Step [1550/3722], Loss: 0.0638\n",
      "Epoch [5/5], Step [1600/3722], Loss: 0.0571\n",
      "Epoch [5/5], Step [1650/3722], Loss: 0.0494\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/5], Step [1700/3722], Loss: 0.0479\n",
      "Epoch [5/5], Step [1750/3722], Loss: 0.0768\n",
      "Epoch [5/5], Step [1800/3722], Loss: 0.0339\n",
      "Epoch [5/5], Step [1850/3722], Loss: 0.0620\n",
      "Epoch [5/5], Step [1900/3722], Loss: 0.0515\n",
      "Epoch [5/5], Step [1950/3722], Loss: 0.0610\n",
      "Epoch [5/5], Step [2000/3722], Loss: 0.0840\n",
      "Epoch [5/5], Step [2050/3722], Loss: 0.0472\n",
      "Epoch [5/5], Step [2100/3722], Loss: 0.0410\n",
      "Epoch [5/5], Step [2150/3722], Loss: 0.0625\n",
      "Epoch [5/5], Step [2200/3722], Loss: 0.0609\n",
      "Epoch [5/5], Step [2250/3722], Loss: 0.0421\n",
      "Epoch [5/5], Step [2300/3722], Loss: 0.0568\n",
      "Epoch [5/5], Step [2350/3722], Loss: 0.0651\n",
      "Epoch [5/5], Step [2400/3722], Loss: 0.0615\n",
      "Epoch [5/5], Step [2450/3722], Loss: 0.0554\n",
      "Epoch [5/5], Step [2500/3722], Loss: 0.0527\n",
      "Epoch [5/5], Step [2550/3722], Loss: 0.0426\n",
      "Epoch [5/5], Step [2600/3722], Loss: 0.0566\n",
      "Epoch [5/5], Step [2650/3722], Loss: 0.0486\n",
      "Epoch [5/5], Step [2700/3722], Loss: 0.0610\n",
      "Epoch [5/5], Step [2750/3722], Loss: 0.0607\n",
      "Epoch [5/5], Step [2800/3722], Loss: 0.0637\n",
      "Epoch [5/5], Step [2850/3722], Loss: 0.0529\n",
      "Epoch [5/5], Step [2900/3722], Loss: 0.0818\n",
      "Epoch [5/5], Step [2950/3722], Loss: 0.0495\n",
      "Epoch [5/5], Step [3000/3722], Loss: 0.0678\n",
      "Epoch [5/5], Step [3050/3722], Loss: 0.0945\n",
      "Epoch [5/5], Step [3100/3722], Loss: 0.0487\n",
      "Epoch [5/5], Step [3150/3722], Loss: 0.0661\n",
      "Epoch [5/5], Step [3200/3722], Loss: 0.0407\n",
      "Epoch [5/5], Step [3250/3722], Loss: 0.0834\n",
      "Epoch [5/5], Step [3300/3722], Loss: 0.0747\n",
      "Epoch [5/5], Step [3350/3722], Loss: 0.0280\n",
      "Epoch [5/5], Step [3400/3722], Loss: 0.0665\n",
      "Epoch [5/5], Step [3450/3722], Loss: 0.0529\n",
      "Epoch [5/5], Step [3500/3722], Loss: 0.0712\n",
      "Epoch [5/5], Step [3550/3722], Loss: 0.0407\n",
      "Epoch [5/5], Step [3600/3722], Loss: 0.0789\n",
      "Epoch [5/5], Step [3650/3722], Loss: 0.0448\n",
      "Epoch [5/5], Step [3700/3722], Loss: 0.0416\n",
      "\n",
      "train-loss: 0.0801,\n",
      "Validation [5/5], Step [0/382], Loss: 0.0332\n",
      "Validation [5/5], Step [50/382], Loss: 0.0226\n",
      "Validation [5/5], Step [100/382], Loss: 0.0166\n",
      "Validation [5/5], Step [150/382], Loss: 0.0225\n",
      "Validation [5/5], Step [200/382], Loss: 0.0306\n",
      "Validation [5/5], Step [250/382], Loss: 0.0170\n",
      "Validation [5/5], Step [300/382], Loss: 0.0226\n",
      "Validation [5/5], Step [350/382], Loss: 0.0312\n",
      "validation loss: 0.0351, \n",
      "\n",
      "Improvement-Detected, save-model\n",
      "Wall time: 10h 59min 45s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "n_epochs = 5\n",
    "print_every = 50\n",
    "valid_loss_min = np.Inf\n",
    "val_loss = []\n",
    "train_loss = []\n",
    "total_step = len(train_dataloader)\n",
    "\n",
    "nan_batches = []\n",
    "\n",
    "run = True\n",
    "\n",
    "for epoch in range(1, n_epochs+1):\n",
    "    \n",
    "    running_loss = 0.0\n",
    "    print(f'Epoch {epoch}\\n')\n",
    "    \n",
    "    # Work through batches\n",
    "    for batch_idx, data in enumerate(train_dataloader): #data: (['idx', 'rgb', 'speed', 'steer', 'throttle', 'brake'])\n",
    "\n",
    "        loss = forward_pass(data)\n",
    "        \n",
    "        # Backprop\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        loss_value = loss.item()\n",
    "        \n",
    "        \"\"\"\n",
    "        if np.isnan(loss_value):\n",
    "            print(\"nan\", batch_idx)\n",
    "            nan_batches.append((batch_idx, data))\n",
    "         \"\"\"   \n",
    "        \n",
    "        running_loss += loss_value\n",
    "        if (batch_idx) % print_every == 0:\n",
    "            print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n",
    "                   .format(epoch, n_epochs, batch_idx, total_step, loss_value ))\n",
    "        \"\"\"\n",
    "        et = time.time()\n",
    "        print(et-at)\n",
    "        at = time.time()\n",
    "        \"\"\"\n",
    "        \n",
    "    # Epoch finished, evaluate network and save if network_learned\n",
    "    train_loss.append(running_loss/total_step)\n",
    "    print(f'\\ntrain-loss: {np.mean(train_loss):.4f},') # TODO SOLVE NAN ISSUES\n",
    "    batch_loss = 0\n",
    "\n",
    "    \n",
    "    # Evaluation on Test set, skipped for now\n",
    "  \n",
    "    with torch.no_grad():\n",
    "        net.eval()\n",
    "        \n",
    "        val_total_step = len(test_dataloader)\n",
    "        \n",
    "        for batch_idx, data in enumerate(test_dataloader):\n",
    "            \n",
    "            loss = forward_pass(data, False)\n",
    "            \n",
    "            loss_value = loss.item()\n",
    "            \n",
    "            if (batch_idx) % print_every == 0:\n",
    "                print ('Validation [{}/{}], Step [{}/{}], Loss: {:.4f}'\n",
    "                       .format(epoch, n_epochs, batch_idx, val_total_step, loss_value ))\n",
    "            \n",
    "            batch_loss += loss_value\n",
    "        val_loss.append(batch_loss/len(test_dataloader))\n",
    "        mean_val_loss = np.mean(val_loss)\n",
    "        \n",
    "        print(f'validation loss: {mean_val_loss:.4f}, \\n') # TODO SOLVE NAN ISSUES\n",
    "\n",
    "        network_learned = mean_val_loss < valid_loss_min\n",
    "        if True:#network_learned:\n",
    "            valid_loss_min = mean_val_loss\n",
    "            torch.save(net.state_dict(), 'resnet'+\"_E-\"+str(epoch)+\"_noise\"+'.pth')\n",
    "            #torch.save(net.state_dict(), \"resnet_E-6.pth\")\n",
    "            print('Improvement-Detected, save-model')\n",
    "\n",
    "    # Back to training\n",
    "    net.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a85942e9",
   "metadata": {},
   "source": [
    "### Test predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09ecda41",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
    "iterator = iter(test_dataloader)\n",
    "#print(next(iter(test_dataloader)).keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b96dc55",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = next(iterator)\n",
    "#data\n",
    "\n",
    "X_rgb = transform_norm(torch.squeeze(data[\"rgb\"])).float().to(device)\n",
    "labels = data[\"command\"]\n",
    "labels = torch.where(labels == -1, torch.tensor(0), labels).to(torch.int64) # Replace by -1 by 0\n",
    "# Convert the labels to a one hot encoded tensor\n",
    "one_hot = torch.nn.functional.one_hot(labels, num_classes=7).to(device)\n",
    "X_cmd = torch.squeeze(one_hot).float().to(device)\n",
    "X_spd = ((data[\"speed\"]-speed_mean)/speed_std).float().to(device)\n",
    "#print(np.mean(X_spd.cpu().numpy()))\n",
    "\n",
    "target_ = (data[\"throttle\"], data[\"steer\"], data[\"brake\"])\n",
    "with torch.no_grad():\n",
    "    net.eval()\n",
    "    outputs_ = net(X_rgb, X_cmd, X_spd)\n",
    "    \n",
    "# Durchschnittlicher abs. fehler\n",
    "for i in [0,1,2]:\n",
    "    print(np.mean(abs(outputs_[i].cpu().numpy()-target_[i].cpu().numpy())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51b81953",
   "metadata": {},
   "outputs": [],
   "source": [
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "015a3ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.round(outputs_[0].cpu().numpy()-target_[0].cpu().numpy(),2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8c4f638",
   "metadata": {},
   "source": [
    "Bias Variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "074bc0f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variance \n",
    "\n",
    "for i in [0,1,2]:\n",
    "    outputs = (outputs_[i].cpu().numpy())\n",
    "    #print(outputs)\n",
    "    mean_outputs = np.mean(outputs_[i].cpu().numpy())\n",
    "    #print(mean_outputs)\n",
    "    diff = (outputs-mean_outputs)**2\n",
    "    #print(diff)\n",
    "    value = np.mean(diff)\n",
    "    print(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4da6a585",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bias\n",
    "for i in [0,1,2]:\n",
    "    targets = (target_[i].cpu().numpy())\n",
    "    #print(outputs)\n",
    "    mean_outputs = np.mean(outputs_[i].cpu().numpy())\n",
    "    #print(mean_outputs)\n",
    "    diff = outputs-mean_outputs\n",
    "    #print(diff)\n",
    "    value = np.mean(diff)\n",
    "    print(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d580060",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in [0,1,2]:\n",
    "    print(np.mean(abs(target_[i].cpu().numpy())))\n",
    "    print(np.std(abs(target_[i].cpu().numpy())))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fd372c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "i =0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88953edc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(np.round(outputs_[i].cpu().numpy(),1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd39c1d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.round(target_[i].cpu().numpy(),1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ab4158b",
   "metadata": {},
   "source": [
    "### IMG Processing\n",
    "\n",
    "BGR is now standard FOR carla agent and training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39dbd043",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "idx, batch = next(enumerate(test_dataloader))\n",
    "print(batch[\"rgb\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd55e6d5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "img = batch[\"rgb\"][0]#.shape\n",
    "img = img.numpy().astype(np.uint8).reshape(160,960,3)\n",
    "print(img.shape)\n",
    "\n",
    "#img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB) # NUR HIER, NICHT IN CARLA AGENT\n",
    "print(img.shape)\n",
    "print(type(img))\n",
    "transform = transforms.Compose([transforms.ToPILImage()])\n",
    "\n",
    "tensor = transform(img)\n",
    "\n",
    "#print(type(tensor))\n",
    "\n",
    "tensor.show()\n",
    "\n",
    "#torch.tensor(tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20307478",
   "metadata": {},
   "outputs": [],
   "source": [
    "pil_img = img.astype(np.uint8).reshape(160,960,3)\n",
    "transform = transforms.Compose([transforms.ToPILImage()])\n",
    "print(pil_img.shape)\n",
    "pil_img = transform(pil_img)\n",
    "pil_img.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49b4b228",
   "metadata": {},
   "source": [
    "TEST Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aec50160",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor = transform_norm(torch.squeeze(data[\"rgb\"],1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48a21ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor = torch.squeeze(transform_norm(data[\"rgb\"])).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "683d06c0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(64):\n",
    "    print(np.mean(tensor.numpy()[i], axis = (1,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfcd1c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(tensor.numpy(), axis = (0,2,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acc09d0c",
   "metadata": {},
   "source": [
    "### adding Navigation and speed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c13bbeae",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "iterator = iter(test_dataloader)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56111d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = next(iterator)\n",
    "#data[\"speed\"]\n",
    "#data[\"command\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59fd7382",
   "metadata": {},
   "source": [
    "Command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22295af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Assume labels is a 1D tensor with values from 0 to 6\n",
    "\n",
    "labels = data[\"command\"]\n",
    "labels = torch.where(labels == -1, torch.tensor(0), labels) # Replace by -1 by 0\n",
    "labels = labels.to(torch.int64)\n",
    "\n",
    "# Convert the labels to a one hot encoded tensor\n",
    "one_hot = torch.nn.functional.one_hot(labels, num_classes=7)\n",
    "one_hot = torch.squeeze(one_hot)\n",
    "\n",
    "print(one_hot.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "799272fb",
   "metadata": {},
   "source": [
    "Speed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce212d44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calc mean over trainingsset\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "print(len(train_dataloader))\n",
    "iterator = iter(train_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8c1a946",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "summe = []\n",
    "for batch_idx, data in enumerate(train_dataloader):\n",
    "    #print(data)\n",
    "    if i % 100 == 0:\n",
    "        print(i)\n",
    "    summe.append(np.mean(data[\"speed\"].numpy()))\n",
    "    i += 1\n",
    "    if i >= 1000:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95cd4153",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(summe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de90cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.mean(summe)) # 2.2078979146598274\n",
    "print(np.std(summe)) # 0.22455625005948113\n",
    "speed_mean = np.mean(summe)\n",
    "speed_std = np.std(summe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19f20dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iterator)\n",
    "#print(np.round(batch[\"speed\"].numpy(),2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd855f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "(batch[\"speed\"]-speed_mean)/speed_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca1cce31",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(((batch[\"speed\"]-speed_mean)/speed_std).numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd7c659b",
   "metadata": {},
   "source": [
    "### Class imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd6c193a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calc mean over trainingsset\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "print(len(train_dataloader))\n",
    "iterator = iter(train_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a92caf74",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "steer = []\n",
    "throttle = []\n",
    "brake = []\n",
    "for batch_idx, data in enumerate(train_dataloader):\n",
    "    #print(data)\n",
    "    if i % 100 == 0:\n",
    "        print(i)\n",
    "    steer.append(np.mean(data[\"steer\"].numpy()))\n",
    "    throttle.append(np.mean(data[\"throttle\"].numpy()))\n",
    "    brake.append(np.mean(data[\"brake\"].numpy()))\n",
    "    i += 1\n",
    "    if i >= 100:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0b198de",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.mean(steer))\n",
    "print(np.mean(throttle))\n",
    "print(np.mean(brake))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7177fc92",
   "metadata": {},
   "source": [
    "### Vanishing/Exploding Gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac119fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "for name, param in net.thr_head.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(name, param.data.cpu().numpy())\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68ba6bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, param in net.spd_input.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(name, np.max(abs(param.data.cpu().numpy())))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd0f44f0",
   "metadata": {},
   "source": [
    "## Saving and Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2eecda0",
   "metadata": {},
   "source": [
    "Not suited for leaderboard agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f516f764",
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.save(net, 'rgb_resnet.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86a81a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#net = torch.load('rgb_resnet.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4e10b82",
   "metadata": {},
   "source": [
    "suited for leaderboard agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55596f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(net.state_dict(), \"rgb_resnet.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccc5d256",
   "metadata": {},
   "outputs": [],
   "source": [
    "root = os.path.join(os.getenv(\"GITLAB_ROOT\"),\n",
    "                                           \"models\", \"resnet_baseline\", \"weights\",\n",
    "                                           \"Resnet_Baseline_V3\")  # TODO Has to be defined\n",
    "path = os.path.join(root, \"resnet_E-5.pth\")\n",
    "print(path)\n",
    "net = MyResnet()\n",
    "net.load_state_dict(torch.load(path)) # TODO Change to some model checkpoint\n",
    "net.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8568190",
   "metadata": {},
   "source": [
    "## Testing Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bef6d6ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
    "idx, X = next(enumerate(test_dataloader))\n",
    "img = transform_norm(X[\"rgb\"])\n",
    "img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "339784d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.squeeze(img,1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e33ce4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
    "#print(len(test_dataloader))\n",
    "at = 0\n",
    "for batch_idx, data in enumerate(test_dataloader):\n",
    "    # further preprocessing\n",
    "    X_rgb = transform_norm(torch.squeeze(data[\"rgb\"])).float()\n",
    "    if False:#augument:\n",
    "        X_rgb = transform_augument(X_rgb)\n",
    "    labels = data[\"command\"]\n",
    "    labels = torch.where(labels == -1, torch.tensor(0), labels).to(torch.int64) # Replace by -1 by 0\n",
    "    # Convert the labels to a one hot encoded tensor\n",
    "    one_hot = torch.nn.functional.one_hot(labels, num_classes=7)\n",
    "    X_cmd = torch.squeeze(one_hot).float()\n",
    "    X_spd = ((data[\"speed\"]-speed_mean)/speed_std).float()\n",
    "    \n",
    "    Y_throttle = data[\"throttle\"].float()\n",
    "    Y_steer = data[\"steer\"].float()\n",
    "    Y_brake = data[\"brake\"].float()\n",
    "\n",
    "    # move to GPU\n",
    "    X_rgb = to_cuda_if_possible(X_rgb)\n",
    "    X_cmd = to_cuda_if_possible(X_cmd)\n",
    "    X_spd = to_cuda_if_possible(X_spd)\n",
    "    \n",
    "    Y_throttle = to_cuda_if_possible(Y_throttle)\n",
    "    Y_steer = to_cuda_if_possible(Y_steer)\n",
    "    Y_brake = to_cuda_if_possible(Y_brake)\n",
    "    \n",
    "    et = time.time()\n",
    "    print(et-at)\n",
    "    at = time.time()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c96bcd38",
   "metadata": {},
   "source": [
    "#### Training secounds per batch\n",
    "\n",
    "1.8778209686279297\n",
    "1.7550039291381836\n",
    "1.9759962558746338\n",
    "2.018435001373291"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "613a0284",
   "metadata": {},
   "source": [
    "#### Only dataloader and preprocessing secounds per batch\n",
    "1.235999584197998\n",
    "1.2680015563964844\n",
    "1.3483715057373047\n",
    "1.2585253715515137\n",
    "1.1267704963684082\n",
    "1.124000072479248\n",
    "1.2410008907318115\n",
    "1.2269997596740723\n",
    "1.200000286102295\n",
    "1.267998456954956\n",
    "1.2166194915771484\n",
    "1.2077960968017578\n",
    "1.2405602931976318\n",
    "1.2019383907318115"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "258399f1",
   "metadata": {},
   "source": [
    "# asdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "314eb467",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# LSCHEN\n",
    "root_dir = \"D:\\\\data\\\\data\"\n",
    "keep_input = [\"lidar\", \"rgb\", \"measurements\"] # \"lidar\"\n",
    "\n",
    "def move_unused_sensors_to_new_folder(root_dir, keep_input):\n",
    "    for (root, dirs, files) in os.walk(root_dir, topdown=True):\n",
    "        # Current folder contains the files\n",
    "        if not dirs:\n",
    "            dir, input_type = os.path.split(root)\n",
    "            if input_type not in keep_input:\n",
    "                path_parts = root.split(os.sep)\n",
    "                idx_data_first = path_parts.index(\"data\")\n",
    "                path_parts[idx_data_first + 1] += \" unused\"\n",
    "                dir_new = os.path.join(*path_parts)\n",
    "                if not os.path.exists(dir_new):\n",
    "                    os.makedirs(dir_new)\n",
    "                shutil.move(root, dir_new)\n",
    "                \n",
    "import shutil\n",
    "move_unused_sensors_to_new_folder(root_dir,keep_input)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dec5fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saves preped data in same folder structure under rgb_prep\n",
    "def rgb_to_disk_2(format):\n",
    "    assert format in [\".npy\", \".npz\", \".pt\"]\n",
    "    fn_save = np.save if format == \".npy\" else np.savez_compressed\n",
    "    # save npy/ npz\n",
    "    df_meta = dataset.df_meta_data\n",
    "    for idx in tqdm(range(len(df_meta))):\n",
    "        path_parts = dataset.df_meta_data[\"dir\"][idx].split(os.sep)\n",
    "        # path_parts[path_parts.index(\"data\") + 1] += \"_prep_npy\"\n",
    "        dir_name_zip = os.path.join(*path_parts, \"rgb_prep\")\n",
    "        if not os.path.exists(dir_name_zip):\n",
    "            os.makedirs(dir_name_zip)\n",
    "            # shutil.copytree(os.path.join(dataset.df_meta_data[\"dir\"][idx], \"measurements\"), os.path.join(*path_parts, \"measurements\"))\n",
    "        path = os.path.join(df_meta.iloc[idx][0], \"rgb\", df_meta.iloc[idx][1])\n",
    "        img_np = dataset.load_data_from_path(path)\n",
    "        img_torch = torch.Tensor(img_np)\n",
    "        img_torch_prep = preprocessing[\"rgb\"](img_torch)\n",
    "        img_np_prep = img_torch_prep.numpy()\n",
    "        filename_np = os.path.join(dir_name_zip, f\"{df_meta.iloc[idx]['rgb'].split('.')[0]}{format}\")\n",
    "        # torch.save(img_torch_prep, filename_torch)\n",
    "        with open(filename_np, 'wb') as f:\n",
    "            fn_save(f, img_np_prep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d21bf30d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = \"D:\\\\data\\\\Train\"\n",
    "\n",
    "config = {\"used_inputs\": [\"rgb\",\"measurements\"], \n",
    "        \"used_measurements\": [\"speed\", \"steer\", \"throttle\", \"brake\", \"command\"],\n",
    "        \"seq_len\": 1\n",
    "        }\n",
    "\n",
    "dataset = CARLADataset(root_dir=train_path, config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13506ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "rgb_to_disk_2(\".npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "731e4a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.df_meta_data.loc[0][\"dir\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "213b0020",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "root = os.path.join(\"C:\\\\\", \"Users\", \"morit\", \"OneDrive\", \"UNI\", \"Master\", \"WS22\", \"APP-RAS\", \"Programming\",\n",
    "                    \"models\", \"resnet_baseline\")\n",
    "\n",
    "path = os.path.join(root,\"resnet_baseline\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2c7646c",
   "metadata": {},
   "outputs": [],
   "source": [
    "path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "330ea1c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.insert(1, 'C:\\\\Users\\\\morit\\\\OneDrive\\\\UNI\\\\Master\\\\WS22\\\\APP-RAS\\\\Programming\\\\models')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3db331af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "#sys.path.insert(1, 'C:\\\\Users\\\\morit\\\\OneDrive\\\\UNI\\\\Master\\\\WS22\\\\APP-RAS\\\\Programming\\\\data_pipeline')\n",
    "#sys.path.append('C:\\\\Users\\\\morit\\\\OneDrive\\\\UNI\\\\Master\\\\WS22\\\\APP-RAS\\\\Programming\\\\models')\n",
    "sys.path.append('C:\\\\Users\\\\morit\\\\OneDrive\\\\UNI\\\\Master\\\\WS22\\\\APP-RAS\\\\Programming\\\\models\\\\resnet_baseline\\\\V3')\n",
    "#sys.path.insert(3, 'C:\\\\Users\\\\morit\\\\OneDrive\\\\UNI\\\\Master\\\\WS22\\\\APP-RAS\\\\Programming\\\\data_preprocessing')\n",
    "from architectures import No_Reg, load_weights\n",
    "\n",
    "#from architectures import No_Reg\n",
    "net = No_Reg()\n",
    "net = load_weights(net, \"resnet_E-5.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bacc899",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append('C:\\\\Users\\\\morit\\\\OneDrive\\\\UNI\\\\Master\\\\WS22\\\\APP-RAS\\\\Programming')\n",
    "import models.resnet_baseline.test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a5cd16a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "sys.path.append('C:\\\\Users\\\\morit\\\\OneDrive\\\\UNI\\\\Master\\\\WS22\\\\APP-RAS\\\\Programming\\\\models')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "481156ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import models\n",
    "\n",
    "#from architectures import No_Reg\n",
    "net = No_Reg()\n",
    "net = load_weights(net, \"resnet_E-5.pth\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
