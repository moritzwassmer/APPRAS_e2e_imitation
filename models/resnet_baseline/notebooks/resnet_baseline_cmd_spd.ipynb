{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3c4750c0",
   "metadata": {},
   "source": [
    "# Model ResNet\n",
    "\n",
    "https://www.pluralsight.com/guides/introduction-to-resnet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7e8eff3",
   "metadata": {},
   "source": [
    "#### TODOS\n",
    "1. DONE Debugging, does output make sense?\n",
    "    1. Resize images\n",
    "    2. preprocessing fixes\n",
    "    5. replace scaling by proper function\n",
    "2. try on leaderboard\n",
    "3. Include Odometry and fuse into heads\n",
    "    - Speed\n",
    "    - Location\n",
    "4. navigation\n",
    "5. controller\n",
    "6. Evaluation on Test set, Modularization\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96a254eb",
   "metadata": {},
   "source": [
    "## Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2969a3fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL STUFF\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "#import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import *\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "\n",
    "# GENERAL STUFF\n",
    "import time\n",
    "import copy\n",
    "import os\n",
    "import sys\n",
    "sys.path.insert(1, 'C:\\\\Users\\\\morit\\\\OneDrive\\\\UNI\\\\Master\\\\WS22\\\\APP-RAS\\\\Programming\\\\data_pipeline') # TODO\n",
    "\n",
    "# DATA ENGINEERING\n",
    "from data_sampler import WeightedSampler\n",
    "from dataset import CARLADataset#, CARLADatasetMultiProcessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ce60a27",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f4066654",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyResnet(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        # ResNet Architecture with pretrained weights, also bigger resnets available\n",
    "        self.net = torchvision.models.resnet18(weights=True)\n",
    "        num_ftrs = self.net.fc.in_features\n",
    "\n",
    "        # Top layer of ResNet which you can modify. We choose Identity to use it as Input for all the heads\n",
    "        self.net.fc = nn.Identity()\n",
    "        \n",
    "        # Input Layer fuer cmd, spd\n",
    "        self.cmd_input = nn.Sequential(\n",
    "            nn.Linear(7, 128),\n",
    "            nn.Tanh(), #nn.LeakyReLU() # TODO\n",
    "            #nn.Dropout(p=0.5, inplace=False)\n",
    "        )\n",
    "        \n",
    "        self.spd_input = nn.Sequential(\n",
    "            nn.Linear(1, 128),\n",
    "            nn.Tanh(), #nn.LeakyReLU() # TODO\n",
    "            #nn.Dropout(p=0.5, inplace=False)\n",
    "        )\n",
    "        \n",
    "        # MLP\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(num_ftrs+128+128, num_ftrs+128+128),\n",
    "            nn.Tanh(), #nn.LeakyReLU()\n",
    "            #nn.Dropout(p=0.5, inplace=False),\n",
    "            nn.Linear(num_ftrs+128+128, num_ftrs+128+128),\n",
    "            nn.Tanh()#, #nn.LeakyReLU()\n",
    "            #nn.Dropout(p=0.5, inplace=False)\n",
    "        )\n",
    "        \n",
    "        # Regression Heads for Throttle, Brake and Steering\n",
    "        self.thr_head = nn.Sequential(\n",
    "            nn.Linear(num_ftrs+128+128, 1),\n",
    "            nn.Sigmoid() # [0,1] Range Output\n",
    "            \n",
    "        )\n",
    "        \n",
    "        self.brk_head = nn.Sequential(\n",
    "            nn.Linear(num_ftrs+128+128, 1),\n",
    "            nn.Sigmoid() # [0,1] Range Output\n",
    "            \n",
    "        )\n",
    "        \n",
    "        self.str_head = nn.Sequential(\n",
    "            nn.Linear(num_ftrs+128+128, 1),\n",
    "            nn.Tanh() # [-1,1] Range Output\n",
    "            \n",
    "        )\n",
    "\n",
    "    # Forward Pass of the Model\n",
    "    def forward(self, rgb, cmd, spd):\n",
    "        rgb = self.net(rgb) # BRG\n",
    "        cmd = self.cmd_input(cmd)\n",
    "        spd = self.spd_input(spd)\n",
    "        \n",
    "        x = torch.cat((rgb, cmd, spd),1)\n",
    "        x = self.mlp(x)\n",
    "        \n",
    "        #x = self.net.fc(x)\n",
    "        return self.thr_head(x), self.str_head(x), self.brk_head(x) # 3 Outputs since we have 3 Heads\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beefd313",
   "metadata": {},
   "source": [
    "## Data Loaders, Data Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2a3c0c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#path_ege_data = os.path.join(\"..\", \"..\", \"data\", \"Dataset Ege\")\n",
    "train_path = \"D:\\\\data\\\\Train\"\n",
    "test_path = \"D:\\\\data\\\\Test\"\n",
    "\n",
    "config = {\"used_inputs\": [\"rgb\",\"measurements\"], \n",
    "        \"used_measurements\": [\"speed\", \"steer\", \"throttle\", \"brake\", \"command\"],\n",
    "        \"seq_len\": 1\n",
    "        }\n",
    "\n",
    "train_dataset = CARLADataset(root_dir=train_path, config=config)\n",
    "test_dataset = CARLADataset(root_dir=test_path, config=config)\n",
    "\n",
    "weighted_sampler = WeightedSampler(dataset=train_dataset)\n",
    "\n",
    "batch_size = 64\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d3485634",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3664"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aa630256",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "382"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ebfb6bf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "augumentations = torch.nn.ModuleList([\n",
    "        transforms.GaussianBlur(9),\n",
    "        transforms.ColorJitter(brightness=1.0, contrast=0.5, saturation=1, hue=0.1),\n",
    "        transforms.RandomErasing()\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f873601b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#mean = torch.tensor([79.6657, 81.5673, 105.6161]) BGR\n",
    "#std = torch.tensor([66.8309, 60.1001, 66.2220])\n",
    "\n",
    "mean = torch.tensor([105.6161, 81.5673, 79.6657]) # RGB\n",
    "std = torch.tensor([66.2220, 60.1001, 66.8309])\n",
    "\n",
    "\n",
    "transform_norm = transforms.Compose([\n",
    "    transforms.Normalize(mean, std),\n",
    "    transforms.Resize([224,224])\n",
    "])\n",
    "\n",
    "transform_augument = transforms.RandomApply(augumentations)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd1c77c4",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f57432c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\morit\\anaconda3\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MyResnet(\n",
       "  (net): ResNet(\n",
       "    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu): ReLU(inplace=True)\n",
       "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "    (layer1): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (layer2): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (layer3): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (layer4): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "    (fc): Identity()\n",
       "  )\n",
       "  (cmd_input): Sequential(\n",
       "    (0): Linear(in_features=7, out_features=128, bias=True)\n",
       "    (1): Tanh()\n",
       "  )\n",
       "  (spd_input): Sequential(\n",
       "    (0): Linear(in_features=1, out_features=128, bias=True)\n",
       "    (1): Tanh()\n",
       "  )\n",
       "  (mlp): Sequential(\n",
       "    (0): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (1): Tanh()\n",
       "    (2): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (3): Tanh()\n",
       "  )\n",
       "  (thr_head): Sequential(\n",
       "    (0): Linear(in_features=768, out_features=1, bias=True)\n",
       "    (1): Sigmoid()\n",
       "  )\n",
       "  (brk_head): Sequential(\n",
       "    (0): Linear(in_features=768, out_features=1, bias=True)\n",
       "    (1): Sigmoid()\n",
       "  )\n",
       "  (str_head): Sequential(\n",
       "    (0): Linear(in_features=768, out_features=1, bias=True)\n",
       "    (1): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialise Model (GPU or CPU)\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "net = MyResnet().cuda() if device else net\n",
    "net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "505b9074",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_cuda_if_possible(data):\n",
    "    return data.to(device) if device else data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "263843ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_pass(data, augument = True):\n",
    "    # further preprocessing\n",
    "    X_rgb = transform_norm(torch.squeeze(data[\"rgb\"])).float()\n",
    "    if False:#augument:\n",
    "        X_rgb = transform_augument(X_rgb)\n",
    "    labels = data[\"command\"]\n",
    "    labels = torch.where(labels == -1, torch.tensor(0), labels).to(torch.int64) # Replace by -1 by 0\n",
    "    # Convert the labels to a one hot encoded tensor\n",
    "    one_hot = torch.nn.functional.one_hot(labels, num_classes=7)\n",
    "    X_cmd = torch.squeeze(one_hot).float()\n",
    "    X_spd = ((data[\"speed\"]-speed_mean)/speed_std).float()\n",
    "    \n",
    "    Y_throttle = data[\"throttle\"].float()\n",
    "    Y_steer = data[\"steer\"].float()\n",
    "    Y_brake = data[\"brake\"].float()\n",
    "\n",
    "    # move to GPU\n",
    "    X_rgb = to_cuda_if_possible(X_rgb)\n",
    "    X_cmd = to_cuda_if_possible(X_cmd)\n",
    "    X_spd = to_cuda_if_possible(X_spd)\n",
    "    \n",
    "    Y_throttle = to_cuda_if_possible(Y_throttle)\n",
    "    Y_steer = to_cuda_if_possible(Y_steer)\n",
    "    Y_brake = to_cuda_if_possible(Y_brake)\n",
    "\n",
    "    # compute outputs\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    Y_hat = net(X_rgb, X_cmd, X_spd)\n",
    "    Y_hat_throttle = to_cuda_if_possible(Y_hat[0])\n",
    "    Y_hat_steer = to_cuda_if_possible(Y_hat[1])\n",
    "    Y_hat_brake = to_cuda_if_possible(Y_hat[2])\n",
    "\n",
    "    # get labels from data\n",
    "    Y_throttle = to_cuda_if_possible(data[\"throttle\"].float())\n",
    "    Y_steer = to_cuda_if_possible(data[\"steer\"].float())\n",
    "    Y_brake = to_cuda_if_possible(data[\"brake\"].float())\n",
    "\n",
    "    # Calculate Loss\n",
    "    loss_throttle = 0.5*criterion(Y_hat_throttle, Y_throttle)\n",
    "    loss_steer = 0.45*criterion(Y_hat_steer, Y_steer)\n",
    "    loss_brake = 0.05*criterion(Y_hat_brake, Y_brake)\n",
    "    loss = sum([loss_throttle, loss_steer, loss_brake])\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "992a3786",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss and Optimizer\n",
    "criterion = nn.L1Loss() ##nn.MSELoss() ##  # Easy to interpret #\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.0001) #,weight_decay=1e-5 #optim.SGD(net.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f7f3d66",
   "metadata": {},
   "source": [
    "## Model Trainer Prototype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bd3eb22e",
   "metadata": {},
   "outputs": [],
   "source": [
    "speed_mean = 2.382234##2.250456762830466\n",
    "speed_std = 1.724884##0.30215840254891313"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e39590c3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "\n",
      "Epoch [1/1], Step [0/3664], Loss: 0.0900\n",
      "Epoch [1/1], Step [50/3664], Loss: 0.0687\n",
      "Epoch [1/1], Step [100/3664], Loss: 0.0485\n",
      "Epoch [1/1], Step [150/3664], Loss: 0.0662\n",
      "Epoch [1/1], Step [200/3664], Loss: 0.0596\n",
      "Epoch [1/1], Step [250/3664], Loss: 0.0572\n",
      "Epoch [1/1], Step [300/3664], Loss: 0.1003\n",
      "Epoch [1/1], Step [350/3664], Loss: 0.0734\n",
      "Epoch [1/1], Step [400/3664], Loss: 0.0750\n",
      "Epoch [1/1], Step [450/3664], Loss: 0.0396\n",
      "Epoch [1/1], Step [500/3664], Loss: 0.0625\n",
      "Epoch [1/1], Step [550/3664], Loss: 0.0844\n",
      "Epoch [1/1], Step [600/3664], Loss: 0.0651\n",
      "Epoch [1/1], Step [650/3664], Loss: 0.0770\n",
      "Epoch [1/1], Step [700/3664], Loss: 0.0603\n",
      "Epoch [1/1], Step [750/3664], Loss: 0.0451\n",
      "Epoch [1/1], Step [800/3664], Loss: 0.0518\n",
      "Epoch [1/1], Step [850/3664], Loss: 0.0729\n",
      "Epoch [1/1], Step [900/3664], Loss: 0.0725\n",
      "Epoch [1/1], Step [950/3664], Loss: 0.0607\n",
      "Epoch [1/1], Step [1000/3664], Loss: 0.0676\n",
      "Epoch [1/1], Step [1050/3664], Loss: 0.0537\n",
      "Epoch [1/1], Step [1100/3664], Loss: 0.0609\n",
      "Epoch [1/1], Step [1150/3664], Loss: 0.0811\n",
      "Epoch [1/1], Step [1200/3664], Loss: 0.0421\n",
      "Epoch [1/1], Step [1250/3664], Loss: 0.0615\n",
      "Epoch [1/1], Step [1300/3664], Loss: 0.0820\n",
      "Epoch [1/1], Step [1350/3664], Loss: 0.0820\n",
      "Epoch [1/1], Step [1400/3664], Loss: 0.0823\n",
      "Epoch [1/1], Step [1450/3664], Loss: 0.0705\n",
      "Epoch [1/1], Step [1500/3664], Loss: 0.0570\n",
      "Epoch [1/1], Step [1550/3664], Loss: 0.0701\n",
      "Epoch [1/1], Step [1600/3664], Loss: 0.0884\n",
      "Epoch [1/1], Step [1650/3664], Loss: 0.0485\n",
      "Epoch [1/1], Step [1700/3664], Loss: 0.0479\n",
      "Epoch [1/1], Step [1750/3664], Loss: 0.0830\n",
      "Epoch [1/1], Step [1800/3664], Loss: 0.0708\n",
      "Epoch [1/1], Step [1850/3664], Loss: 0.0642\n",
      "Epoch [1/1], Step [1900/3664], Loss: 0.0708\n",
      "Epoch [1/1], Step [1950/3664], Loss: 0.0751\n",
      "Epoch [1/1], Step [2000/3664], Loss: 0.0747\n",
      "Epoch [1/1], Step [2050/3664], Loss: 0.0749\n",
      "Epoch [1/1], Step [2100/3664], Loss: 0.0448\n",
      "Epoch [1/1], Step [2150/3664], Loss: 0.0566\n",
      "Epoch [1/1], Step [2200/3664], Loss: 0.0685\n",
      "Epoch [1/1], Step [2250/3664], Loss: 0.0550\n",
      "Epoch [1/1], Step [2300/3664], Loss: 0.0656\n",
      "Epoch [1/1], Step [2350/3664], Loss: 0.0697\n",
      "Epoch [1/1], Step [2400/3664], Loss: 0.0724\n",
      "Epoch [1/1], Step [2450/3664], Loss: 0.0754\n",
      "Epoch [1/1], Step [2500/3664], Loss: 0.0457\n",
      "Epoch [1/1], Step [2550/3664], Loss: 0.0688\n",
      "Epoch [1/1], Step [2600/3664], Loss: 0.0821\n",
      "Epoch [1/1], Step [2650/3664], Loss: 0.0778\n",
      "Epoch [1/1], Step [2700/3664], Loss: 0.0576\n",
      "Epoch [1/1], Step [2750/3664], Loss: 0.0689\n",
      "Epoch [1/1], Step [2800/3664], Loss: 0.0912\n",
      "Epoch [1/1], Step [2850/3664], Loss: 0.0613\n",
      "Epoch [1/1], Step [2900/3664], Loss: 0.0468\n",
      "Epoch [1/1], Step [2950/3664], Loss: 0.0432\n",
      "Epoch [1/1], Step [3000/3664], Loss: 0.0632\n",
      "Epoch [1/1], Step [3050/3664], Loss: 0.0570\n",
      "Epoch [1/1], Step [3100/3664], Loss: 0.0646\n",
      "Epoch [1/1], Step [3150/3664], Loss: 0.0780\n",
      "Epoch [1/1], Step [3200/3664], Loss: 0.0664\n",
      "Epoch [1/1], Step [3250/3664], Loss: 0.0652\n",
      "Epoch [1/1], Step [3300/3664], Loss: 0.0605\n",
      "Epoch [1/1], Step [3350/3664], Loss: 0.0483\n",
      "Epoch [1/1], Step [3400/3664], Loss: 0.0399\n",
      "Epoch [1/1], Step [3450/3664], Loss: 0.0833\n",
      "Epoch [1/1], Step [3500/3664], Loss: 0.0619\n",
      "Epoch [1/1], Step [3550/3664], Loss: 0.0576\n",
      "Epoch [1/1], Step [3600/3664], Loss: 0.0692\n",
      "Epoch [1/1], Step [3650/3664], Loss: 0.0844\n",
      "\n",
      "train-loss: 0.0641,\n",
      "Validation [1/1], Step [0/382], Loss: 0.0427\n",
      "Validation [1/1], Step [50/382], Loss: 0.0271\n",
      "Validation [1/1], Step [100/382], Loss: 0.0250\n",
      "Validation [1/1], Step [150/382], Loss: 0.0511\n",
      "Validation [1/1], Step [200/382], Loss: 0.0412\n",
      "Validation [1/1], Step [250/382], Loss: 0.0296\n",
      "Validation [1/1], Step [300/382], Loss: 0.0460\n",
      "Validation [1/1], Step [350/382], Loss: 0.0313\n",
      "validation loss: 0.0343, \n",
      "\n",
      "Improvement-Detected, save-model\n",
      "Wall time: 2h 23min 38s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "n_epochs = 1\n",
    "print_every = 50\n",
    "valid_loss_min = 0.0343#np.Inf\n",
    "val_loss = []\n",
    "train_loss = []\n",
    "total_step = len(train_dataloader)\n",
    "\n",
    "nan_batches = []\n",
    "\n",
    "run = True\n",
    "\n",
    "for epoch in range(1, n_epochs+1):\n",
    "    \n",
    "    running_loss = 0.0\n",
    "    print(f'Epoch {epoch}\\n')\n",
    "    \n",
    "    # Work through batches\n",
    "    for batch_idx, data in enumerate(train_dataloader): #data: (['idx', 'rgb', 'speed', 'steer', 'throttle', 'brake'])\n",
    "\n",
    "        loss = forward_pass(data)\n",
    "        \n",
    "        # Backprop\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        loss_value = loss.item()\n",
    "        \n",
    "        \"\"\"\n",
    "        if np.isnan(loss_value):\n",
    "            print(\"nan\", batch_idx)\n",
    "            nan_batches.append((batch_idx, data))\n",
    "         \"\"\"   \n",
    "        \n",
    "        running_loss += loss_value\n",
    "        if (batch_idx) % print_every == 0:\n",
    "            print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n",
    "                   .format(epoch, n_epochs, batch_idx, total_step, loss_value ))\n",
    "        \"\"\"\n",
    "        et = time.time()\n",
    "        print(et-at)\n",
    "        at = time.time()\n",
    "        \"\"\"\n",
    "        \n",
    "    # Epoch finished, evaluate network and save if network_learned\n",
    "    train_loss.append(running_loss/total_step)\n",
    "    print(f'\\ntrain-loss: {np.mean(train_loss):.4f},') # TODO SOLVE NAN ISSUES\n",
    "    batch_loss = 0\n",
    "\n",
    "    \n",
    "    # Evaluation on Test set, skipped for now\n",
    "\n",
    "    with torch.no_grad():\n",
    "        net.eval()\n",
    "        \n",
    "        val_total_step = len(test_dataloader)\n",
    "        \n",
    "        for batch_idx, data in enumerate(test_dataloader):\n",
    "            \n",
    "            loss = forward_pass(data, False)\n",
    "            \n",
    "            loss_value = loss.item()\n",
    "            \n",
    "            if (batch_idx) % print_every == 0:\n",
    "                print ('Validation [{}/{}], Step [{}/{}], Loss: {:.4f}'\n",
    "                       .format(epoch, n_epochs, batch_idx, val_total_step, loss_value ))\n",
    "            \n",
    "            batch_loss += loss_value\n",
    "        val_loss.append(batch_loss/len(test_dataloader))\n",
    "        mean_val_loss = np.mean(val_loss)\n",
    "        \n",
    "        print(f'validation loss: {mean_val_loss:.4f}, \\n') # TODO SOLVE NAN ISSUES\n",
    "\n",
    "        network_learned = mean_val_loss < valid_loss_min\n",
    "        if network_learned:\n",
    "            valid_loss_min = mean_val_loss\n",
    "            #torch.save(net.state_dict(), 'resnet'+\"_E-\"+str(epoch)+'.pth')\n",
    "            torch.save(net.state_dict(), \"resnet_E-6.pth\")\n",
    "            print('Improvement-Detected, save-model')\n",
    "\n",
    "    # Back to training\n",
    "    net.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a85942e9",
   "metadata": {},
   "source": [
    "### Test predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "09ecda41",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
    "iterator = iter(test_dataloader)\n",
    "#print(next(iter(test_dataloader)).keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7b96dc55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.03389787296273193\n",
      "0.02896353057844317\n",
      "0.015627471198564002\n"
     ]
    }
   ],
   "source": [
    "data = next(iterator)\n",
    "#data\n",
    "\n",
    "X_rgb = transform_norm(torch.squeeze(data[\"rgb\"])).float().to(device)\n",
    "labels = data[\"command\"]\n",
    "labels = torch.where(labels == -1, torch.tensor(0), labels).to(torch.int64) # Replace by -1 by 0\n",
    "# Convert the labels to a one hot encoded tensor\n",
    "one_hot = torch.nn.functional.one_hot(labels, num_classes=7).to(device)\n",
    "X_cmd = torch.squeeze(one_hot).float().to(device)\n",
    "X_spd = ((data[\"speed\"]-speed_mean)/speed_std).float().to(device)\n",
    "#print(np.mean(X_spd.cpu().numpy()))\n",
    "\n",
    "target_ = (data[\"throttle\"], data[\"steer\"], data[\"brake\"])\n",
    "with torch.no_grad():\n",
    "    net.eval()\n",
    "    outputs_ = net(X_rgb, X_cmd, X_spd)\n",
    "    \n",
    "# Durchschnittlicher abs. fehler\n",
    "for i in [0,1,2]:\n",
    "    print(np.mean(abs(outputs_[i].cpu().numpy()-target_[i].cpu().numpy())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51b81953",
   "metadata": {},
   "outputs": [],
   "source": [
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "015a3ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.round(outputs_[0].cpu().numpy()-target_[0].cpu().numpy(),2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8c4f638",
   "metadata": {},
   "source": [
    "Bias Variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "074bc0f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variance \n",
    "\n",
    "for i in [0,1,2]:\n",
    "    outputs = (outputs_[i].cpu().numpy())\n",
    "    #print(outputs)\n",
    "    mean_outputs = np.mean(outputs_[i].cpu().numpy())\n",
    "    #print(mean_outputs)\n",
    "    diff = (outputs-mean_outputs)**2\n",
    "    #print(diff)\n",
    "    value = np.mean(diff)\n",
    "    print(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4da6a585",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bias\n",
    "for i in [0,1,2]:\n",
    "    targets = (target_[i].cpu().numpy())\n",
    "    #print(outputs)\n",
    "    mean_outputs = np.mean(outputs_[i].cpu().numpy())\n",
    "    #print(mean_outputs)\n",
    "    diff = outputs-mean_outputs\n",
    "    #print(diff)\n",
    "    value = np.mean(diff)\n",
    "    print(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d580060",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in [0,1,2]:\n",
    "    print(np.mean(abs(target_[i].cpu().numpy())))\n",
    "    print(np.std(abs(target_[i].cpu().numpy())))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fd372c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "i =0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88953edc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(np.round(outputs_[i].cpu().numpy(),1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd39c1d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.round(target_[i].cpu().numpy(),1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ab4158b",
   "metadata": {},
   "source": [
    "### IMG Processing\n",
    "\n",
    "BGR is now standard FOR carla agent and training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "39dbd043",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 1, 3, 160, 960])\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "idx, batch = next(enumerate(test_dataloader))\n",
    "print(batch[\"rgb\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fd55e6d5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(160, 960, 3)\n",
      "(160, 960, 3)\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "img = batch[\"rgb\"][0]#.shape\n",
    "img = img.numpy().astype(np.uint8).reshape(160,960,3)\n",
    "print(img.shape)\n",
    "\n",
    "#img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB) # NUR HIER, NICHT IN CARLA AGENT\n",
    "print(img.shape)\n",
    "print(type(img))\n",
    "transform = transforms.Compose([transforms.ToPILImage()])\n",
    "\n",
    "tensor = transform(img)\n",
    "\n",
    "#print(type(tensor))\n",
    "\n",
    "tensor.show()\n",
    "\n",
    "#torch.tensor(tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "20307478",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(160, 960, 3)\n"
     ]
    }
   ],
   "source": [
    "pil_img = img.astype(np.uint8).reshape(160,960,3)\n",
    "transform = transforms.Compose([transforms.ToPILImage()])\n",
    "print(pil_img.shape)\n",
    "pil_img = transform(pil_img)\n",
    "pil_img.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49b4b228",
   "metadata": {},
   "source": [
    "TEST Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aec50160",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor = transform_norm(torch.squeeze(data[\"rgb\"],1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48a21ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor = torch.squeeze(transform_norm(data[\"rgb\"])).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "683d06c0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(64):\n",
    "    print(np.mean(tensor.numpy()[i], axis = (1,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfcd1c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(tensor.numpy(), axis = (0,2,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acc09d0c",
   "metadata": {},
   "source": [
    "### adding Navigation and speed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c13bbeae",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "iterator = iter(test_dataloader)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56111d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = next(iterator)\n",
    "#data[\"speed\"]\n",
    "#data[\"command\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59fd7382",
   "metadata": {},
   "source": [
    "Command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22295af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Assume labels is a 1D tensor with values from 0 to 6\n",
    "\n",
    "labels = data[\"command\"]\n",
    "labels = torch.where(labels == -1, torch.tensor(0), labels) # Replace by -1 by 0\n",
    "labels = labels.to(torch.int64)\n",
    "\n",
    "# Convert the labels to a one hot encoded tensor\n",
    "one_hot = torch.nn.functional.one_hot(labels, num_classes=7)\n",
    "one_hot = torch.squeeze(one_hot)\n",
    "\n",
    "print(one_hot.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "799272fb",
   "metadata": {},
   "source": [
    "Speed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce212d44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calc mean over trainingsset\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "print(len(train_dataloader))\n",
    "iterator = iter(train_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8c1a946",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "summe = []\n",
    "for batch_idx, data in enumerate(train_dataloader):\n",
    "    #print(data)\n",
    "    if i % 100 == 0:\n",
    "        print(i)\n",
    "    summe.append(np.mean(data[\"speed\"].numpy()))\n",
    "    i += 1\n",
    "    if i >= 1000:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95cd4153",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(summe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de90cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.mean(summe)) # 2.2078979146598274\n",
    "print(np.std(summe)) # 0.22455625005948113\n",
    "speed_mean = np.mean(summe)\n",
    "speed_std = np.std(summe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19f20dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iterator)\n",
    "#print(np.round(batch[\"speed\"].numpy(),2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd855f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "(batch[\"speed\"]-speed_mean)/speed_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca1cce31",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(((batch[\"speed\"]-speed_mean)/speed_std).numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd7c659b",
   "metadata": {},
   "source": [
    "### Class imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bd6c193a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3664\n"
     ]
    }
   ],
   "source": [
    "# calc mean over trainingsset\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "print(len(train_dataloader))\n",
    "iterator = iter(train_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a92caf74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "steer = []\n",
    "throttle = []\n",
    "brake = []\n",
    "for batch_idx, data in enumerate(train_dataloader):\n",
    "    #print(data)\n",
    "    if i % 100 == 0:\n",
    "        print(i)\n",
    "    steer.append(np.mean(data[\"steer\"].numpy()))\n",
    "    throttle.append(np.mean(data[\"throttle\"].numpy()))\n",
    "    brake.append(np.mean(data[\"brake\"].numpy()))\n",
    "    i += 1\n",
    "    if i >= 100:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e0b198de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.2187499999999712e-05\n",
      "0.33941115939128463\n",
      "0.26609375\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(steer))\n",
    "print(np.mean(throttle))\n",
    "print(np.mean(brake))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7177fc92",
   "metadata": {},
   "source": [
    "### Vanishing/Exploding Gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac119fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "for name, param in net.thr_head.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(name, param.data.cpu().numpy())\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68ba6bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, param in net.spd_input.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(name, np.max(abs(param.data.cpu().numpy())))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd0f44f0",
   "metadata": {},
   "source": [
    "## Saving and Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2eecda0",
   "metadata": {},
   "source": [
    "Not suited for leaderboard agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f516f764",
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.save(net, 'rgb_resnet.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86a81a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#net = torch.load('rgb_resnet.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4e10b82",
   "metadata": {},
   "source": [
    "suited for leaderboard agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55596f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.save(net.state_dict(), \"pretrained_1E.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ccc5d256",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MyResnet(\n",
       "  (net): ResNet(\n",
       "    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu): ReLU(inplace=True)\n",
       "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "    (layer1): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (layer2): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (layer3): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (layer4): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "    (fc): Identity()\n",
       "  )\n",
       "  (cmd_input): Sequential(\n",
       "    (0): Linear(in_features=7, out_features=128, bias=True)\n",
       "    (1): Tanh()\n",
       "  )\n",
       "  (spd_input): Sequential(\n",
       "    (0): Linear(in_features=1, out_features=128, bias=True)\n",
       "    (1): Tanh()\n",
       "  )\n",
       "  (mlp): Sequential(\n",
       "    (0): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (1): Tanh()\n",
       "    (2): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (3): Tanh()\n",
       "  )\n",
       "  (thr_head): Sequential(\n",
       "    (0): Linear(in_features=768, out_features=1, bias=True)\n",
       "    (1): Sigmoid()\n",
       "  )\n",
       "  (brk_head): Sequential(\n",
       "    (0): Linear(in_features=768, out_features=1, bias=True)\n",
       "    (1): Sigmoid()\n",
       "  )\n",
       "  (str_head): Sequential(\n",
       "    (0): Linear(in_features=768, out_features=1, bias=True)\n",
       "    (1): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#net = MyResnet()\n",
    "#net.load_state_dict(torch.load(\"resnet_E-4.pth\"))\n",
    "#net.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8568190",
   "metadata": {},
   "source": [
    "## Testing Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bef6d6ee",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input and output must have the same number of spatial dimensions, but got input with spatial dimensions of [3, 160, 960] and output size of [224, 224]. Please provide input tensor in (N, C, d1, d2, ...,dK) format and output size in (o1, o2, ...,oK) format.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_8264\\2626154568.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mtest_dataloader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_dataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0midx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_dataloader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtransform_norm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"rgb\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mimg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torchvision\\transforms\\transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, img)\u001b[0m\n\u001b[0;32m     93\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     94\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 95\u001b[1;33m             \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     96\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     97\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1188\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1189\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1190\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1191\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1192\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torchvision\\transforms\\transforms.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, img)\u001b[0m\n\u001b[0;32m    344\u001b[0m             \u001b[0mPIL\u001b[0m \u001b[0mImage\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mRescaled\u001b[0m \u001b[0mimage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    345\u001b[0m         \"\"\"\n\u001b[1;32m--> 346\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minterpolation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mantialias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    347\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    348\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torchvision\\transforms\\functional.py\u001b[0m in \u001b[0;36mresize\u001b[1;34m(img, size, interpolation, max_size, antialias)\u001b[0m\n\u001b[0;32m    474\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mF_pil\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moutput_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minterpolation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpil_interpolation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    475\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 476\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mF_t\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moutput_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minterpolation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minterpolation\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mantialias\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mantialias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    477\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    478\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torchvision\\transforms\\functional_tensor.py\u001b[0m in \u001b[0;36mresize\u001b[1;34m(img, size, interpolation, antialias)\u001b[0m\n\u001b[0;32m    467\u001b[0m     \u001b[0malign_corners\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0minterpolation\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m\"bilinear\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"bicubic\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    468\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 469\u001b[1;33m     \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minterpolate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minterpolation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malign_corners\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0malign_corners\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mantialias\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mantialias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    470\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    471\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0minterpolation\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"bicubic\"\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mout_dtype\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muint8\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36minterpolate\u001b[1;34m(input, size, scale_factor, mode, align_corners, recompute_scale_factor, antialias)\u001b[0m\n\u001b[0;32m   3864\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3865\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mdim\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3866\u001b[1;33m                 raise ValueError(\n\u001b[0m\u001b[0;32m   3867\u001b[0m                     \u001b[1;34m\"Input and output must have the same number of spatial dimensions, but got \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3868\u001b[0m                     \u001b[1;34mf\"input with spatial dimensions of {list(input.shape[2:])} and output size of {size}. \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Input and output must have the same number of spatial dimensions, but got input with spatial dimensions of [3, 160, 960] and output size of [224, 224]. Please provide input tensor in (N, C, d1, d2, ...,dK) format and output size in (o1, o2, ...,oK) format."
     ]
    }
   ],
   "source": [
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
    "idx, X = next(enumerate(test_dataloader))\n",
    "img = transform_norm(X[\"rgb\"])\n",
    "img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "339784d7",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'img' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_8264\\2189231395.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'img' is not defined"
     ]
    }
   ],
   "source": [
    "torch.squeeze(img,1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2e33ce4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1674293197.7053356\n",
      "1.474029541015625\n",
      "1.254969835281372\n",
      "1.2530007362365723\n",
      "1.1779956817626953\n",
      "1.2210910320281982\n",
      "1.2300705909729004\n",
      "1.1875627040863037\n",
      "1.3062374591827393\n",
      "1.1637141704559326\n",
      "1.1044037342071533\n",
      "1.2210023403167725\n",
      "1.1440010070800781\n",
      "1.2349956035614014\n",
      "1.1000020503997803\n",
      "1.2449994087219238\n",
      "1.261162281036377\n",
      "1.2310504913330078\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_8264\\203229808.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m#print(len(test_dataloader))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_dataloader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m     \u001b[1;31m# further preprocessing\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mX_rgb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtransform_norm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"rgb\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    626\u001b[0m                 \u001b[1;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    627\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore[call-arg]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 628\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    629\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    630\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[1;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    669\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    670\u001b[0m         \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 671\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    672\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    673\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     56\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 58\u001b[1;33m                 \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     59\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     56\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 58\u001b[1;33m                 \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     59\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\OneDrive\\UNI\\Master\\WS22\\APP-RAS\\Programming\\data_pipeline\\dataset.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     50\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mdata_point_idx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0midx_lagged\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0midx\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m                 \u001b[0mfile_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__get_file_path_from_df\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_idx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata_point_idx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 52\u001b[1;33m                 \u001b[0mdata_t\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_data_from_path\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     53\u001b[0m                 \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx_array\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata_t\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m                 \u001b[0midx_array\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\OneDrive\\UNI\\Master\\WS22\\APP-RAS\\Programming\\data_pipeline\\dataset.py\u001b[0m in \u001b[0;36mload_data_from_path\u001b[1;34m(self, path)\u001b[0m\n\u001b[0;32m    130\u001b[0m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    131\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mfile_format\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m\".png\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\".jpg\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 132\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    133\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcvtColor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCOLOR_BGR2RGB\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# TODO CHANGED TO RGB\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    134\u001b[0m             \u001b[1;31m# reshape to #channels; height; width\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
    "#print(len(test_dataloader))\n",
    "at = 0\n",
    "for batch_idx, data in enumerate(test_dataloader):\n",
    "    # further preprocessing\n",
    "    X_rgb = transform_norm(torch.squeeze(data[\"rgb\"])).float()\n",
    "    if False:#augument:\n",
    "        X_rgb = transform_augument(X_rgb)\n",
    "    labels = data[\"command\"]\n",
    "    labels = torch.where(labels == -1, torch.tensor(0), labels).to(torch.int64) # Replace by -1 by 0\n",
    "    # Convert the labels to a one hot encoded tensor\n",
    "    one_hot = torch.nn.functional.one_hot(labels, num_classes=7)\n",
    "    X_cmd = torch.squeeze(one_hot).float()\n",
    "    X_spd = ((data[\"speed\"]-speed_mean)/speed_std).float()\n",
    "    \n",
    "    Y_throttle = data[\"throttle\"].float()\n",
    "    Y_steer = data[\"steer\"].float()\n",
    "    Y_brake = data[\"brake\"].float()\n",
    "\n",
    "    # move to GPU\n",
    "    X_rgb = to_cuda_if_possible(X_rgb)\n",
    "    X_cmd = to_cuda_if_possible(X_cmd)\n",
    "    X_spd = to_cuda_if_possible(X_spd)\n",
    "    \n",
    "    Y_throttle = to_cuda_if_possible(Y_throttle)\n",
    "    Y_steer = to_cuda_if_possible(Y_steer)\n",
    "    Y_brake = to_cuda_if_possible(Y_brake)\n",
    "    \n",
    "    et = time.time()\n",
    "    print(et-at)\n",
    "    at = time.time()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c96bcd38",
   "metadata": {},
   "source": [
    "#### Training secounds per batch\n",
    "\n",
    "1.8778209686279297\n",
    "1.7550039291381836\n",
    "1.9759962558746338\n",
    "2.018435001373291"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "613a0284",
   "metadata": {},
   "source": [
    "#### Only dataloader and preprocessing secounds per batch\n",
    "1.235999584197998\n",
    "1.2680015563964844\n",
    "1.3483715057373047\n",
    "1.2585253715515137\n",
    "1.1267704963684082\n",
    "1.124000072479248\n",
    "1.2410008907318115\n",
    "1.2269997596740723\n",
    "1.200000286102295\n",
    "1.267998456954956\n",
    "1.2166194915771484\n",
    "1.2077960968017578\n",
    "1.2405602931976318\n",
    "1.2019383907318115"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "258399f1",
   "metadata": {},
   "source": [
    "# asdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "314eb467",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# LSCHEN\\nroot_dir = \"D:\\\\data\\\\data\"\\nkeep_input = [\"lidar\", \"rgb\", \"measurements\"] # \"lidar\"\\n\\ndef move_unused_sensors_to_new_folder(root_dir, keep_input):\\n    for (root, dirs, files) in os.walk(root_dir, topdown=True):\\n        # Current folder contains the files\\n        if not dirs:\\n            dir, input_type = os.path.split(root)\\n            if input_type not in keep_input:\\n                path_parts = root.split(os.sep)\\n                idx_data_first = path_parts.index(\"data\")\\n                path_parts[idx_data_first + 1] += \" unused\"\\n                dir_new = os.path.join(*path_parts)\\n                if not os.path.exists(dir_new):\\n                    os.makedirs(dir_new)\\n                shutil.move(root, dir_new)\\n                \\nimport shutil\\nmove_unused_sensors_to_new_folder(root_dir,keep_input)\\n'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "# LSCHEN\n",
    "root_dir = \"D:\\\\data\\\\data\"\n",
    "keep_input = [\"lidar\", \"rgb\", \"measurements\"] # \"lidar\"\n",
    "\n",
    "def move_unused_sensors_to_new_folder(root_dir, keep_input):\n",
    "    for (root, dirs, files) in os.walk(root_dir, topdown=True):\n",
    "        # Current folder contains the files\n",
    "        if not dirs:\n",
    "            dir, input_type = os.path.split(root)\n",
    "            if input_type not in keep_input:\n",
    "                path_parts = root.split(os.sep)\n",
    "                idx_data_first = path_parts.index(\"data\")\n",
    "                path_parts[idx_data_first + 1] += \" unused\"\n",
    "                dir_new = os.path.join(*path_parts)\n",
    "                if not os.path.exists(dir_new):\n",
    "                    os.makedirs(dir_new)\n",
    "                shutil.move(root, dir_new)\n",
    "                \n",
    "import shutil\n",
    "move_unused_sensors_to_new_folder(root_dir,keep_input)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7dec5fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saves preped data in same folder structure under rgb_prep\n",
    "def rgb_to_disk_2(format):\n",
    "    assert format in [\".npy\", \".npz\", \".pt\"]\n",
    "    fn_save = np.save if format == \".npy\" else np.savez_compressed\n",
    "    # save npy/ npz\n",
    "    df_meta = dataset.df_meta_data\n",
    "    for idx in tqdm(range(len(df_meta))):\n",
    "        path_parts = dataset.df_meta_data[\"dir\"][idx].split(os.sep)\n",
    "        # path_parts[path_parts.index(\"data\") + 1] += \"_prep_npy\"\n",
    "        dir_name_zip = os.path.join(*path_parts, \"rgb_prep\")\n",
    "        if not os.path.exists(dir_name_zip):\n",
    "            os.makedirs(dir_name_zip)\n",
    "            # shutil.copytree(os.path.join(dataset.df_meta_data[\"dir\"][idx], \"measurements\"), os.path.join(*path_parts, \"measurements\"))\n",
    "        path = os.path.join(df_meta.iloc[idx][0], \"rgb\", df_meta.iloc[idx][1])\n",
    "        img_np = dataset.load_data_from_path(path)\n",
    "        img_torch = torch.Tensor(img_np)\n",
    "        img_torch_prep = preprocessing[\"rgb\"](img_torch)\n",
    "        img_np_prep = img_torch_prep.numpy()\n",
    "        filename_np = os.path.join(dir_name_zip, f\"{df_meta.iloc[idx]['rgb'].split('.')[0]}{format}\")\n",
    "        # torch.save(img_torch_prep, filename_torch)\n",
    "        with open(filename_np, 'wb') as f:\n",
    "            fn_save(f, img_np_prep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d21bf30d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = \"D:\\\\data\\\\Train\"\n",
    "\n",
    "config = {\"used_inputs\": [\"rgb\",\"measurements\"], \n",
    "        \"used_measurements\": [\"speed\", \"steer\", \"throttle\", \"brake\", \"command\"],\n",
    "        \"seq_len\": 1\n",
    "        }\n",
    "\n",
    "dataset = CARLADataset(root_dir=train_path, config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "13506ad5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                       | 0/258841 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'preprocessing' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_15812\\3399023220.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtqdm\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mrgb_to_disk_2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\".npy\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_15812\\3794635102.py\u001b[0m in \u001b[0;36mrgb_to_disk_2\u001b[1;34m(format)\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[0mimg_np\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_data_from_path\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m         \u001b[0mimg_torch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg_np\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m         \u001b[0mimg_torch_prep\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpreprocessing\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"rgb\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg_torch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m         \u001b[0mimg_np_prep\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimg_torch_prep\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m         \u001b[0mfilename_np\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdir_name_zip\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34mf\"{df_meta.iloc[idx]['rgb'].split('.')[0]}{format}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'preprocessing' is not defined"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "rgb_to_disk_2(\".npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "731e4a85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'D:data\\\\Train\\\\cycl_dataset_23_11\\\\Routes_non-straight-junction-Scen4_Town01_junction_Seed2000\\\\non-straight-junction-Scen4_Town01_junction_route0_11_23_20_09_07'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.df_meta_data.loc[0][\"dir\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
