{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3c4750c0",
   "metadata": {},
   "source": [
    "# Model ResNet\n",
    "\n",
    "https://www.pluralsight.com/guides/introduction-to-resnet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7e8eff3",
   "metadata": {},
   "source": [
    "#### TODOS\n",
    "1. DONE Debugging, does output make sense?\n",
    "    1. Resize images\n",
    "    2. preprocessing fixes\n",
    "    5. replace scaling by proper function\n",
    "2. try on leaderboard\n",
    "3. Include Odometry and fuse into heads\n",
    "    - Speed\n",
    "    - Location\n",
    "4. navigation\n",
    "5. controller\n",
    "6. Evaluation on Test set, Modularization\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96a254eb",
   "metadata": {},
   "source": [
    "## Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2969a3fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL STUFF\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "#import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import *\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "\n",
    "# GENERAL STUFF\n",
    "import time\n",
    "import copy\n",
    "import os\n",
    "import sys\n",
    "sys.path.insert(1, 'C:\\\\Users\\\\morit\\\\OneDrive\\\\UNI\\\\Master\\\\WS22\\\\APP-RAS\\\\Programming\\\\data_pipeline') # TODO\n",
    "\n",
    "# DATA ENGINEERING\n",
    "from data_sampler import WeightedSampler\n",
    "from dataset import CARLADataset #, CARLADatasetMultiProcessing\n",
    "from utils import train_test_split, create_metadata_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ce60a27",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f4066654",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyResnet(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        # ResNet Architecture with pretrained weights, also bigger resnets available\n",
    "        self.net = torchvision.models.resnet18(weights=True)\n",
    "        num_ftrs = self.net.fc.in_features\n",
    "\n",
    "        # Top layer of ResNet which you can modify. We choose Identity to use it as Input for all the heads\n",
    "        self.net.fc = nn.Identity()\n",
    "        \n",
    "        # Input Layer fuer cmd, spd\n",
    "        self.cmd_input = nn.Sequential(\n",
    "            nn.Linear(7, 128),\n",
    "            nn.Tanh(), #nn.LeakyReLU() # TODO\n",
    "            #nn.Dropout(p=0.5, inplace=False)\n",
    "        )\n",
    "        \n",
    "        self.spd_input = nn.Sequential(\n",
    "            nn.Linear(1, 128),\n",
    "            nn.Tanh(), #nn.LeakyReLU() # TODO\n",
    "            #nn.Dropout(p=0.5, inplace=False)\n",
    "        )\n",
    "        \n",
    "        # MLP\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(num_ftrs+128+128, num_ftrs+128+128),\n",
    "            nn.Tanh(), #nn.LeakyReLU()\n",
    "            #nn.Dropout(p=0.5, inplace=False),\n",
    "            nn.Linear(num_ftrs+128+128, num_ftrs+128+128),\n",
    "            nn.Tanh()#, #nn.LeakyReLU()\n",
    "            #nn.Dropout(p=0.5, inplace=False)\n",
    "        )\n",
    "        \n",
    "        # Regression Heads for Throttle, Brake and Steering\n",
    "        self.thr_head = nn.Sequential(\n",
    "            nn.Linear(num_ftrs+128+128, 1),\n",
    "            nn.Sigmoid() # [0,1] Range Output\n",
    "            \n",
    "        )\n",
    "        \n",
    "        self.brk_head = nn.Sequential(\n",
    "            nn.Linear(num_ftrs+128+128, 1),\n",
    "            nn.Sigmoid() # [0,1] Range Output\n",
    "            \n",
    "        )\n",
    "        \n",
    "        self.str_head = nn.Sequential(\n",
    "            nn.Linear(num_ftrs+128+128, 1),\n",
    "            nn.Tanh() # [-1,1] Range Output\n",
    "            \n",
    "        )\n",
    "\n",
    "    # Forward Pass of the Model\n",
    "    def forward(self, rgb, cmd, spd):\n",
    "        rgb = self.net(rgb) # BRG\n",
    "        cmd = self.cmd_input(cmd)\n",
    "        spd = self.spd_input(spd)\n",
    "        \n",
    "        x = torch.cat((rgb, cmd, spd),1)\n",
    "        x = self.mlp(x)\n",
    "        \n",
    "        #x = self.net.fc(x)\n",
    "        return self.thr_head(x), self.str_head(x), self.brk_head(x) # 3 Outputs since we have 3 Heads\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beefd313",
   "metadata": {},
   "source": [
    "## Data Loaders, Data Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2a3c0c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#path_ege_data = os.path.join(\"..\", \"..\", \"data\", \"Dataset Ege\")\n",
    "train_path = \"D:\\\\data\\\\Train\"\n",
    "test_path = \"D:\\\\data\\\\Test\"\n",
    "\n",
    "config = {\"used_inputs\": [\"rgb_prep\",\"measurements\"], \n",
    "        \"used_measurements\": [\"speed\", \"steer\", \"throttle\", \"brake\", \"command\"],\n",
    "        \"seq_len\": 1\n",
    "        }\n",
    "\n",
    "train_meta = create_metadata_df(train_path, config[\"used_inputs\"])\n",
    "train_dataset = CARLADataset(root_dir=train_path, config=config,df_meta_data=train_meta)\n",
    "\n",
    "test_meta = create_metadata_df(test_path, config[\"used_inputs\"])\n",
    "test_dataset = CARLADataset(root_dir=test_path, config=config,df_meta_data=test_meta)\n",
    "\n",
    "batch_size = 64\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d3485634",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3664"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aa630256",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "382"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ebfb6bf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "augumentations = torch.nn.ModuleList([\n",
    "        transforms.GaussianBlur(9),\n",
    "        transforms.ColorJitter(brightness=1.0, contrast=0.5, saturation=1, hue=0.1),\n",
    "        transforms.RandomErasing()\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f873601b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#mean = torch.tensor([79.6657, 81.5673, 105.6161]) BGR\n",
    "#std = torch.tensor([66.8309, 60.1001, 66.2220])\n",
    "\n",
    "mean = torch.tensor([105.6161, 81.5673, 79.6657]) # RGB\n",
    "std = torch.tensor([66.2220, 60.1001, 66.8309])\n",
    "\n",
    "\n",
    "transform_norm = transforms.Compose([\n",
    "    transforms.Normalize(mean, std),\n",
    "    transforms.Resize([224,224])\n",
    "])\n",
    "\n",
    "transform_augument = transforms.RandomApply(augumentations)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd1c77c4",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f57432c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\morit\\anaconda3\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MyResnet(\n",
       "  (net): ResNet(\n",
       "    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu): ReLU(inplace=True)\n",
       "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "    (layer1): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (layer2): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (layer3): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (layer4): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "    (fc): Identity()\n",
       "  )\n",
       "  (cmd_input): Sequential(\n",
       "    (0): Linear(in_features=7, out_features=128, bias=True)\n",
       "    (1): Tanh()\n",
       "  )\n",
       "  (spd_input): Sequential(\n",
       "    (0): Linear(in_features=1, out_features=128, bias=True)\n",
       "    (1): Tanh()\n",
       "  )\n",
       "  (mlp): Sequential(\n",
       "    (0): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (1): Tanh()\n",
       "    (2): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (3): Tanh()\n",
       "  )\n",
       "  (thr_head): Sequential(\n",
       "    (0): Linear(in_features=768, out_features=1, bias=True)\n",
       "    (1): Sigmoid()\n",
       "  )\n",
       "  (brk_head): Sequential(\n",
       "    (0): Linear(in_features=768, out_features=1, bias=True)\n",
       "    (1): Sigmoid()\n",
       "  )\n",
       "  (str_head): Sequential(\n",
       "    (0): Linear(in_features=768, out_features=1, bias=True)\n",
       "    (1): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialise Model (GPU or CPU)\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "net = MyResnet().cuda() if device else net\n",
    "net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "505b9074",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_cuda_if_possible(data):\n",
    "    return data.to(device) if device else data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "263843ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_pass(data, augument = True):\n",
    "    # further preprocessing\n",
    "    #X_rgb = transform_norm(torch.squeeze(data[\"rgb_prep\"])).float()\n",
    "    X_rgb = torch.squeeze(data[\"rgb_prep\"])[:,[2,1,0]].float() # BGR --> RGB\n",
    "    \"\"\"\n",
    "    if False:#augument:\n",
    "        X_rgb = transform_augument(X_rgb)\n",
    "    \"\"\"\n",
    "    labels = data[\"command\"]\n",
    "    labels = torch.where(labels == -1, torch.tensor(0), labels).to(torch.int64) # Replace by -1 by 0\n",
    "    # Convert the labels to a one hot encoded tensor\n",
    "    one_hot = torch.nn.functional.one_hot(labels, num_classes=7)\n",
    "    X_cmd = torch.squeeze(one_hot).float()\n",
    "    X_spd = ((data[\"speed\"]-speed_mean)/speed_std).float()\n",
    "    \n",
    "    Y_throttle = data[\"throttle\"].float()\n",
    "    Y_steer = data[\"steer\"].float()\n",
    "    Y_brake = data[\"brake\"].float()\n",
    "\n",
    "    # move to GPU\n",
    "    X_rgb = to_cuda_if_possible(X_rgb)\n",
    "    X_cmd = to_cuda_if_possible(X_cmd)\n",
    "    X_spd = to_cuda_if_possible(X_spd)\n",
    "    \n",
    "    Y_throttle = to_cuda_if_possible(Y_throttle)\n",
    "    Y_steer = to_cuda_if_possible(Y_steer)\n",
    "    Y_brake = to_cuda_if_possible(Y_brake)\n",
    "\n",
    "    # compute outputs\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    Y_hat = net(X_rgb, X_cmd, X_spd)\n",
    "    Y_hat_throttle = to_cuda_if_possible(Y_hat[0])\n",
    "    Y_hat_steer = to_cuda_if_possible(Y_hat[1])\n",
    "    Y_hat_brake = to_cuda_if_possible(Y_hat[2])\n",
    "\n",
    "    # get labels from data\n",
    "    Y_throttle = to_cuda_if_possible(data[\"throttle\"].float())\n",
    "    Y_steer = to_cuda_if_possible(data[\"steer\"].float())\n",
    "    Y_brake = to_cuda_if_possible(data[\"brake\"].float())\n",
    "\n",
    "    # Calculate Loss\n",
    "    loss_throttle = 0.5*criterion(Y_hat_throttle, Y_throttle)\n",
    "    loss_steer = 0.45*criterion(Y_hat_steer, Y_steer)\n",
    "    loss_brake = 0.05*criterion(Y_hat_brake, Y_brake)\n",
    "    loss = sum([loss_throttle, loss_steer, loss_brake])\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "992a3786",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss and Optimizer\n",
    "criterion = nn.L1Loss() ##nn.MSELoss() ##  # Easy to interpret #\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.0001) #,weight_decay=1e-5 #optim.SGD(net.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f7f3d66",
   "metadata": {},
   "source": [
    "## Model Trainer Prototype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "bd3eb22e",
   "metadata": {},
   "outputs": [],
   "source": [
    "speed_mean = 2.382234##2.250456762830466\n",
    "speed_std = 1.724884##0.30215840254891313"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e39590c3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "\n",
      "Epoch [1/1], Step [0/3664], Loss: 0.2570\n",
      "Epoch [1/1], Step [1/3664], Loss: 0.2890\n",
      "Epoch [1/1], Step [2/3664], Loss: 0.2577\n",
      "Epoch [1/1], Step [3/3664], Loss: 0.2166\n",
      "Epoch [1/1], Step [4/3664], Loss: 0.2165\n",
      "Epoch [1/1], Step [5/3664], Loss: 0.2486\n",
      "Epoch [1/1], Step [6/3664], Loss: 0.2551\n",
      "Epoch [1/1], Step [7/3664], Loss: 0.2362\n",
      "Epoch [1/1], Step [8/3664], Loss: 0.2302\n",
      "Epoch [1/1], Step [9/3664], Loss: 0.2193\n",
      "Epoch [1/1], Step [10/3664], Loss: 0.2232\n",
      "Epoch [1/1], Step [11/3664], Loss: 0.2242\n",
      "Epoch [1/1], Step [12/3664], Loss: 0.2042\n",
      "Epoch [1/1], Step [13/3664], Loss: 0.2188\n",
      "Epoch [1/1], Step [14/3664], Loss: 0.2324\n",
      "Epoch [1/1], Step [15/3664], Loss: 0.2263\n",
      "Epoch [1/1], Step [16/3664], Loss: 0.2316\n",
      "Epoch [1/1], Step [17/3664], Loss: 0.2108\n",
      "Epoch [1/1], Step [18/3664], Loss: 0.2102\n",
      "Epoch [1/1], Step [19/3664], Loss: 0.2127\n",
      "Epoch [1/1], Step [20/3664], Loss: 0.2123\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    626\u001b[0m                 \u001b[1;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    627\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore[call-arg]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 628\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    629\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    630\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[1;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    669\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    670\u001b[0m         \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 671\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    672\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    673\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     56\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 58\u001b[1;33m                 \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     59\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     56\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 58\u001b[1;33m                 \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     59\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\OneDrive\\UNI\\Master\\WS22\\APP-RAS\\Programming\\data_pipeline\\dataset.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     52\u001b[0m         \u001b[0midx_lagged\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0midx\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mseq_len\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m         \u001b[0msample\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 54\u001b[1;33m         \u001b[0msample\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"idx\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0midx_lagged\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0midx\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     55\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0minput_idx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdf_meta_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "n_epochs = 1\n",
    "print_every = 1\n",
    "valid_loss_min = 0.0343#np.Inf\n",
    "val_loss = []\n",
    "train_loss = []\n",
    "total_step = len(train_dataloader)\n",
    "\n",
    "nan_batches = []\n",
    "\n",
    "run = True\n",
    "\n",
    "for epoch in range(1, n_epochs+1):\n",
    "    \n",
    "    running_loss = 0.0\n",
    "    print(f'Epoch {epoch}\\n')\n",
    "    \n",
    "    # Work through batches\n",
    "    for batch_idx, data in enumerate(train_dataloader): #data: (['idx', 'rgb', 'speed', 'steer', 'throttle', 'brake'])\n",
    "\n",
    "        loss = forward_pass(data)\n",
    "        \n",
    "        # Backprop\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        loss_value = loss.item()\n",
    "        \n",
    "        \"\"\"\n",
    "        if np.isnan(loss_value):\n",
    "            print(\"nan\", batch_idx)\n",
    "            nan_batches.append((batch_idx, data))\n",
    "         \"\"\"   \n",
    "        \n",
    "        running_loss += loss_value\n",
    "        if (batch_idx) % print_every == 0:\n",
    "            print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n",
    "                   .format(epoch, n_epochs, batch_idx, total_step, loss_value ))\n",
    "\n",
    "        \"\"\"\n",
    "        et = time.time()\n",
    "        print(et-at)\n",
    "        at = time.time()\n",
    "        \"\"\"\n",
    "\n",
    "        \n",
    "    # Epoch finished, evaluate network and save if network_learned\n",
    "    train_loss.append(running_loss/total_step)\n",
    "    print(f'\\ntrain-loss: {np.mean(train_loss):.4f},') # TODO SOLVE NAN ISSUES\n",
    "    batch_loss = 0\n",
    "\n",
    "    \n",
    "    # Evaluation on Test set, skipped for now\n",
    "\n",
    "    with torch.no_grad():\n",
    "        net.eval()\n",
    "        \n",
    "        val_total_step = len(test_dataloader)\n",
    "        \n",
    "        for batch_idx, data in enumerate(test_dataloader):\n",
    "            \n",
    "            loss = forward_pass(data, False)\n",
    "            \n",
    "            loss_value = loss.item()\n",
    "            \n",
    "            if (batch_idx) % print_every == 0:\n",
    "                print ('Validation [{}/{}], Step [{}/{}], Loss: {:.4f}'\n",
    "                       .format(epoch, n_epochs, batch_idx, val_total_step, loss_value ))\n",
    "            \n",
    "            batch_loss += loss_value\n",
    "        val_loss.append(batch_loss/len(test_dataloader))\n",
    "        mean_val_loss = np.mean(val_loss)\n",
    "        \n",
    "        print(f'validation loss: {mean_val_loss:.4f}, \\n') # TODO SOLVE NAN ISSUES\n",
    "\n",
    "        network_learned = mean_val_loss < valid_loss_min\n",
    "        if network_learned:\n",
    "            valid_loss_min = mean_val_loss\n",
    "            #torch.save(net.state_dict(), 'resnet'+\"_E-\"+str(epoch)+'.pth')\n",
    "            torch.save(net.state_dict(), \"resnet_E-6.pth\")\n",
    "            print('Improvement-Detected, save-model')\n",
    "\n",
    "    # Back to training\n",
    "    net.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5174600c",
   "metadata": {},
   "source": [
    "### TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9cf2ee5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
    "iterator = iter(test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1b39439e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = next(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f0ca7c32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 3, 160, 960])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rgb = torch.squeeze(data[\"rgb_prep\"])[:,[2,1,0]]\n",
    "np.shape(rgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "00a3f0c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "rgb = data[\"rgb_prep\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "038ea0b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 1, 3, 160, 960])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(rgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dd431e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "rgb = torch.squeeze(rgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3327a959",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 3, 160, 960])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(rgb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a85942e9",
   "metadata": {},
   "source": [
    "### Test predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "09ecda41",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
    "iterator = iter(test_dataloader)\n",
    "#print(next(iter(test_dataloader)).keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b96dc55",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = next(iterator)\n",
    "#data\n",
    "\n",
    "X_rgb = transform_norm(torch.squeeze(data[\"rgb\"])).float().to(device)\n",
    "labels = data[\"command\"]\n",
    "labels = torch.where(labels == -1, torch.tensor(0), labels).to(torch.int64) # Replace by -1 by 0\n",
    "# Convert the labels to a one hot encoded tensor\n",
    "one_hot = torch.nn.functional.one_hot(labels, num_classes=7).to(device)\n",
    "X_cmd = torch.squeeze(one_hot).float().to(device)\n",
    "X_spd = ((data[\"speed\"]-speed_mean)/speed_std).float().to(device)\n",
    "#print(np.mean(X_spd.cpu().numpy()))\n",
    "\n",
    "target_ = (data[\"throttle\"], data[\"steer\"], data[\"brake\"])\n",
    "with torch.no_grad():\n",
    "    net.eval()\n",
    "    outputs_ = net(X_rgb, X_cmd, X_spd)\n",
    "    \n",
    "# Durchschnittlicher abs. fehler\n",
    "for i in [0,1,2]:\n",
    "    print(np.mean(abs(outputs_[i].cpu().numpy()-target_[i].cpu().numpy())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51b81953",
   "metadata": {},
   "outputs": [],
   "source": [
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "015a3ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.round(outputs_[0].cpu().numpy()-target_[0].cpu().numpy(),2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8c4f638",
   "metadata": {},
   "source": [
    "Bias Variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "074bc0f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variance \n",
    "\n",
    "for i in [0,1,2]:\n",
    "    outputs = (outputs_[i].cpu().numpy())\n",
    "    #print(outputs)\n",
    "    mean_outputs = np.mean(outputs_[i].cpu().numpy())\n",
    "    #print(mean_outputs)\n",
    "    diff = (outputs-mean_outputs)**2\n",
    "    #print(diff)\n",
    "    value = np.mean(diff)\n",
    "    print(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4da6a585",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bias\n",
    "for i in [0,1,2]:\n",
    "    targets = (target_[i].cpu().numpy())\n",
    "    #print(outputs)\n",
    "    mean_outputs = np.mean(outputs_[i].cpu().numpy())\n",
    "    #print(mean_outputs)\n",
    "    diff = outputs-mean_outputs\n",
    "    #print(diff)\n",
    "    value = np.mean(diff)\n",
    "    print(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d580060",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in [0,1,2]:\n",
    "    print(np.mean(abs(target_[i].cpu().numpy())))\n",
    "    print(np.std(abs(target_[i].cpu().numpy())))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fd372c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "i =0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88953edc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(np.round(outputs_[i].cpu().numpy(),1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd39c1d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.round(target_[i].cpu().numpy(),1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ab4158b",
   "metadata": {},
   "source": [
    "### IMG Processing\n",
    "\n",
    "BGR is now standard FOR carla agent and training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39dbd043",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "idx, batch = next(enumerate(test_dataloader))\n",
    "print(batch[\"rgb\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd55e6d5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "img = batch[\"rgb\"][0]#.shape\n",
    "img = img.numpy().astype(np.uint8).reshape(160,960,3)\n",
    "print(img.shape)\n",
    "\n",
    "#img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB) # NUR HIER, NICHT IN CARLA AGENT\n",
    "print(img.shape)\n",
    "print(type(img))\n",
    "transform = transforms.Compose([transforms.ToPILImage()])\n",
    "\n",
    "tensor = transform(img)\n",
    "\n",
    "#print(type(tensor))\n",
    "\n",
    "tensor.show()\n",
    "\n",
    "#torch.tensor(tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20307478",
   "metadata": {},
   "outputs": [],
   "source": [
    "pil_img = img.astype(np.uint8).reshape(160,960,3)\n",
    "transform = transforms.Compose([transforms.ToPILImage()])\n",
    "print(pil_img.shape)\n",
    "pil_img = transform(pil_img)\n",
    "pil_img.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49b4b228",
   "metadata": {},
   "source": [
    "TEST Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aec50160",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor = transform_norm(torch.squeeze(data[\"rgb\"],1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48a21ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor = torch.squeeze(transform_norm(data[\"rgb\"])).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "683d06c0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(64):\n",
    "    print(np.mean(tensor.numpy()[i], axis = (1,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfcd1c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(tensor.numpy(), axis = (0,2,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acc09d0c",
   "metadata": {},
   "source": [
    "### adding Navigation and speed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c13bbeae",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "iterator = iter(test_dataloader)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56111d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = next(iterator)\n",
    "#data[\"speed\"]\n",
    "#data[\"command\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59fd7382",
   "metadata": {},
   "source": [
    "Command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22295af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Assume labels is a 1D tensor with values from 0 to 6\n",
    "\n",
    "labels = data[\"command\"]\n",
    "labels = torch.where(labels == -1, torch.tensor(0), labels) # Replace by -1 by 0\n",
    "labels = labels.to(torch.int64)\n",
    "\n",
    "# Convert the labels to a one hot encoded tensor\n",
    "one_hot = torch.nn.functional.one_hot(labels, num_classes=7)\n",
    "one_hot = torch.squeeze(one_hot)\n",
    "\n",
    "print(one_hot.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "799272fb",
   "metadata": {},
   "source": [
    "Speed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce212d44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calc mean over trainingsset\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "print(len(train_dataloader))\n",
    "iterator = iter(train_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8c1a946",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "summe = []\n",
    "for batch_idx, data in enumerate(train_dataloader):\n",
    "    #print(data)\n",
    "    if i % 100 == 0:\n",
    "        print(i)\n",
    "    summe.append(np.mean(data[\"speed\"].numpy()))\n",
    "    i += 1\n",
    "    if i >= 1000:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95cd4153",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(summe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de90cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.mean(summe)) # 2.2078979146598274\n",
    "print(np.std(summe)) # 0.22455625005948113\n",
    "speed_mean = np.mean(summe)\n",
    "speed_std = np.std(summe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19f20dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iterator)\n",
    "#print(np.round(batch[\"speed\"].numpy(),2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd855f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "(batch[\"speed\"]-speed_mean)/speed_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca1cce31",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(((batch[\"speed\"]-speed_mean)/speed_std).numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd7c659b",
   "metadata": {},
   "source": [
    "### Class imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd6c193a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calc mean over trainingsset\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "print(len(train_dataloader))\n",
    "iterator = iter(train_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a92caf74",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "steer = []\n",
    "throttle = []\n",
    "brake = []\n",
    "for batch_idx, data in enumerate(train_dataloader):\n",
    "    #print(data)\n",
    "    if i % 100 == 0:\n",
    "        print(i)\n",
    "    steer.append(np.mean(data[\"steer\"].numpy()))\n",
    "    throttle.append(np.mean(data[\"throttle\"].numpy()))\n",
    "    brake.append(np.mean(data[\"brake\"].numpy()))\n",
    "    i += 1\n",
    "    if i >= 100:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0b198de",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.mean(steer))\n",
    "print(np.mean(throttle))\n",
    "print(np.mean(brake))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7177fc92",
   "metadata": {},
   "source": [
    "### Vanishing/Exploding Gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac119fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "for name, param in net.thr_head.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(name, param.data.cpu().numpy())\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68ba6bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, param in net.spd_input.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(name, np.max(abs(param.data.cpu().numpy())))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd0f44f0",
   "metadata": {},
   "source": [
    "## Saving and Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2eecda0",
   "metadata": {},
   "source": [
    "Not suited for leaderboard agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f516f764",
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.save(net, 'rgb_resnet.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86a81a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#net = torch.load('rgb_resnet.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4e10b82",
   "metadata": {},
   "source": [
    "suited for leaderboard agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55596f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.save(net.state_dict(), \"pretrained_1E.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccc5d256",
   "metadata": {},
   "outputs": [],
   "source": [
    "#net = MyResnet()\n",
    "#net.load_state_dict(torch.load(\"resnet_E-4.pth\"))\n",
    "#net.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8568190",
   "metadata": {},
   "source": [
    "## Testing Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bef6d6ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
    "idx, X = next(enumerate(test_dataloader))\n",
    "img = transform_norm(X[\"rgb\"])\n",
    "img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "339784d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.squeeze(img,1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2e33ce4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1674333664.0873091\n",
      "1.7781405448913574\n",
      "1.7159996032714844\n",
      "1.7738769054412842\n",
      "1.708998680114746\n",
      "1.6113920211791992\n",
      "1.6854381561279297\n",
      "1.8790006637573242\n",
      "1.6829984188079834\n",
      "1.7120006084442139\n",
      "1.6170008182525635\n",
      "1.8352971076965332\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_7996\\2554019828.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m#print(len(test_dataloader))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_dataloader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m     \u001b[1;31m# further preprocessing\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[1;31m#X_rgb = transform_norm(torch.squeeze(data[\"rgb_prep\"])).float()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    626\u001b[0m                 \u001b[1;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    627\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore[call-arg]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 628\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    629\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    630\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[1;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    669\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    670\u001b[0m         \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 671\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    672\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    673\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     56\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 58\u001b[1;33m                 \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     59\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     56\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 58\u001b[1;33m                 \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     59\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\OneDrive\\UNI\\Master\\WS22\\APP-RAS\\Programming\\data_pipeline\\dataset.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     74\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mdata_point_idx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0midx_lagged\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0midx\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m                 \u001b[0mfile_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_file_path_from_df\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_idx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata_point_idx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 76\u001b[1;33m                 \u001b[0mdata_t\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_data_from_path\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     77\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0mmeas_idx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmeas\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mused_measurements\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     78\u001b[0m                     \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mmeas_idx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0midx_array\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata_t\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mmeas\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\OneDrive\\UNI\\Master\\WS22\\APP-RAS\\Programming\\data_pipeline\\dataset.py\u001b[0m in \u001b[0;36mload_data_from_path\u001b[1;34m(self, path)\u001b[0m\n\u001b[0;32m    116\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    117\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mfile_format\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\".json\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 118\u001b[1;33m             \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'r'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    119\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    120\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mfile_format\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\".npy\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\_bootlocale.py\u001b[0m in \u001b[0;36mgetpreferredencoding\u001b[1;34m(do_setlocale)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplatform\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"win\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m     \u001b[1;32mdef\u001b[0m \u001b[0mgetpreferredencoding\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdo_setlocale\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflags\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutf8_mode\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[1;34m'UTF-8'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "#print(len(test_dataloader))\n",
    "at = 0\n",
    "for batch_idx, data in enumerate(test_dataloader):\n",
    "    # further preprocessing\n",
    "    #X_rgb = transform_norm(torch.squeeze(data[\"rgb_prep\"])).float()\n",
    "    X_rgb = torch.squeeze(data[\"rgb_prep\"])[:,[2,1,0]].float() # BGR --> RGB\n",
    "    \"\"\"\n",
    "    if False:#augument:\n",
    "        X_rgb = transform_augument(X_rgb)\n",
    "    \"\"\"\n",
    "    labels = data[\"command\"]\n",
    "    labels = torch.where(labels == -1, torch.tensor(0), labels).to(torch.int64) # Replace by -1 by 0\n",
    "    # Convert the labels to a one hot encoded tensor\n",
    "    one_hot = torch.nn.functional.one_hot(labels, num_classes=7)\n",
    "    X_cmd = torch.squeeze(one_hot).float()\n",
    "    X_spd = ((data[\"speed\"]-speed_mean)/speed_std).float()\n",
    "    \n",
    "    Y_throttle = data[\"throttle\"].float()\n",
    "    Y_steer = data[\"steer\"].float()\n",
    "    Y_brake = data[\"brake\"].float()\n",
    "\n",
    "    # move to GPU\n",
    "    X_rgb = to_cuda_if_possible(X_rgb)\n",
    "    X_cmd = to_cuda_if_possible(X_cmd)\n",
    "    X_spd = to_cuda_if_possible(X_spd)\n",
    "    \n",
    "    Y_throttle = to_cuda_if_possible(Y_throttle)\n",
    "    Y_steer = to_cuda_if_possible(Y_steer)\n",
    "    Y_brake = to_cuda_if_possible(Y_brake)\n",
    "\n",
    "    \n",
    "    et = time.time()\n",
    "    print(et-at)\n",
    "    at = time.time()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c96bcd38",
   "metadata": {},
   "source": [
    "#### Training secounds per batch\n",
    "\n",
    "1.8778209686279297\n",
    "1.7550039291381836\n",
    "1.9759962558746338\n",
    "2.018435001373291"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "613a0284",
   "metadata": {},
   "source": [
    "#### Only dataloader and preprocessing secounds per batch\n",
    "1.235999584197998\n",
    "1.2680015563964844\n",
    "1.3483715057373047\n",
    "1.2585253715515137\n",
    "1.1267704963684082\n",
    "1.124000072479248\n",
    "1.2410008907318115\n",
    "1.2269997596740723\n",
    "1.200000286102295\n",
    "1.267998456954956\n",
    "1.2166194915771484\n",
    "1.2077960968017578\n",
    "1.2405602931976318\n",
    "1.2019383907318115"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c69e5af7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "110it [02:39,  1.45s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_7996\\754676744.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtqdm\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mtest_dataloader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_dataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_dataloader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tqdm\\std.py\u001b[0m in \u001b[0;36m__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1193\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1194\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1195\u001b[1;33m             \u001b[1;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[1;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1196\u001b[0m                 \u001b[1;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1197\u001b[0m                 \u001b[1;31m# Update and possibly print the progressbar.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    626\u001b[0m                 \u001b[1;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    627\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore[call-arg]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 628\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    629\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    630\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[1;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    669\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    670\u001b[0m         \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 671\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    672\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    673\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     56\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 58\u001b[1;33m                 \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     59\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     56\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 58\u001b[1;33m                 \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     59\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\OneDrive\\UNI\\Master\\WS22\\APP-RAS\\Programming\\data_pipeline\\dataset.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     62\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mdata_point_idx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0midx_lagged\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0midx\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m                 \u001b[0mfile_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_file_path_from_df\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_idx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata_point_idx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 64\u001b[1;33m                 \u001b[0mdata_t\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_data_from_path\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     65\u001b[0m                 \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx_array\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata_t\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m                 \u001b[0midx_array\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\OneDrive\\UNI\\Master\\WS22\\APP-RAS\\Programming\\data_pipeline\\dataset.py\u001b[0m in \u001b[0;36mload_data_from_path\u001b[1;34m(self, path)\u001b[0m\n\u001b[0;32m    119\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    120\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mfile_format\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\".npy\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 121\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mallow_pickle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    122\u001b[0m             \u001b[1;31m# if lidar\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    123\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\numpy\\lib\\npyio.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(file, mmap_mode, allow_pickle, fix_imports, encoding)\u001b[0m\n\u001b[0;32m    438\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mformat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen_memmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmmap_mode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    439\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 440\u001b[1;33m                 return format.read_array(fid, allow_pickle=allow_pickle,\n\u001b[0m\u001b[0;32m    441\u001b[0m                                          pickle_kwargs=pickle_kwargs)\n\u001b[0;32m    442\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\numpy\\lib\\format.py\u001b[0m in \u001b[0;36mread_array\u001b[1;34m(fp, allow_pickle, pickle_kwargs)\u001b[0m\n\u001b[0;32m    755\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misfileobj\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    756\u001b[0m             \u001b[1;31m# We can use the fast fromfile() function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 757\u001b[1;33m             \u001b[0marray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfromfile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcount\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcount\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    758\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    759\u001b[0m             \u001b[1;31m# This is not a real file. We have to read it the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "for batch_idx, data in tqdm(enumerate(test_dataloader)):\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "258399f1",
   "metadata": {},
   "source": [
    "# asdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "314eb467",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# LÖSCHEN\n",
    "root_dir = \"D:\\\\data\\\\data\"\n",
    "keep_input = [\"lidar\", \"rgb\", \"measurements\"] # \"lidar\"\n",
    "\n",
    "def move_unused_sensors_to_new_folder(root_dir, keep_input):\n",
    "    for (root, dirs, files) in os.walk(root_dir, topdown=True):\n",
    "        # Current folder contains the files\n",
    "        if not dirs:\n",
    "            dir, input_type = os.path.split(root)\n",
    "            if input_type not in keep_input:\n",
    "                path_parts = root.split(os.sep)\n",
    "                idx_data_first = path_parts.index(\"data\")\n",
    "                path_parts[idx_data_first + 1] += \" unused\"\n",
    "                dir_new = os.path.join(*path_parts)\n",
    "                if not os.path.exists(dir_new):\n",
    "                    os.makedirs(dir_new)\n",
    "                shutil.move(root, dir_new)\n",
    "                \n",
    "import shutil\n",
    "move_unused_sensors_to_new_folder(root_dir,keep_input)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dec5fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saves preped data in same folder structure under rgb_prep\n",
    "def rgb_to_disk_2(format):\n",
    "    assert format in [\".npy\", \".npz\", \".pt\"]\n",
    "    fn_save = np.save if format == \".npy\" else np.savez_compressed\n",
    "    # save npy/ npz\n",
    "    df_meta = dataset.df_meta_data\n",
    "    for idx in tqdm(range(len(df_meta))):\n",
    "        path_parts = dataset.df_meta_data[\"dir\"][idx].split(os.sep)\n",
    "        # path_parts[path_parts.index(\"data\") + 1] += \"_prep_npy\"\n",
    "        dir_name_zip = os.path.join(*path_parts, \"rgb_prep\")\n",
    "        if not os.path.exists(dir_name_zip):\n",
    "            os.makedirs(dir_name_zip)\n",
    "            # shutil.copytree(os.path.join(dataset.df_meta_data[\"dir\"][idx], \"measurements\"), os.path.join(*path_parts, \"measurements\"))\n",
    "        path = os.path.join(df_meta.iloc[idx][0], \"rgb\", df_meta.iloc[idx][1])\n",
    "        img_np = dataset.load_data_from_path(path)\n",
    "        img_torch = torch.Tensor(img_np)\n",
    "        img_torch_prep = preprocessing[\"rgb\"](img_torch)\n",
    "        img_np_prep = img_torch_prep.numpy()\n",
    "        filename_np = os.path.join(dir_name_zip, f\"{df_meta.iloc[idx]['rgb'].split('.')[0]}{format}\")\n",
    "        # torch.save(img_torch_prep, filename_torch)\n",
    "        with open(filename_np, 'wb') as f:\n",
    "            fn_save(f, img_np_prep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d21bf30d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = \"D:\\\\data\\\\Train\"\n",
    "\n",
    "config = {\"used_inputs\": [\"rgb\",\"measurements\"], \n",
    "        \"used_measurements\": [\"speed\", \"steer\", \"throttle\", \"brake\", \"command\"],\n",
    "        \"seq_len\": 1\n",
    "        }\n",
    "\n",
    "dataset = CARLADataset(root_dir=train_path, config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13506ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "rgb_to_disk_2(\".npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "731e4a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.df_meta_data.loc[0][\"dir\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
