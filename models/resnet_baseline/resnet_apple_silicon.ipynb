{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Model ResNet\n","\n","https://www.pluralsight.com/guides/introduction-to-resnet"]},{"cell_type":"markdown","metadata":{},"source":["#### TODOS\n","1. DONE Debugging, does output make sense?\n","    1. Resize images\n","    2. preprocessing fixes\n","    5. replace scaling by proper function\n","2. try on leaderboard\n","3. Include Odometry and fuse into heads\n","    - Speed\n","    - Location\n","4. navigation\n","5. controller\n","6. Evaluation on Test set, Modularization\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["## Dependencies"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2023-01-07T22:36:44.452818Z","iopub.status.busy":"2023-01-07T22:36:44.452400Z","iopub.status.idle":"2023-01-07T22:36:46.324881Z","shell.execute_reply":"2023-01-07T22:36:46.323967Z","shell.execute_reply.started":"2023-01-07T22:36:44.452782Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/Users/julianvonklitzing/miniforge3/envs/carla/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n","  from .autonotebook import tqdm as notebook_tqdm\n"]}],"source":["# MODEL STUFF\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","#import torch.nn.functional as F\n","import numpy as np\n","import torchvision\n","from torchvision import *\n","from torch.utils.data import Dataset, DataLoader\n","from PIL import Image\n","from torchvision import transforms\n","\n","# GENERAL STUFF\n","import time\n","import copy\n","import os\n","import sys\n","from tqdm import tqdm"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2023-01-07T22:37:01.859175Z","iopub.status.busy":"2023-01-07T22:37:01.858549Z","iopub.status.idle":"2023-01-07T22:37:02.026285Z","shell.execute_reply":"2023-01-07T22:37:02.025423Z","shell.execute_reply.started":"2023-01-07T22:37:01.859133Z"},"trusted":true},"outputs":[],"source":["sys.path.append(\"../data_pipeline\")\n","# import data_sampler, dataset\n","from data_sampler import WeightedSampler\n","from dataset import CARLADataset#, CARLADatasetMultiProcessing\n","from utils import train_test_split, create_metadata_df"]},{"cell_type":"markdown","metadata":{},"source":["## Model"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2023-01-07T22:37:04.125251Z","iopub.status.busy":"2023-01-07T22:37:04.124871Z","iopub.status.idle":"2023-01-07T22:37:04.136063Z","shell.execute_reply":"2023-01-07T22:37:04.134948Z","shell.execute_reply.started":"2023-01-07T22:37:04.125218Z"},"trusted":true},"outputs":[],"source":["class MyResnet(nn.Module):\n","    \n","    def __init__(self):\n","        super().__init__()\n","\n","        \n","        # ResNet Architecture with pretrained weights, also bigger resnets available\n","        self.net = torchvision.models.resnet18(pretrained=True) # weights=True\n","        num_ftrs = self.net.fc.in_features\n","\n","        # Top layer of ResNet which you can modify. We choose Identity to use it as Input for all the heads\n","        self.net.fc = nn.Identity()\n","        \n","        # Input Layer fuer cmd, spd\n","        self.cmd_input = nn.Sequential(\n","            nn.Linear(7, 7),\n","            nn.LeakyReLU() # TODO\n","        )\n","        \n","        self.spd_input = nn.Sequential(\n","            nn.Linear(1, 1),\n","            nn.LeakyReLU() # TODO\n","        )\n","        \n","        # Regression Heads for Throttle, Brake and Steering\n","        self.thr_head = nn.Sequential(\n","            nn.Linear(num_ftrs+8, 1),\n","            nn.Sigmoid() # [0,1] Range Output\n","        )\n","        \n","        self.brk_head = nn.Sequential(\n","            nn.Linear(num_ftrs+8, 1),\n","            nn.Sigmoid() # [0,1] Range Output\n","        )\n","        \n","        self.str_head = nn.Sequential(\n","            nn.Linear(num_ftrs+8, 1),\n","            nn.Tanh() # [-1,1] Range Output\n","        )\n","\n","    # Forward Pass of the Model\n","    def forward(self, rgb, cmd, spd):\n","        rgb = self.net(rgb) # BRG\n","        cmd = self.cmd_input(cmd)\n","        spd = self.spd_input(spd)\n","        \n","        x = torch.cat((rgb, cmd, spd),1)\n","        \n","        #x = self.net.fc(x)\n","        return self.thr_head(x), self.str_head(x), self.brk_head(x) # 3 Outputs since we have 3 Heads"]},{"cell_type":"markdown","metadata":{},"source":["## Data Loaders, Data Sets"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2023-01-07T22:41:24.468678Z","iopub.status.busy":"2023-01-07T22:41:24.468305Z","iopub.status.idle":"2023-01-07T22:41:40.975176Z","shell.execute_reply":"2023-01-07T22:41:40.974105Z","shell.execute_reply.started":"2023-01-07T22:41:24.468646Z"},"trusted":true},"outputs":[],"source":["# path_data = \"../data/Dataset Ege/Dataset Ege 1\"\n","path_data = os.path.join(\"..\", \"data\", \"data\")\n","\n","# train_path = \"../input\" #data must be shared publically, doing local for now\n","# test_path = \"../input\"\n","\n","config = {\"used_inputs\": [\"rgb\",\"measurements\"], \n","        \"used_measurements\": [\"speed\", \"steer\", \"throttle\", \"brake\", \"command\"],\n","        \"seq_len\": 1\n","        }\n","\n","df_meta_data = create_metadata_df(path_data, config[\"used_inputs\"])\n","df_meta_data_train, df_meta_data_test = train_test_split(df_meta_data, towns={\"train\": [\"Town04\", \"Town06\"], \"test\": [\"Town05\"]})\n","# df_meta_data_train, df_meta_data_test = df_meta_data, df_meta_data\n","\n","train_dataset = CARLADataset(root_dir=path_data, df_meta_data=df_meta_data_train, config=config)\n","test_dataset = CARLADataset(root_dir=path_data, df_meta_data=df_meta_data_test, config=config)\n","\n","# weighted_sampler = WeightedSampler(dataset=train_dataset)\n","\n","batch_size = 64\n","train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2023-01-07T22:41:40.977595Z","iopub.status.busy":"2023-01-07T22:41:40.977155Z","iopub.status.idle":"2023-01-07T22:41:40.983946Z","shell.execute_reply":"2023-01-07T22:41:40.982977Z","shell.execute_reply.started":"2023-01-07T22:41:40.977559Z"},"trusted":true},"outputs":[{"data":{"text/plain":["13"]},"execution_count":13,"metadata":{},"output_type":"execute_result"}],"source":["len(train_dataloader)"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2023-01-07T22:41:40.986445Z","iopub.status.busy":"2023-01-07T22:41:40.985667Z","iopub.status.idle":"2023-01-07T22:41:40.995423Z","shell.execute_reply":"2023-01-07T22:41:40.994274Z","shell.execute_reply.started":"2023-01-07T22:41:40.986404Z"},"trusted":true},"outputs":[{"data":{"text/plain":["13"]},"execution_count":14,"metadata":{},"output_type":"execute_result"}],"source":["len(test_dataloader)"]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2023-01-07T22:41:58.474485Z","iopub.status.busy":"2023-01-07T22:41:58.473791Z","iopub.status.idle":"2023-01-07T22:41:58.480087Z","shell.execute_reply":"2023-01-07T22:41:58.479000Z","shell.execute_reply.started":"2023-01-07T22:41:58.474448Z"},"trusted":true},"outputs":[],"source":["mean = torch.tensor([79.6657, 81.5673, 105.6161])\n","std = torch.tensor([66.8309, 60.1001, 66.2220])\n","\n","\n","transform_norm = transforms.Compose([\n","    transforms.Normalize(mean, std)\n","])\n"]},{"cell_type":"markdown","metadata":{},"source":["## Training"]},{"cell_type":"code","execution_count":21,"metadata":{"execution":{"iopub.execute_input":"2023-01-07T22:41:59.079464Z","iopub.status.busy":"2023-01-07T22:41:59.078769Z","iopub.status.idle":"2023-01-07T22:42:00.233882Z","shell.execute_reply":"2023-01-07T22:42:00.232905Z","shell.execute_reply.started":"2023-01-07T22:41:59.079425Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["mps\n"]},{"name":"stderr","output_type":"stream","text":["/Users/julianvonklitzing/miniforge3/envs/carla/lib/python3.8/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n","  warnings.warn(\n","/Users/julianvonklitzing/miniforge3/envs/carla/lib/python3.8/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n","  warnings.warn(msg)\n"]},{"data":{"text/plain":["MyResnet(\n","  (net): ResNet(\n","    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n","    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (relu): ReLU(inplace=True)\n","    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n","    (layer1): Sequential(\n","      (0): BasicBlock(\n","        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","      (1): BasicBlock(\n","        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","    (layer2): Sequential(\n","      (0): BasicBlock(\n","        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (downsample): Sequential(\n","          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        )\n","      )\n","      (1): BasicBlock(\n","        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","    (layer3): Sequential(\n","      (0): BasicBlock(\n","        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (downsample): Sequential(\n","          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        )\n","      )\n","      (1): BasicBlock(\n","        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","    (layer4): Sequential(\n","      (0): BasicBlock(\n","        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (downsample): Sequential(\n","          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        )\n","      )\n","      (1): BasicBlock(\n","        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n","    (fc): Identity()\n","  )\n","  (cmd_input): Sequential(\n","    (0): Linear(in_features=7, out_features=7, bias=True)\n","    (1): LeakyReLU(negative_slope=0.01)\n","  )\n","  (spd_input): Sequential(\n","    (0): Linear(in_features=1, out_features=1, bias=True)\n","    (1): LeakyReLU(negative_slope=0.01)\n","  )\n","  (thr_head): Sequential(\n","    (0): Linear(in_features=520, out_features=1, bias=True)\n","    (1): Sigmoid()\n","  )\n","  (brk_head): Sequential(\n","    (0): Linear(in_features=520, out_features=1, bias=True)\n","    (1): Sigmoid()\n","  )\n","  (str_head): Sequential(\n","    (0): Linear(in_features=520, out_features=1, bias=True)\n","    (1): Tanh()\n","  )\n",")"]},"execution_count":21,"metadata":{},"output_type":"execute_result"}],"source":["# Initialise Model (GPU or CPU)\n","device = torch.device('cuda:0' if torch.cuda.is_available() else 'mps' if torch.has_mps else 'cpu')\n","print(device)\n","net = MyResnet()\n","net.to(device)"]},{"cell_type":"code","execution_count":22,"metadata":{"execution":{"iopub.execute_input":"2023-01-07T22:42:02.543372Z","iopub.status.busy":"2023-01-07T22:42:02.542907Z","iopub.status.idle":"2023-01-07T22:42:02.549078Z","shell.execute_reply":"2023-01-07T22:42:02.548087Z","shell.execute_reply.started":"2023-01-07T22:42:02.543336Z"},"trusted":true},"outputs":[],"source":["def to_cuda_if_possible(data):\n","    return data.to(device) if device else data"]},{"cell_type":"code","execution_count":23,"id":"cf33d999","metadata":{"execution":{"iopub.execute_input":"2023-01-07T22:42:02.551235Z","iopub.status.busy":"2023-01-07T22:42:02.550450Z","iopub.status.idle":"2023-01-07T22:42:02.563283Z","shell.execute_reply":"2023-01-07T22:42:02.562229Z","shell.execute_reply.started":"2023-01-07T22:42:02.551201Z"},"trusted":true},"outputs":[],"source":["def forward_pass(data):\n","    # further preprocessing\n","    start_prep = time.time()\n","    X_rgb = torch.squeeze(transform_norm(data[\"rgb\"])).float()\n","    labels = data[\"command\"]\n","    # labels = torch.where(labels == -1, torch.tensor(0), labels).to(torch.int64) # Replace by -1 by 0\n","    labels = torch.where(labels == -1, torch.tensor(0, dtype=labels.dtype), labels).to(torch.int64) # Replace by -1 by 0\n","    # Convert the labels to a one hot encoded tensor\n","    one_hot = torch.nn.functional.one_hot(labels, num_classes=7)\n","    X_cmd = torch.squeeze(one_hot).float()\n","    X_spd = ((data[\"speed\"]-speed_mean)/speed_std).float()\n","    \n","    Y_throttle = data[\"throttle\"].float()\n","    Y_steer = data[\"steer\"].float()\n","    Y_brake = data[\"brake\"].float()\n","    end_prep = time.time()\n","    time_prep = end_prep - start_prep\n","    # move to GPU\n","    start_trans_cuda = time.time()\n","    X_rgb = to_cuda_if_possible(X_rgb)\n","    X_cmd = to_cuda_if_possible(X_cmd)\n","    X_spd = to_cuda_if_possible(X_spd)\n","    \n","    Y_throttle = to_cuda_if_possible(Y_throttle)\n","    Y_steer = to_cuda_if_possible(Y_steer)\n","    Y_brake = to_cuda_if_possible(Y_brake)\n","    end_trans_cuda = time.time()\n","    time_trans_cuda = end_trans_cuda - start_trans_cuda\n","    # compute outputs\n","    start_forward = time.time()\n","    optimizer.zero_grad()\n","    Y_hat = net(X_rgb, X_cmd, X_spd)\n","    end_forward = time.time()\n","    time_forward = end_forward - start_forward\n","    start_trans_cuda_2 = time.time()\n","    Y_hat_throttle = to_cuda_if_possible(Y_hat[0])\n","    Y_hat_steer = to_cuda_if_possible(Y_hat[1])\n","    Y_hat_brake = to_cuda_if_possible(Y_hat[2])\n","\n","    # get labels from data\n","    Y_throttle = to_cuda_if_possible(data[\"throttle\"].float())\n","    Y_steer = to_cuda_if_possible(data[\"steer\"].float())\n","    Y_brake = to_cuda_if_possible(data[\"brake\"].float())\n","    end_trans_cuda_2 = time.time()\n","    time_trans_cuda_2 = end_trans_cuda_2 - start_trans_cuda_2\n","\n","    # Calculate Loss\n","    start_loss = time.time()\n","    loss_throttle = 0.7*criterion(Y_hat_throttle, Y_throttle)\n","    loss_steer = 0.2*criterion(Y_hat_steer, Y_steer)\n","    loss_brake = 0.1*criterion(Y_hat_brake, Y_brake)\n","    loss = sum([loss_throttle, loss_steer, loss_brake])\n","    end_loss = time.time()\n","    time_loss = end_loss - start_loss\n","    return loss, time_prep, time_trans_cuda, time_forward, time_trans_cuda_2, time_loss"]},{"cell_type":"code","execution_count":24,"id":"46d58d68","metadata":{"execution":{"iopub.execute_input":"2023-01-07T22:42:06.089827Z","iopub.status.busy":"2023-01-07T22:42:06.089469Z","iopub.status.idle":"2023-01-07T22:42:06.099089Z","shell.execute_reply":"2023-01-07T22:42:06.098003Z","shell.execute_reply.started":"2023-01-07T22:42:06.089796Z"},"trusted":true},"outputs":[],"source":["# Loss and Optimizer\n","criterion = nn.L1Loss() # Easy to interpret #nn.MSELoss()\n","optimizer = optim.Adam(net.parameters(), lr=0.0001) #optim.SGD(net.parameters(), lr=0.001, momentum=0.9)"]},{"cell_type":"markdown","id":"56752af1","metadata":{},"source":["## Model Trainer Prototype"]},{"cell_type":"code","execution_count":25,"id":"a0a52438","metadata":{},"outputs":[{"data":{"text/plain":["'\\nChecking time consumption in training pipeline:\\n- Data Loading: cannot directly measure it because done in for loop itself and other stuff in for loop\\n- Data Preprocessing\\n- Training step\\n- Validation step\\n\\n'"]},"execution_count":25,"metadata":{},"output_type":"execute_result"}],"source":["\"\"\"\n","Checking time consumption in training pipeline:\n","- Data Loading: cannot directly measure it because done in for loop itself and other stuff in for loop\n","- Data Preprocessing\n","- Training step\n","- Validation step\n","\n","\"\"\""]},{"cell_type":"code","execution_count":26,"id":"e41d6111","metadata":{"execution":{"iopub.execute_input":"2023-01-07T22:42:06.901844Z","iopub.status.busy":"2023-01-07T22:42:06.901504Z","iopub.status.idle":"2023-01-07T22:42:06.907336Z","shell.execute_reply":"2023-01-07T22:42:06.906159Z","shell.execute_reply.started":"2023-01-07T22:42:06.901814Z"},"trusted":true},"outputs":[],"source":["speed_mean = 2.250456762830466\n","speed_std = 0.30215840254891313"]},{"cell_type":"code","execution_count":92,"id":"bbad5d60","metadata":{},"outputs":[],"source":["times_prep, times_trans_cuda, times_forward, times_trans_cuda_2, times_loss, times_backprop, times_val, times_epoch, times_monitoring = [], [], [], [], [], [], [], [], []"]},{"cell_type":"code","execution_count":93,"id":"10b25967","metadata":{"execution":{"iopub.execute_input":"2023-01-07T22:42:07.432209Z","iopub.status.busy":"2023-01-07T22:42:07.431857Z","iopub.status.idle":"2023-01-07T22:42:09.494061Z","shell.execute_reply":"2023-01-07T22:42:09.492981Z","shell.execute_reply.started":"2023-01-07T22:42:07.432180Z"},"scrolled":true,"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["<timed exec>:36: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 1\n","\n","Epoch [1/1], Step [0/13], Loss: 0.0993\n","\n","train-loss: 0.0829,\n","CPU times: user 1.35 s, sys: 657 ms, total: 2.01 s\n","Wall time: 4.48 s\n"]}],"source":["%%time\n","\n","n_epochs = 1\n","print_every = 200\n","valid_loss_min = np.Inf\n","val_loss = []\n","train_loss = []\n","total_step = len(train_dataloader)\n","\n","validate = False\n","\n","for epoch in range(1, n_epochs+1):\n","    start_epoch = time.time()\n","    \n","    running_loss = 0.0\n","    print(f'Epoch {epoch}\\n')\n","    \n","    # Work through batches\n","    for batch_idx, data in enumerate(train_dataloader):\n","        \n","        loss, time_prep, time_trans_cuda, time_forward, time_trans_cuda_2, time_loss = forward_pass(data)\n","        times_prep.append(time_prep)\n","        times_trans_cuda.append(time_trans_cuda)\n","        times_forward.append(time_forward)\n","        times_trans_cuda_2.append(time_trans_cuda_2)\n","        times_loss.append(time_loss)\n","\n","        start_backprop = time.time()\n","        # Backprop\n","        loss.backward()\n","        optimizer.step()\n","        end_backprop = time.time()\n","        time_backprop = end_backprop - start_backprop\n","        times_backprop.append(time_backprop)\n","        \n","        start_monitoring = time.time()\n","        running_loss += loss.item()\n","        if (batch_idx) % print_every is 0:\n","            print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n","                   .format(epoch, n_epochs, batch_idx, total_step, loss.item()))\n","        end_monitoring = time.time()\n","        time_monitoring = end_monitoring - start_monitoring\n","        times_monitoring.append(time_monitoring)\n","        \n","    # Epoch finished, evaluate network and save if network_learned\n","    train_loss.append(running_loss/total_step)\n","    print(f'\\ntrain-loss: {np.mean(train_loss):.4f},') # TODO SOLVE NAN ISSUES\n","    batch_loss = 0\n","\n","    \n","    # Evaluation on Test set, skipped for now\n","    \n","    start_val = time.time()\n","    if validate:\n","        with torch.no_grad():\n","            net.eval()\n","            \n","            for batch_idx, data in enumerate(test_dataloader):\n","                \n","                loss, time_prep, time_trans_cuda, time_forward, time_trans_cuda_2, time_loss = forward_pass(data)\n","                \n","                batch_loss += loss.item()\n","            val_loss.append(batch_loss/len(test_dataloader))\n","            #network_learned = batch_loss < valid_loss_min\n","            print(f'validation loss: {np.mean(val_loss):.4f}, \\n') # TODO SOLVE NAN ISSUES\n","\n","            \n","            if False:#network_learned:\n","                valid_loss_min = batch_loss\n","                torch.save(net.state_dict(), 'resnet.pt')\n","                print('Improvement-Detected, save-model')\n","    end_val = time.time()\n","    time_val = end_val - start_val\n","    times_val.append(time_val)\n","\n","    # Back to training\n","    net.train()\n","    end_epoch = time.time()\n","    time_epoch = end_epoch - start_epoch\n","    times_epoch.append(time_epoch)\n","    \n","    "]},{"cell_type":"code","execution_count":94,"id":"b59dfb82","metadata":{},"outputs":[],"source":["import pandas as pd\n","\n","df_speed_stats = pd.DataFrame({\n","\"times_prep\" : times_prep, \n","\"times_trans_cuda\" : times_trans_cuda, \n","\"times_forward\": times_forward, \n","\"times_trans_cuda_2\" : times_trans_cuda_2, \n","\"times_loss\" : times_loss, \n","\"times_backprop\" : times_backprop, \n","\"times_monitoring\" : times_monitoring, \n","})\n","df_speed_stats = df_speed_stats.sum().to_frame().T\n","df_speed_stats[\"time_val\"] = times_val[0]\n","df_speed_stats[\"time_untracked\"] = times_epoch[0] - df_speed_stats.sum().sum()\n","df_speed_stats = df_speed_stats.T\n","df_speed_stats.columns = [\"time_sec\"]\n","df_speed_stats[\"time_%\"] = df_speed_stats[\"time_sec\"] / df_speed_stats[\"time_sec\"].sum()\n","df_speed_stats = df_speed_stats.sort_values(by=\"time_%\", ascending=False)"]},{"cell_type":"code","execution_count":95,"id":"b37396a4","metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>time_sec</th>\n","      <th>time_%</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>times_monitoring</th>\n","      <td>2.222558</td>\n","      <td>0.496390</td>\n","    </tr>\n","    <tr>\n","      <th>time_untracked</th>\n","      <td>0.816905</td>\n","      <td>0.182449</td>\n","    </tr>\n","    <tr>\n","      <th>times_trans_cuda_2</th>\n","      <td>0.726366</td>\n","      <td>0.162228</td>\n","    </tr>\n","    <tr>\n","      <th>times_backprop</th>\n","      <td>0.492207</td>\n","      <td>0.109930</td>\n","    </tr>\n","    <tr>\n","      <th>times_forward</th>\n","      <td>0.098928</td>\n","      <td>0.022095</td>\n","    </tr>\n","    <tr>\n","      <th>times_prep</th>\n","      <td>0.070115</td>\n","      <td>0.015660</td>\n","    </tr>\n","    <tr>\n","      <th>times_trans_cuda</th>\n","      <td>0.038943</td>\n","      <td>0.008698</td>\n","    </tr>\n","    <tr>\n","      <th>times_loss</th>\n","      <td>0.011417</td>\n","      <td>0.002550</td>\n","    </tr>\n","    <tr>\n","      <th>time_val</th>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                    time_sec    time_%\n","times_monitoring    2.222558  0.496390\n","time_untracked      0.816905  0.182449\n","times_trans_cuda_2  0.726366  0.162228\n","times_backprop      0.492207  0.109930\n","times_forward       0.098928  0.022095\n","times_prep          0.070115  0.015660\n","times_trans_cuda    0.038943  0.008698\n","times_loss          0.011417  0.002550\n","time_val            0.000000  0.000000"]},"execution_count":95,"metadata":{},"output_type":"execute_result"}],"source":["df_speed_stats"]},{"cell_type":"code","execution_count":31,"id":"542f620d","metadata":{},"outputs":[],"source":["df_speed_stats.to_pickle(\"df_speed_stats_second.pkl\")"]},{"cell_type":"code","execution_count":null,"id":"a6b14469","metadata":{},"outputs":[],"source":["# after 16 min 400/1514 batches are finished --> ~ 74min for an epoch (Town 04/05)\n","# 56GB (entire set as Moritz trained)/ (20GB train set --> Town04/05): 2.8 * 74min = 207 min (3.5 hours)\n"]},{"cell_type":"markdown","metadata":{},"source":["### Test predictions"]},{"cell_type":"code","execution_count":60,"metadata":{"execution":{"iopub.execute_input":"2023-01-07T22:33:41.873037Z","iopub.status.busy":"2023-01-07T22:33:41.870465Z","iopub.status.idle":"2023-01-07T22:33:41.880153Z","shell.execute_reply":"2023-01-07T22:33:41.879180Z","shell.execute_reply.started":"2023-01-07T22:33:41.873001Z"},"trusted":true},"outputs":[],"source":["test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n","iterator = iter(test_dataloader)\n","#print(next(iter(test_dataloader)).keys())"]},{"cell_type":"code","execution_count":61,"metadata":{"execution":{"iopub.execute_input":"2023-01-07T22:33:41.881979Z","iopub.status.busy":"2023-01-07T22:33:41.881517Z","iopub.status.idle":"2023-01-07T22:33:41.925225Z","shell.execute_reply":"2023-01-07T22:33:41.924361Z","shell.execute_reply.started":"2023-01-07T22:33:41.881944Z"},"trusted":true},"outputs":[],"source":["data = next(iterator)\n","#data"]},{"cell_type":"code","execution_count":62,"metadata":{"execution":{"iopub.execute_input":"2023-01-07T22:33:41.928547Z","iopub.status.busy":"2023-01-07T22:33:41.928282Z","iopub.status.idle":"2023-01-07T22:33:41.961332Z","shell.execute_reply":"2023-01-07T22:33:41.958756Z","shell.execute_reply.started":"2023-01-07T22:33:41.928523Z"},"trusted":true},"outputs":[{"ename":"RuntimeError","evalue":"expected scalar type long int but found double","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_24/2500187256.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mX_rgb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransform_norm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"rgb\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"command\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint64\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Replace by -1 by 0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;31m# Convert the labels to a one hot encoded tensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mone_hot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mone_hot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: expected scalar type long int but found double"]}],"source":["X_rgb = torch.squeeze(transform_norm(data[\"rgb\"])).float().to(device)\n","labels = data[\"command\"]\n","labels = torch.where(labels == -1, torch.tensor(0), labels).to(torch.int64) # Replace by -1 by 0\n","# Convert the labels to a one hot encoded tensor\n","one_hot = torch.nn.functional.one_hot(labels, num_classes=7).to(device)\n","X_cmd = torch.squeeze(one_hot).float().to(device)\n","X_spd = ((data[\"speed\"]-speed_mean)/speed_std).float().to(device)\n","\n","target_ = (data[\"throttle\"], data[\"steer\"], data[\"brake\"])\n","with torch.no_grad():\n","    net.eval()\n","    outputs_ = net(X_rgb, X_cmd, X_spd)"]},{"cell_type":"code","execution_count":60,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["0.29738437256956357\n","\n","0.017281432621530257\n","\n","0.016537449466326848\n"]}],"source":["# Durchschnittlicher abs. fehler\n","for i in [0,1,2]:\n","    print(np.mean(abs(outputs_[i].cpu().numpy()-target_[i].cpu().numpy())))"]},{"cell_type":"markdown","metadata":{},"source":["Bias Variance"]},{"cell_type":"code","execution_count":53,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["0.013398256\n","\n","0.00063567644\n","\n","0.008254821\n"]}],"source":["# Variance \n","\n","for i in [0,1,2]:\n","    outputs = (outputs_[i].cpu().numpy())\n","    #print(outputs)\n","    mean_outputs = np.mean(outputs_[i].cpu().numpy())\n","    #print(mean_outputs)\n","    diff = (outputs-mean_outputs)**2\n","    #print(diff)\n","    value = np.mean(diff)\n","    print(value)"]},{"cell_type":"code","execution_count":54,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["-0.2074748\n","\n","0.010485157\n","\n","4.656613e-10\n"]}],"source":["# Bias\n","for i in [0,1,2]:\n","    targets = (target_[i].cpu().numpy())\n","    #print(outputs)\n","    mean_outputs = np.mean(outputs_[i].cpu().numpy())\n","    #print(mean_outputs)\n","    diff = outputs-mean_outputs\n","    #print(diff)\n","    value = np.mean(diff)\n","    print(value)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\"\"\"\n","for i in [0,1,2]:\n","    print(np.mean(abs(target_[i].cpu().numpy())))\n","    print(np.std(abs(target_[i].cpu().numpy())))\n","\"\"\""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["i =0"]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":true},"outputs":[],"source":["print(np.round(outputs_[i].cpu().numpy(),1))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["print(np.round(target_[i].cpu().numpy(),1))"]},{"cell_type":"markdown","metadata":{},"source":["### IMG Processing\n","\n","BGR is now standard FOR carla agent and training"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import cv2\n","idx, batch = next(enumerate(test_dataloader))\n","print(batch[\"rgb\"].shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":true},"outputs":[],"source":["img = batch[\"rgb\"][0]#.shape\n","img = img.numpy().astype(np.uint8).reshape(160,960,3)\n","print(img.shape)\n","\n","#img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB) # NUR HIER, NICHT IN CARLA AGENT\n","print(img.shape)\n","print(type(img))\n","transform = transforms.Compose([transforms.ToPILImage()])\n","\n","tensor = transform(img)\n","\n","#print(type(tensor))\n","\n","tensor.show()\n","\n","#torch.tensor(tensor)"]},{"cell_type":"markdown","metadata":{},"source":["TEST Normalization"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["tensor = transform_norm(torch.squeeze(data[\"rgb\"],1))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["tensor = torch.squeeze(transform_norm(data[\"rgb\"])).float()"]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":true},"outputs":[],"source":["for i in range(64):\n","    print(np.mean(tensor.numpy()[i], axis = (1,2)))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["np.mean(tensor.numpy(), axis = (0,2,3))"]},{"cell_type":"markdown","metadata":{},"source":["### adding Navigation and speed"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["test_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","iterator = iter(test_dataloader)\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["data = next(iterator)\n","#data[\"speed\"]\n","#data[\"command\"]"]},{"cell_type":"markdown","metadata":{},"source":["Command"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import torch\n","\n","# Assume labels is a 1D tensor with values from 0 to 6\n","\n","labels = data[\"command\"]\n","labels = torch.where(labels == -1, torch.tensor(0), labels) # Replace by -1 by 0\n","labels = labels.to(torch.int64)\n","\n","# Convert the labels to a one hot encoded tensor\n","one_hot = torch.nn.functional.one_hot(labels, num_classes=7)\n","one_hot = torch.squeeze(one_hot)\n","\n","print(one_hot.shape)"]},{"cell_type":"markdown","metadata":{},"source":["Speed"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["7347\n"]}],"source":["# calc mean over trainingsset\n","train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","print(len(train_dataloader))\n","iterator = iter(train_dataloader)"]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["0\n","\n","100\n","\n","200\n","\n","300\n","\n","400\n","\n","500\n","\n","600\n","\n","700\n","\n","800\n","\n","900\n","\n","1000\n"]}],"source":["i = 0\n","summe = []\n","for batch_idx, data in enumerate(train_dataloader):\n","    #print(data)\n","    if i % 100 == 0:\n","        print(i)\n","    summe.append(np.mean(data[\"speed\"].numpy()))\n","    if i >= 1000:\n","        break\n","    i += 1\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#print(summe)"]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["2.250456762830466\n","\n","0.30215840254891313\n"]}],"source":["print(np.mean(summe)) # 2.2078979146598274\n","print(np.std(summe)) # 0.22455625005948113\n","speed_mean = np.mean(summe)\n","speed_std = np.std(summe)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["batch = next(iterator)\n","#print(np.round(batch[\"speed\"].numpy(),2))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["(batch[\"speed\"]-speed_mean)/speed_std"]},{"cell_type":"markdown","metadata":{},"source":["### Vanishing/Exploding Gradients"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\"\"\"\n","for name, param in net.thr_head.named_parameters():\n","    if param.requires_grad:\n","        print(name, param.data.cpu().numpy())\n","\"\"\""]},{"cell_type":"code","execution_count":57,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["0.weight 1.216921\n","\n","0.bias 0.0005766605\n"]}],"source":["for name, param in net.spd_input.named_parameters():\n","    if param.requires_grad:\n","        print(name, np.max(abs(param.data.cpu().numpy())))"]},{"cell_type":"markdown","metadata":{},"source":["## Saving and Loading"]},{"cell_type":"markdown","metadata":{},"source":["Not suited for leaderboard agents"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#torch.save(net, 'rgb_resnet.pth')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#net = torch.load('rgb_resnet.pth')"]},{"cell_type":"markdown","metadata":{},"source":["suited for leaderboard agents"]},{"cell_type":"code","execution_count":20,"metadata":{},"outputs":[],"source":["#torch.save(net.state_dict(), \"rgb_resnet_cmd_spd.pth\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#net = MyResnet()\n","#net.load_state_dict(torch.load(\"rgb_resnet_cmd_spd.pth\"))\n","#net.cuda()"]},{"cell_type":"markdown","metadata":{},"source":["## Testing Time"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n","idx, X = next(enumerate(test_dataloader))\n","img = transform_norm(X[\"rgb\"])\n","img.shape"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["torch.squeeze(img,1).shape"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# ohne preprocessing ca 16-17 sekunden. Mit preprocessing ca 37 sekunden ~2gb\n","# 24.12: 44 batches -> preprocessing 26 sec, training & preprocessing 69 sec\n","test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n","print(len(test_dataloader))\n","at = time.time()\n","for batch_idx, data in enumerate(test_dataloader):\n","    #print(batch_idx)\n","    data_ = torch.squeeze(transform_norm(data[\"rgb\"]),1).float()\n","    #print(data_.shape)\n","et = time.time()\n","print(et-at)\n"]}],"metadata":{"kernelspec":{"display_name":"carla","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.13"},"vscode":{"interpreter":{"hash":"bc80d0638afb8ec7c43f4b834002a598fcddbd6e8bf5db40ad8cba47e68e6a97"}}},"nbformat":4,"nbformat_minor":5}
