{
	"name": "TypeError",
	"message": "_create_method_from_trace(): incompatible function arguments. The following argument types are supported:\n    1. (self: torch._C.ScriptModule, name: str, func: function, input_tuple: tuple, var_name_lookup_fn: function, strict: bool, force_outplace: bool, argument_names: List[str] = []) -> None\n\nInvoked with: <torch.ScriptModule object at 0x16a2aeb30>, 'forward', BaselineV1(\n  (net): ResNet(\n    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (relu): ReLU(inplace=True)\n    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n    (layer1): Sequential(\n      (0): BasicBlock(\n        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n      (1): BasicBlock(\n        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (layer2): Sequential(\n      (0): BasicBlock(\n        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (downsample): Sequential(\n          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (1): BasicBlock(\n        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (layer3): Sequential(\n      (0): BasicBlock(\n        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (downsample): Sequential(\n          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (1): BasicBlock(\n        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (layer4): Sequential(\n      (0): BasicBlock(\n        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (downsample): Sequential(\n          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (1): BasicBlock(\n        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n    (fc): Identity()\n  )\n  (cmd_input): Sequential(\n    (0): Linear(in_features=7, out_features=7, bias=True)\n    (1): LeakyReLU(negative_slope=0.01)\n  )\n  (spd_input): Sequential(\n    (0): Linear(in_features=1, out_features=1, bias=True)\n    (1): LeakyReLU(negative_slope=0.01)\n  )\n  (thr_head): Sequential(\n    (0): Linear(in_features=520, out_features=1, bias=True)\n    (1): Sigmoid()\n  )\n  (brk_head): Sequential(\n    (0): Linear(in_features=520, out_features=1, bias=True)\n    (1): Sigmoid()\n  )\n  (str_head): Sequential(\n    (0): Linear(in_features=520, out_features=1, bias=True)\n    (1): Tanh()\n  )\n), (tensor([[[[-0.5037, -0.2045, -0.2045,  ..., -1.1920, -1.1920, -1.1920],\n          [-1.1920, -1.1920, -1.1920,  ..., -1.1920, -1.1920, -1.1920],\n          [-1.1920, -1.1920, -1.1920,  ..., -1.1920, -1.1920, -1.1920],\n          ...,\n          [-1.1920, -1.1920, -1.1920,  ..., -1.1920, -0.9975, -0.9975],\n          [-0.9975, -0.6833, -0.5636,  ..., -1.1920, -1.1920, -1.1920],\n          [-1.1920, -1.1920, -1.1920,  ..., -1.1920, -0.9975, -0.9975]],\n\n         [[-1.3572, -1.3572, -1.3572,  ..., -1.1409, -0.9911, -0.9911],\n          [-0.8913, -0.7249, -0.5252,  ..., -1.3572, -1.3572, -1.1409],\n          [-1.3572, -1.3572, -1.3572,  ..., -0.3755, -0.5252, -0.5918],\n          ...,\n          [-0.9911, -0.7249, -0.5252,  ..., -1.3572, -1.3572, -1.3572],\n          [-1.3572, -1.3572, -1.3572,  ..., -1.3572, -1.3572, -1.3572],\n          [-1.3572, -1.3572, -1.1409,  ..., -1.1409, -0.7915, -0.6584]],\n\n         [[-1.2627, -1.0815, -0.9002,  ..., -1.5949, -1.5949, -1.3986],\n          [-1.3986, -1.5949, -1.5949,  ..., -1.5949, -1.5949, -1.5949],\n          [-1.5949, -1.3986, -1.3986,  ..., -1.2627, -1.0211, -0.9606],\n          ...,\n          [-1.3986, -1.3986, -1.3986,  ..., -1.1721, -1.1721, -1.0815],\n          [-1.1721, -1.0815, -1.0211,  ..., -1.5949, -1.0815, -1.3986],\n          [-1.5949, -1.0815, -1.3986,  ..., -1.5949, -1.3986, -1.2627]]],\n\n\n        [[[-0.9975, -0.6833, -0.9975,  ...,  0.9626,  1.4115,  1.7557],\n          [ 1.0973,  1.5312,  1.8604,  ...,  0.0200,  0.1247,  0.1995],\n          [-0.0100,  0.0798,  0.1696,  ..., -0.0698, -0.0100,  0.0499],\n          ...,\n          [ 0.1995,  0.3342,  0.4539,  ..., -0.6234, -0.0997, -0.6234],\n          [-0.7731, -0.6234, -0.8629,  ...,  0.0499, -0.0698, -0.0100],\n          [-0.6234, -0.2045, -0.3990,  ..., -0.0698, -0.0399, -0.0399]],\n\n         [[-0.0094,  0.1403,  0.3233,  ..., -1.1409, -0.7915, -0.8913],\n          [-0.8913, -0.5918, -0.8913,  ..., -0.2590, -0.2923, -0.2091],\n          [-0.7249, -0.3422, -0.5252,  ..., -0.0427, -0.0094,  0.0238],\n          ...,\n          [ 1.2218,  1.1220,  1.1386,  ...,  0.6561,  0.7892,  0.8891],\n          [-0.4753,  0.2401, -0.0760,  ..., -1.3572, -1.3572,  0.6062],\n          [-1.3572, -1.3572,  0.8724,  ...,  1.2551,  1.1553,  1.1386]],\n\n         [[ 0.7306,  0.6551,  0.6551,  ...,  0.3078,  0.3984,  0.4739],\n          [-0.7945, -0.0697, -0.4925,  ..., -1.5949, -1.5949,  0.1417],\n          [-1.5949, -1.5949,  0.6400,  ...,  0.7457,  0.6400,  0.6551],\n          ...,\n          [-0.4321, -0.1905, -0.0697,  ..., -0.7945,  0.0058, -0.4019],\n          [-0.7945,  0.1115, -0.2962,  ..., -0.5529,  0.1870, -0.3717],\n          [-0.6737,  0.0964, -0.3717,  ..., -0.5982, -0.2358, -0.2358]]],\n\n\n        [[[-0.7731, -0.1596, -0.7731,  ..., -0.5037, -0.3990, -0.3092],\n          [-0.3541, -0.2793, -0.1596,  ..., -0.3092, -0.1596, -0.0698],\n          [-0.2045, -0.0997,  0.0200,  ...,  0.7831,  1.3068,  1.7108],\n          ...,\n          [ 0.5287,  0.8280,  1.2021,  ..., -0.3541, -0.6833, -0.8629],\n          [-0.9975, -0.9975, -0.8629,  ..., -0.2793, -0.3990, -0.4439],\n          [-0.6234, -0.2344, -0.6833,  ...,  0.1546,  0.5886,  0.9926]],\n\n         [[ 0.4398,  0.8225,  1.2551,  ..., -1.1409, -0.9911, -0.8913],\n          [-0.9911, -0.8913, -0.7915,  ..., -0.3422, -0.5252, -0.5918],\n          [-0.7915, -0.2590, -0.8913,  ...,  0.6228,  1.0222,  1.4215],\n          ...,\n          [ 1.0388,  0.9723,  0.9556,  ...,  0.1902,  0.1902,  0.1403],\n          [-0.2590,  0.4731,  0.8059,  ...,  0.1569,  0.0571,  2.3699],\n          [ 0.4065,  1.3217,  2.3200,  ...,  0.8059,  0.7560,  0.7726]],\n\n         [[ 0.5947,  0.5192,  0.5192,  ..., -0.2358, -0.2358, -0.2660],\n          [-0.5227,  0.0964,  0.2927,  ..., -1.3986, -0.7945,  1.7122],\n          [ 0.6400,  1.0477,  1.7424,  ...,  0.3682,  0.3078,  0.3229],\n          ...,\n          [-1.0211, -0.4019, -0.6737,  ..., -1.1721, -0.6737, -0.7945],\n          [-0.7945, -0.4321, -0.4925,  ..., -0.9002, -0.1452, -0.6284],\n          [-0.8398, -0.0999, -0.6284,  ..., -0.7039, -0.5227, -0.6284]]],\n\n\n        ...,\n\n\n        [[[-0.6833, -0.3092, -0.0399,  ..., -0.6833,  0.2294,  0.4240],\n          [ 0.4389,  0.3791,  0.3641,  ..., -0.6833, -0.3092, -0.8629],\n          [-0.7731, -0.5037, -0.8629,  ..., -0.5037,  0.0948, -0.3541],\n          ...,\n          [-0.5037,  0.1546, -0.0100,  ..., -0.7731, -0.4439, -0.8629],\n          [-0.6833, -0.3990, -0.8629,  ..., -1.1920, -0.7731, -0.9975],\n          [-0.2344,  0.2294,  0.4838,  ...,  0.4240,  0.9327, -0.3541]],\n\n         [[-0.5918,  0.1902,  0.0238,  ..., -0.8913, -0.4753, -0.8913],\n          [-0.9911, -0.5918, -0.9911,  ..., -1.1409, -0.9911, -1.1409],\n          [ 0.2901,  0.8225,  1.1054,  ..., -0.2590,  0.3733, -0.1426],\n          ...,\n          [-0.4753,  0.1403, -0.1093,  ..., -1.3572, -0.7915, -0.7915],\n          [ 1.3716,  1.6045,  1.8375,  ...,  1.3716,  1.6711,  1.9040],\n          [ 1.4215,  1.6711,  1.8874,  ..., -0.2923,  0.3733,  0.1070]],\n\n         [[-0.7492, -0.1452, -0.4321,  ..., -1.5949, -1.1721, -1.1721],\n          [ 0.8816,  1.0628,  1.2894,  ...,  0.5494,  0.8363,  1.0175],\n          [ 0.6249,  0.8665,  1.0930,  ..., -0.6284,  0.0662, -0.2962],\n          ...,\n          [ 1.0930,  1.3347,  1.5310,  ...,  1.0477,  1.1535,  1.2894],\n          [ 0.9722,  1.0779,  1.1988,  ...,  1.5461,  1.6065,  1.6971],\n          [ 1.2290,  1.3045,  1.4253,  ..., -1.5949, -1.0815, -1.0211]]],\n\n\n        [[[-0.5037, -0.2793, -0.0100,  ...,  0.3192,  0.3192,  0.3641],\n          [ 1.6509,  2.1447,  2.4141,  ..., -0.8629, -0.6234, -0.9975],\n          [-0.7731, -0.1297, -0.6833,  ..., -0.3990, -0.1297, -0.6833],\n          ...,\n          [ 0.3342,  0.2444,  0.2294,  ..., -0.5037, -0.2793, -0.3541],\n          [-0.0698,  0.2743, -0.0100,  ..., -0.3541,  0.2294, -0.3541],\n          [-0.9975, -0.9975, -0.9975,  ...,  0.2743,  0.1546,  0.0948]],\n\n         [[ 0.3733,  0.2401,  0.1902,  ..., -0.6584, -0.2923, -0.4753],\n          [-0.2091,  0.2235, -0.2923,  ..., -0.3755,  0.2401, -0.3755],\n          [-1.1409, -1.1409, -0.9911,  ...,  0.1403,  0.0571, -0.0427],\n          ...,\n          [-0.7249, -0.5918, -0.5252,  ..., -0.3422, -0.3755, -0.4753],\n          [-0.8913, -0.7915, -0.5918,  ..., -1.3572, -1.3572, -1.3572],\n          [-1.1409, -1.3572, -1.3572,  ..., -0.7915, -0.6584, -0.5918]],\n\n         [[-1.0815, -0.8398, -0.8398,  ..., -0.5529, -0.5982, -0.7039],\n          [-1.1721, -1.0815, -0.8398,  ..., -1.5949, -1.5949, -1.5949],\n          [-1.5949, -1.5949, -1.5949,  ..., -1.0815, -1.0211, -0.9606],\n          ...,\n          [ 0.0511,  0.1115,  0.2323,  ..., -0.4019, -0.4623, -0.4321],\n          [-0.2660, -0.2962, -0.2660,  ..., -1.1721, -0.7039, -0.9002],\n          [-0.3415, -0.0093, -0.2358,  ..., -0.4321,  0.1115, -0.2962]]],\n\n\n        [[[ 0.0200,  0.0798,  0.2294,  ..., -0.0698,  1.2021,  2.0550],\n          [ 0.5287,  1.2619,  1.8904,  ..., -0.9975, -0.5636, -0.9975],\n          [-0.6833, -0.3541, -0.8629,  ..., -1.1920, -0.8629, -1.1920],\n          ...,\n          [-0.9975, -0.6833, -0.7731,  ..., -0.8629, -0.6833, -0.9975],\n          [-0.7731, -0.4439, -0.8629,  ..., -0.9975, -0.7731, -0.9975],\n          [-0.8629, -0.7731, -0.6833,  ..., -0.9975, -0.7731, -0.8629]],\n\n         [[-1.1409, -0.7915, -0.8913,  ..., -0.9911, -0.7915, -1.1409],\n          [-0.9911, -0.7915, -0.9911,  ..., -0.9911, -0.7249, -0.9911],\n          [-1.1409, -1.1409, -0.9911,  ..., -0.9911, -0.7249, -0.9911],\n          ...,\n          [-0.7915, -0.0760, -0.3755,  ..., -0.7915, -0.5252, -0.4753],\n          [-0.5252, -0.4254, -0.3422,  ..., -0.5918, -0.4753, -0.3755],\n          [-0.4753, -0.3755, -0.3422,  ..., -0.7915, -0.0760, -0.5252]],\n\n         [[-1.0211, -0.3415, -0.7039,  ..., -1.0815, -0.9002, -0.8398],\n          [-0.7945, -0.7039, -0.5982,  ..., -0.9606, -0.9002, -0.7945],\n          [-0.9002, -0.7492, -0.7039,  ..., -1.0211, -0.4019, -0.7945],\n          ...,\n          [-0.5982, -0.4925, -0.4019,  ..., -0.2358, -0.2660, -0.2660],\n          [-0.3113, -0.4019, -0.4019,  ..., -0.2962, -0.3113, -0.3113],\n          [-0.2660, -0.2962, -0.2962,  ..., -1.5949, -1.1721, -1.1721]]]],\n       device='mps:0'),), <function _create_interpreter_name_lookup_fn.<locals>._get_interpreter_name_for_var at 0x16a181f70>, tensor([[ 5.4698],\n        [-5.9602],\n        [-6.3166],\n        [ 5.8640],\n        [ 5.7413],\n        [-7.4479],\n        [ 2.7846],\n        [ 2.3699],\n        [ 6.3503],\n        [-7.4479],\n        [-7.4479],\n        [-7.4503],\n        [-7.4479],\n        [-7.4380],\n        [-7.4479],\n        [ 2.2253],\n        [ 5.6724],\n        [-7.4479],\n        [-7.4479],\n        [ 5.7514],\n        [-7.4479],\n        [ 6.1727],\n        [ 2.2901],\n        [ 1.6873],\n        [-7.4479],\n        [ 2.5695],\n        [-1.3784],\n        [-5.9167],\n        [ 4.8650],\n        [-7.4479],\n        [-7.4479],\n        [-7.2550],\n        [-7.3076],\n        [ 4.7662],\n        [ 2.0279],\n        [-7.4479],\n        [-6.1667],\n        [-0.0880],\n        [ 5.9941],\n        [-7.4479],\n        [-7.4479],\n        [ 2.8503],\n        [ 5.6612],\n        [ 6.4779],\n        [-7.4479],\n        [-7.0289],\n        [-7.2889],\n        [-7.4451],\n        [-7.2174],\n        [-7.4479],\n        [ 4.8682],\n        [-3.0873],\n        [-7.4479],\n        [-7.4479],\n        [ 6.1519],\n        [-7.4479],\n        [ 5.0508],\n        [ 4.7436],\n        [ 5.8650],\n        [-7.0141],\n        [ 3.7058],\n        [ 1.6129],\n        [-7.4479],\n        [ 4.0441]], device='mps:0'), False, ['rgb', 'cmd', 'spd']",
	"stack": "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)\nCell \u001b[0;32mIn[15], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model_trainer\u001b[39m.\u001b[39;49mrun()\n\nFile \u001b[0;32m~/Documents/GitHub/end2endappras/models/resnet_baseline/notebooks/../../model_trainer.py:70\u001b[0m, in \u001b[0;36mModelTrainer.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[39mfor\u001b[39;00m batch_idx, (X_true, y_true) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataloader_train):\n\u001b[1;32m     68\u001b[0m     \u001b[39m# forward\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     start_forward \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[0;32m---> 70\u001b[0m     loss_list \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward_pass(X_true, y_true, writer, epoch)\n\u001b[1;32m     71\u001b[0m     loss \u001b[39m=\u001b[39m \u001b[39msum\u001b[39m(loss_list) \u001b[39m/\u001b[39m \u001b[39m3\u001b[39m\n\u001b[1;32m     72\u001b[0m     times_forward\u001b[39m.\u001b[39mappend(time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m start_forward)\n\nFile \u001b[0;32m~/Documents/GitHub/end2endappras/models/resnet_baseline/notebooks/../../model_trainer.py:134\u001b[0m, in \u001b[0;36mModelTrainer.forward_pass\u001b[0;34m(self, X_true, Y_true, writer, epoch)\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[39m# write model graph\u001b[39;00m\n\u001b[1;32m    133\u001b[0m \u001b[39mif\u001b[39;00m epoch \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m--> 134\u001b[0m     writer\u001b[39m.\u001b[39;49madd_graph(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel, \u001b[39m*\u001b[39;49mX_true)\n\u001b[1;32m    136\u001b[0m \u001b[39m# forward pass\u001b[39;00m\n\u001b[1;32m    137\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptimizer\u001b[39m.\u001b[39mzero_grad()\n\nFile \u001b[0;32m~/miniforge3/envs/carla/lib/python3.8/site-packages/torch/utils/tensorboard/writer.py:841\u001b[0m, in \u001b[0;36mSummaryWriter.add_graph\u001b[0;34m(self, model, input_to_model, verbose, use_strict_trace)\u001b[0m\n\u001b[1;32m    837\u001b[0m torch\u001b[39m.\u001b[39m_C\u001b[39m.\u001b[39m_log_api_usage_once(\u001b[39m\"\u001b[39m\u001b[39mtensorboard.logging.add_graph\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    838\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(model, \u001b[39m\"\u001b[39m\u001b[39mforward\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m    839\u001b[0m     \u001b[39m# A valid PyTorch model should have a 'forward' method\u001b[39;00m\n\u001b[1;32m    840\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_file_writer()\u001b[39m.\u001b[39madd_graph(\n\u001b[0;32m--> 841\u001b[0m         graph(model, input_to_model, verbose, use_strict_trace)\n\u001b[1;32m    842\u001b[0m     )\n\u001b[1;32m    843\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    844\u001b[0m     \u001b[39m# Caffe2 models do not have the 'forward' method\u001b[39;00m\n\u001b[1;32m    845\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39mcaffe2\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mproto\u001b[39;00m \u001b[39mimport\u001b[39;00m caffe2_pb2\n\nFile \u001b[0;32m~/miniforge3/envs/carla/lib/python3.8/site-packages/torch/utils/tensorboard/_pytorch_graph.py:337\u001b[0m, in \u001b[0;36mgraph\u001b[0;34m(model, args, verbose, use_strict_trace)\u001b[0m\n\u001b[1;32m    335\u001b[0m \u001b[39mwith\u001b[39;00m _set_model_to_eval(model):\n\u001b[1;32m    336\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 337\u001b[0m         trace \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mjit\u001b[39m.\u001b[39;49mtrace(model, args, strict\u001b[39m=\u001b[39;49muse_strict_trace)\n\u001b[1;32m    338\u001b[0m         graph \u001b[39m=\u001b[39m trace\u001b[39m.\u001b[39mgraph\n\u001b[1;32m    339\u001b[0m         torch\u001b[39m.\u001b[39m_C\u001b[39m.\u001b[39m_jit_pass_inline(graph)\n\nFile \u001b[0;32m~/miniforge3/envs/carla/lib/python3.8/site-packages/torch/jit/_trace.py:759\u001b[0m, in \u001b[0;36mtrace\u001b[0;34m(func, example_inputs, optimize, check_trace, check_inputs, check_tolerance, strict, _force_outplace, _module_class, _compilation_unit)\u001b[0m\n\u001b[1;32m    756\u001b[0m     \u001b[39mreturn\u001b[39;00m func\n\u001b[1;32m    758\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(func, torch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mModule):\n\u001b[0;32m--> 759\u001b[0m     \u001b[39mreturn\u001b[39;00m trace_module(\n\u001b[1;32m    760\u001b[0m         func,\n\u001b[1;32m    761\u001b[0m         {\u001b[39m\"\u001b[39;49m\u001b[39mforward\u001b[39;49m\u001b[39m\"\u001b[39;49m: example_inputs},\n\u001b[1;32m    762\u001b[0m         \u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m    763\u001b[0m         check_trace,\n\u001b[1;32m    764\u001b[0m         wrap_check_inputs(check_inputs),\n\u001b[1;32m    765\u001b[0m         check_tolerance,\n\u001b[1;32m    766\u001b[0m         strict,\n\u001b[1;32m    767\u001b[0m         _force_outplace,\n\u001b[1;32m    768\u001b[0m         _module_class,\n\u001b[1;32m    769\u001b[0m     )\n\u001b[1;32m    771\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m    772\u001b[0m     \u001b[39mhasattr\u001b[39m(func, \u001b[39m\"\u001b[39m\u001b[39m__self__\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    773\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(func\u001b[39m.\u001b[39m\u001b[39m__self__\u001b[39m, torch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mModule)\n\u001b[1;32m    774\u001b[0m     \u001b[39mand\u001b[39;00m func\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mforward\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    775\u001b[0m ):\n\u001b[1;32m    776\u001b[0m     \u001b[39mreturn\u001b[39;00m trace_module(\n\u001b[1;32m    777\u001b[0m         func\u001b[39m.\u001b[39m\u001b[39m__self__\u001b[39m,\n\u001b[1;32m    778\u001b[0m         {\u001b[39m\"\u001b[39m\u001b[39mforward\u001b[39m\u001b[39m\"\u001b[39m: example_inputs},\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    785\u001b[0m         _module_class,\n\u001b[1;32m    786\u001b[0m     )\n\nFile \u001b[0;32m~/miniforge3/envs/carla/lib/python3.8/site-packages/torch/jit/_trace.py:976\u001b[0m, in \u001b[0;36mtrace_module\u001b[0;34m(mod, inputs, optimize, check_trace, check_inputs, check_tolerance, strict, _force_outplace, _module_class, _compilation_unit)\u001b[0m\n\u001b[1;32m    972\u001b[0m     argument_names \u001b[39m=\u001b[39m get_callable_argument_names(func)\n\u001b[1;32m    974\u001b[0m example_inputs \u001b[39m=\u001b[39m make_tuple(example_inputs)\n\u001b[0;32m--> 976\u001b[0m module\u001b[39m.\u001b[39;49m_c\u001b[39m.\u001b[39;49m_create_method_from_trace(\n\u001b[1;32m    977\u001b[0m     method_name,\n\u001b[1;32m    978\u001b[0m     func,\n\u001b[1;32m    979\u001b[0m     example_inputs,\n\u001b[1;32m    980\u001b[0m     var_lookup_fn,\n\u001b[1;32m    981\u001b[0m     strict,\n\u001b[1;32m    982\u001b[0m     _force_outplace,\n\u001b[1;32m    983\u001b[0m     argument_names,\n\u001b[1;32m    984\u001b[0m )\n\u001b[1;32m    985\u001b[0m check_trace_method \u001b[39m=\u001b[39m module\u001b[39m.\u001b[39m_c\u001b[39m.\u001b[39m_get_method(method_name)\n\u001b[1;32m    987\u001b[0m \u001b[39m# Check the trace against new traces created from user-specified inputs\u001b[39;00m\n\n\u001b[0;31mTypeError\u001b[0m: _create_method_from_trace(): incompatible function arguments. The following argument types are supported:\n    1. (self: torch._C.ScriptModule, name: str, func: function, input_tuple: tuple, var_name_lookup_fn: function, strict: bool, force_outplace: bool, argument_names: List[str] = []) -> None\n\nInvoked with: <torch.ScriptModule object at 0x16a2aeb30>, 'forward', BaselineV1(\n  (net): ResNet(\n    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (relu): ReLU(inplace=True)\n    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n    (layer1): Sequential(\n      (0): BasicBlock(\n        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n      (1): BasicBlock(\n        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (layer2): Sequential(\n      (0): BasicBlock(\n        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (downsample): Sequential(\n          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (1): BasicBlock(\n        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (layer3): Sequential(\n      (0): BasicBlock(\n        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (downsample): Sequential(\n          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (1): BasicBlock(\n        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (layer4): Sequential(\n      (0): BasicBlock(\n        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (downsample): Sequential(\n          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (1): BasicBlock(\n        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n    (fc): Identity()\n  )\n  (cmd_input): Sequential(\n    (0): Linear(in_features=7, out_features=7, bias=True)\n    (1): LeakyReLU(negative_slope=0.01)\n  )\n  (spd_input): Sequential(\n    (0): Linear(in_features=1, out_features=1, bias=True)\n    (1): LeakyReLU(negative_slope=0.01)\n  )\n  (thr_head): Sequential(\n    (0): Linear(in_features=520, out_features=1, bias=True)\n    (1): Sigmoid()\n  )\n  (brk_head): Sequential(\n    (0): Linear(in_features=520, out_features=1, bias=True)\n    (1): Sigmoid()\n  )\n  (str_head): Sequential(\n    (0): Linear(in_features=520, out_features=1, bias=True)\n    (1): Tanh()\n  )\n), (tensor([[[[-0.5037, -0.2045, -0.2045,  ..., -1.1920, -1.1920, -1.1920],\n          [-1.1920, -1.1920, -1.1920,  ..., -1.1920, -1.1920, -1.1920],\n          [-1.1920, -1.1920, -1.1920,  ..., -1.1920, -1.1920, -1.1920],\n          ...,\n          [-1.1920, -1.1920, -1.1920,  ..., -1.1920, -0.9975, -0.9975],\n          [-0.9975, -0.6833, -0.5636,  ..., -1.1920, -1.1920, -1.1920],\n          [-1.1920, -1.1920, -1.1920,  ..., -1.1920, -0.9975, -0.9975]],\n\n         [[-1.3572, -1.3572, -1.3572,  ..., -1.1409, -0.9911, -0.9911],\n          [-0.8913, -0.7249, -0.5252,  ..., -1.3572, -1.3572, -1.1409],\n          [-1.3572, -1.3572, -1.3572,  ..., -0.3755, -0.5252, -0.5918],\n          ...,\n          [-0.9911, -0.7249, -0.5252,  ..., -1.3572, -1.3572, -1.3572],\n          [-1.3572, -1.3572, -1.3572,  ..., -1.3572, -1.3572, -1.3572],\n          [-1.3572, -1.3572, -1.1409,  ..., -1.1409, -0.7915, -0.6584]],\n\n         [[-1.2627, -1.0815, -0.9002,  ..., -1.5949, -1.5949, -1.3986],\n          [-1.3986, -1.5949, -1.5949,  ..., -1.5949, -1.5949, -1.5949],\n          [-1.5949, -1.3986, -1.3986,  ..., -1.2627, -1.0211, -0.9606],\n          ...,\n          [-1.3986, -1.3986, -1.3986,  ..., -1.1721, -1.1721, -1.0815],\n          [-1.1721, -1.0815, -1.0211,  ..., -1.5949, -1.0815, -1.3986],\n          [-1.5949, -1.0815, -1.3986,  ..., -1.5949, -1.3986, -1.2627]]],\n\n\n        [[[-0.9975, -0.6833, -0.9975,  ...,  0.9626,  1.4115,  1.7557],\n          [ 1.0973,  1.5312,  1.8604,  ...,  0.0200,  0.1247,  0.1995],\n          [-0.0100,  0.0798,  0.1696,  ..., -0.0698, -0.0100,  0.0499],\n          ...,\n          [ 0.1995,  0.3342,  0.4539,  ..., -0.6234, -0.0997, -0.6234],\n          [-0.7731, -0.6234, -0.8629,  ...,  0.0499, -0.0698, -0.0100],\n          [-0.6234, -0.2045, -0.3990,  ..., -0.0698, -0.0399, -0.0399]],\n\n         [[-0.0094,  0.1403,  0.3233,  ..., -1.1409, -0.7915, -0.8913],\n          [-0.8913, -0.5918, -0.8913,  ..., -0.2590, -0.2923, -0.2091],\n          [-0.7249, -0.3422, -0.5252,  ..., -0.0427, -0.0094,  0.0238],\n          ...,\n          [ 1.2218,  1.1220,  1.1386,  ...,  0.6561,  0.7892,  0.8891],\n          [-0.4753,  0.2401, -0.0760,  ..., -1.3572, -1.3572,  0.6062],\n          [-1.3572, -1.3572,  0.8724,  ...,  1.2551,  1.1553,  1.1386]],\n\n         [[ 0.7306,  0.6551,  0.6551,  ...,  0.3078,  0.3984,  0.4739],\n          [-0.7945, -0.0697, -0.4925,  ..., -1.5949, -1.5949,  0.1417],\n          [-1.5949, -1.5949,  0.6400,  ...,  0.7457,  0.6400,  0.6551],\n          ...,\n          [-0.4321, -0.1905, -0.0697,  ..., -0.7945,  0.0058, -0.4019],\n          [-0.7945,  0.1115, -0.2962,  ..., -0.5529,  0.1870, -0.3717],\n          [-0.6737,  0.0964, -0.3717,  ..., -0.5982, -0.2358, -0.2358]]],\n\n\n        [[[-0.7731, -0.1596, -0.7731,  ..., -0.5037, -0.3990, -0.3092],\n          [-0.3541, -0.2793, -0.1596,  ..., -0.3092, -0.1596, -0.0698],\n          [-0.2045, -0.0997,  0.0200,  ...,  0.7831,  1.3068,  1.7108],\n          ...,\n          [ 0.5287,  0.8280,  1.2021,  ..., -0.3541, -0.6833, -0.8629],\n          [-0.9975, -0.9975, -0.8629,  ..., -0.2793, -0.3990, -0.4439],\n          [-0.6234, -0.2344, -0.6833,  ...,  0.1546,  0.5886,  0.9926]],\n\n         [[ 0.4398,  0.8225,  1.2551,  ..., -1.1409, -0.9911, -0.8913],\n          [-0.9911, -0.8913, -0.7915,  ..., -0.3422, -0.5252, -0.5918],\n          [-0.7915, -0.2590, -0.8913,  ...,  0.6228,  1.0222,  1.4215],\n          ...,\n          [ 1.0388,  0.9723,  0.9556,  ...,  0.1902,  0.1902,  0.1403],\n          [-0.2590,  0.4731,  0.8059,  ...,  0.1569,  0.0571,  2.3699],\n          [ 0.4065,  1.3217,  2.3200,  ...,  0.8059,  0.7560,  0.7726]],\n\n         [[ 0.5947,  0.5192,  0.5192,  ..., -0.2358, -0.2358, -0.2660],\n          [-0.5227,  0.0964,  0.2927,  ..., -1.3986, -0.7945,  1.7122],\n          [ 0.6400,  1.0477,  1.7424,  ...,  0.3682,  0.3078,  0.3229],\n          ...,\n          [-1.0211, -0.4019, -0.6737,  ..., -1.1721, -0.6737, -0.7945],\n          [-0.7945, -0.4321, -0.4925,  ..., -0.9002, -0.1452, -0.6284],\n          [-0.8398, -0.0999, -0.6284,  ..., -0.7039, -0.5227, -0.6284]]],\n\n\n        ...,\n\n\n        [[[-0.6833, -0.3092, -0.0399,  ..., -0.6833,  0.2294,  0.4240],\n          [ 0.4389,  0.3791,  0.3641,  ..., -0.6833, -0.3092, -0.8629],\n          [-0.7731, -0.5037, -0.8629,  ..., -0.5037,  0.0948, -0.3541],\n          ...,\n          [-0.5037,  0.1546, -0.0100,  ..., -0.7731, -0.4439, -0.8629],\n          [-0.6833, -0.3990, -0.8629,  ..., -1.1920, -0.7731, -0.9975],\n          [-0.2344,  0.2294,  0.4838,  ...,  0.4240,  0.9327, -0.3541]],\n\n         [[-0.5918,  0.1902,  0.0238,  ..., -0.8913, -0.4753, -0.8913],\n          [-0.9911, -0.5918, -0.9911,  ..., -1.1409, -0.9911, -1.1409],\n          [ 0.2901,  0.8225,  1.1054,  ..., -0.2590,  0.3733, -0.1426],\n          ...,\n          [-0.4753,  0.1403, -0.1093,  ..., -1.3572, -0.7915, -0.7915],\n          [ 1.3716,  1.6045,  1.8375,  ...,  1.3716,  1.6711,  1.9040],\n          [ 1.4215,  1.6711,  1.8874,  ..., -0.2923,  0.3733,  0.1070]],\n\n         [[-0.7492, -0.1452, -0.4321,  ..., -1.5949, -1.1721, -1.1721],\n          [ 0.8816,  1.0628,  1.2894,  ...,  0.5494,  0.8363,  1.0175],\n          [ 0.6249,  0.8665,  1.0930,  ..., -0.6284,  0.0662, -0.2962],\n          ...,\n          [ 1.0930,  1.3347,  1.5310,  ...,  1.0477,  1.1535,  1.2894],\n          [ 0.9722,  1.0779,  1.1988,  ...,  1.5461,  1.6065,  1.6971],\n          [ 1.2290,  1.3045,  1.4253,  ..., -1.5949, -1.0815, -1.0211]]],\n\n\n        [[[-0.5037, -0.2793, -0.0100,  ...,  0.3192,  0.3192,  0.3641],\n          [ 1.6509,  2.1447,  2.4141,  ..., -0.8629, -0.6234, -0.9975],\n          [-0.7731, -0.1297, -0.6833,  ..., -0.3990, -0.1297, -0.6833],\n          ...,\n          [ 0.3342,  0.2444,  0.2294,  ..., -0.5037, -0.2793, -0.3541],\n          [-0.0698,  0.2743, -0.0100,  ..., -0.3541,  0.2294, -0.3541],\n          [-0.9975, -0.9975, -0.9975,  ...,  0.2743,  0.1546,  0.0948]],\n\n         [[ 0.3733,  0.2401,  0.1902,  ..., -0.6584, -0.2923, -0.4753],\n          [-0.2091,  0.2235, -0.2923,  ..., -0.3755,  0.2401, -0.3755],\n          [-1.1409, -1.1409, -0.9911,  ...,  0.1403,  0.0571, -0.0427],\n          ...,\n          [-0.7249, -0.5918, -0.5252,  ..., -0.3422, -0.3755, -0.4753],\n          [-0.8913, -0.7915, -0.5918,  ..., -1.3572, -1.3572, -1.3572],\n          [-1.1409, -1.3572, -1.3572,  ..., -0.7915, -0.6584, -0.5918]],\n\n         [[-1.0815, -0.8398, -0.8398,  ..., -0.5529, -0.5982, -0.7039],\n          [-1.1721, -1.0815, -0.8398,  ..., -1.5949, -1.5949, -1.5949],\n          [-1.5949, -1.5949, -1.5949,  ..., -1.0815, -1.0211, -0.9606],\n          ...,\n          [ 0.0511,  0.1115,  0.2323,  ..., -0.4019, -0.4623, -0.4321],\n          [-0.2660, -0.2962, -0.2660,  ..., -1.1721, -0.7039, -0.9002],\n          [-0.3415, -0.0093, -0.2358,  ..., -0.4321,  0.1115, -0.2962]]],\n\n\n        [[[ 0.0200,  0.0798,  0.2294,  ..., -0.0698,  1.2021,  2.0550],\n          [ 0.5287,  1.2619,  1.8904,  ..., -0.9975, -0.5636, -0.9975],\n          [-0.6833, -0.3541, -0.8629,  ..., -1.1920, -0.8629, -1.1920],\n          ...,\n          [-0.9975, -0.6833, -0.7731,  ..., -0.8629, -0.6833, -0.9975],\n          [-0.7731, -0.4439, -0.8629,  ..., -0.9975, -0.7731, -0.9975],\n          [-0.8629, -0.7731, -0.6833,  ..., -0.9975, -0.7731, -0.8629]],\n\n         [[-1.1409, -0.7915, -0.8913,  ..., -0.9911, -0.7915, -1.1409],\n          [-0.9911, -0.7915, -0.9911,  ..., -0.9911, -0.7249, -0.9911],\n          [-1.1409, -1.1409, -0.9911,  ..., -0.9911, -0.7249, -0.9911],\n          ...,\n          [-0.7915, -0.0760, -0.3755,  ..., -0.7915, -0.5252, -0.4753],\n          [-0.5252, -0.4254, -0.3422,  ..., -0.5918, -0.4753, -0.3755],\n          [-0.4753, -0.3755, -0.3422,  ..., -0.7915, -0.0760, -0.5252]],\n\n         [[-1.0211, -0.3415, -0.7039,  ..., -1.0815, -0.9002, -0.8398],\n          [-0.7945, -0.7039, -0.5982,  ..., -0.9606, -0.9002, -0.7945],\n          [-0.9002, -0.7492, -0.7039,  ..., -1.0211, -0.4019, -0.7945],\n          ...,\n          [-0.5982, -0.4925, -0.4019,  ..., -0.2358, -0.2660, -0.2660],\n          [-0.3113, -0.4019, -0.4019,  ..., -0.2962, -0.3113, -0.3113],\n          [-0.2660, -0.2962, -0.2962,  ..., -1.5949, -1.1721, -1.1721]]]],\n       device='mps:0'),), <function _create_interpreter_name_lookup_fn.<locals>._get_interpreter_name_for_var at 0x16a181f70>, tensor([[ 5.4698],\n        [-5.9602],\n        [-6.3166],\n        [ 5.8640],\n        [ 5.7413],\n        [-7.4479],\n        [ 2.7846],\n        [ 2.3699],\n        [ 6.3503],\n        [-7.4479],\n        [-7.4479],\n        [-7.4503],\n        [-7.4479],\n        [-7.4380],\n        [-7.4479],\n        [ 2.2253],\n        [ 5.6724],\n        [-7.4479],\n        [-7.4479],\n        [ 5.7514],\n        [-7.4479],\n        [ 6.1727],\n        [ 2.2901],\n        [ 1.6873],\n        [-7.4479],\n        [ 2.5695],\n        [-1.3784],\n        [-5.9167],\n        [ 4.8650],\n        [-7.4479],\n        [-7.4479],\n        [-7.2550],\n        [-7.3076],\n        [ 4.7662],\n        [ 2.0279],\n        [-7.4479],\n        [-6.1667],\n        [-0.0880],\n        [ 5.9941],\n        [-7.4479],\n        [-7.4479],\n        [ 2.8503],\n        [ 5.6612],\n        [ 6.4779],\n        [-7.4479],\n        [-7.0289],\n        [-7.2889],\n        [-7.4451],\n        [-7.2174],\n        [-7.4479],\n        [ 4.8682],\n        [-3.0873],\n        [-7.4479],\n        [-7.4479],\n        [ 6.1519],\n        [-7.4479],\n        [ 5.0508],\n        [ 4.7436],\n        [ 5.8650],\n        [-7.0141],\n        [ 3.7058],\n        [ 1.6129],\n        [-7.4479],\n        [ 4.0441]], device='mps:0'), False, ['rgb', 'cmd', 'spd']"
}