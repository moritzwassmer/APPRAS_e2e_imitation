{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Content\n",
    "This notebook experiments with different file formats (uncompressed and compressed) methods for rgb input (should be the same for lidar) for saving preprocessed data to disk. <br>\n",
    "--> Result: use .npy (uncompressed: big but very fast to load) or .npz (compressed: smaller but slower to load)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/julianvonklitzing/miniforge3/envs/carla/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from dataset import CARLADataset\n",
    "from data_preprocessing import preprocessing\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from zipfile import ZipFile, ZIP_DEFLATED\n",
    "import zipfile\n",
    "import shutil\n",
    "import tarfile\n",
    "from utils import train_test_split, create_metadata_df\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess rgb files and save them as zipped/compressed torch tensors to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compress_file(path_source, path_destination):\n",
    "   path_pwd = os.getcwd()\n",
    "   directory, file_name = os.path.split(path_source)\n",
    "   file_name_zip = file_name + \".zip\"\n",
    "   os.chdir(directory)\n",
    "   with ZipFile(file_name_zip, 'w', ZIP_DEFLATED) as zip:\n",
    "      zip.write(file_name)\n",
    "   os.chdir(path_pwd)\n",
    "   # old_file = os.path.join(directory, file_name_zip)\n",
    "   # shutil.copy(old_file, path_destination)\n",
    "   # os.remove(old_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_file(file_path):\n",
    "    path_pwd = os.getcwd()\n",
    "    directory, file_name = os.path.split(file_path)\n",
    "    file_name_zip = file_name + \".zip\"\n",
    "    os.chdir(directory)\n",
    "\n",
    "\n",
    "    with ZipFile(file_name, 'r') as zip:\n",
    "        #file_name = os.path.splitext(path_source)[0].split(os.sep)[-1]\n",
    "        # zip.extract(file_name, path_destination)\n",
    "        zip.extractall() # path=path_destination\n",
    "    \n",
    "    os.chdir(path_pwd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"../../input/dataset-ege-1/Dataset Ege 1\"\n",
    "\n",
    "path_data = os.path.join(\"..\", \"..\", \"data\", \"data\")\n",
    "\n",
    "config = {\"used_inputs\": [\"rgb\",\"measurements\"], \n",
    "        \"used_measurements\": [\"speed\", \"steer\", \"throttle\", \"brake\", \"command\"],\n",
    "        \"seq_len\": 1\n",
    "        }\n",
    "\n",
    "df_meta_data = create_metadata_df(path_data, config[\"used_inputs\"])\n",
    "\n",
    "dataset = CARLADataset(root_dir=path_data, df_meta_data=df_meta_data, config=config)\n",
    "\n",
    "\n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 217/217 [00:22<00:00,  9.68it/s]\n"
     ]
    }
   ],
   "source": [
    "df_meta = dataset.df_meta_data\n",
    "for idx in tqdm(range(len(df_meta))):\n",
    "    dir_name_zip = os.path.join(df_meta.iloc[idx][\"dir\"], \"rgb_prep_zip\")\n",
    "    if not os.path.exists(dir_name_zip):\n",
    "        os.makedirs(dir_name_zip)\n",
    "    path = os.path.join(df_meta.iloc[idx][0], \"rgb\", df_meta.iloc[idx][1])\n",
    "    img_np = dataset.load_data_from_path(path)\n",
    "    img_torch = torch.Tensor(img_np)\n",
    "    img_torch_prep = preprocessing[\"rgb\"](img_torch)\n",
    "    filename_torch = os.path.join(dir_name_zip, f\"{df_meta.iloc[idx]['rgb'].split('.')[0]}.pt\")\n",
    "    torch.save(img_torch_prep, filename_torch)\n",
    "    compress_file(filename_torch, dir_name_zip)\n",
    "    os.remove(filename_torch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 258866/258866 [47:20<00:00, 91.14it/s] \n"
     ]
    }
   ],
   "source": [
    "# save npy/ npz\n",
    "df_meta = dataset.df_meta_data\n",
    "for idx in tqdm(range(len(df_meta))):\n",
    "    path_parts = dataset.df_meta_data[\"dir\"][idx].split(os.sep)\n",
    "    path_parts[2] += \"_prep\"\n",
    "    dir_name_zip = os.path.join(*path_parts, \"rgb\")\n",
    "    if not os.path.exists(dir_name_zip):\n",
    "        os.makedirs(dir_name_zip)\n",
    "        shutil.copytree(os.path.join(dataset.df_meta_data[\"dir\"][idx], \"measurements\"), os.path.join(*path_parts, \"measurements\"))\n",
    "    path = os.path.join(df_meta.iloc[idx][0], \"rgb\", df_meta.iloc[idx][1])\n",
    "    img_np = dataset.load_data_from_path(path)\n",
    "    img_torch = torch.Tensor(img_np)\n",
    "    img_torch_prep = preprocessing[\"rgb\"](img_torch)\n",
    "    img_np_prep = img_torch_prep.numpy()\n",
    "    filename_np = os.path.join(dir_name_zip, f\"{df_meta.iloc[idx]['rgb'].split('.')[0]}.npy\")\n",
    "    # torch.save(img_torch_prep, filename_torch)\n",
    "    with open(filename_np, 'wb') as f:\n",
    "        np.save(f, img_np)\n",
    "        # np.savez_compressed(f, img_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rgb_in_GB</th>\n",
       "      <th>measurements_in_GB</th>\n",
       "      <th>time_hours</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>55.926342</td>\n",
       "      <td>1.138127</td>\n",
       "      <td>35.953611</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   rgb_in_GB  measurements_in_GB  time_hours\n",
       "0  55.926342            1.138127   35.953611"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.get_statistics()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess rgb files and save them (high storage demand)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 217/217 [00:02<00:00, 91.49it/s]\n"
     ]
    }
   ],
   "source": [
    "df_meta = dataset.df_meta_data\n",
    "for idx in tqdm(range(len(df_meta))):\n",
    "    dir_name_zip = os.path.join(df_meta.iloc[idx][\"dir\"], \"rgb_prep\")\n",
    "    if not os.path.exists(dir_name_zip):\n",
    "        os.makedirs(dir_name_zip)\n",
    "    path = os.path.join(df_meta.iloc[idx][0], \"rgb\", df_meta.iloc[idx][1])\n",
    "    img_np = dataset.load_data_from_path(path)\n",
    "    img_torch = torch.Tensor(img_np)\n",
    "    img_torch_prep = preprocessing[\"rgb\"](img_torch)\n",
    "    filename_torch = os.path.join(dir_name_zip, f\"{df_meta.iloc[idx]['rgb'].split('.')[0]}.pt\")\n",
    "    torch.save(img_torch_prep, filename_torch)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare the approaches (loading speed)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing on the fly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"used_inputs\": [\"rgb\", \"measurements\"], \n",
    "        \"used_measurements\": [\"speed\", \"steer\", \"throttle\"],\n",
    "        \"seq_len\": 1\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = CARLADataset(root_dir=path_data, config=config)\n",
    "dl = DataLoader(dataset=dataset, batch_size=16, num_workers=0, sampler=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 217/217 [00:01<00:00, 171.97it/s]\n"
     ]
    }
   ],
   "source": [
    "df_meta = dataset.df_meta_data\n",
    "for idx in tqdm(range(len(df_meta))):\n",
    "    path = os.path.join(df_meta.iloc[idx][0], \"rgb\", df_meta.iloc[idx][1])\n",
    "    img_np = dataset.load_data_from_path(path)\n",
    "    img_torch = torch.Tensor(img_np)\n",
    "    img_torch_prep = preprocessing[\"rgb\"](img_torch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14/14 [00:01<00:00, 10.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Unfair to use DataLoader (with batches) because for other approaches the Dataset class was not adjusted (time consuming)\n",
    "count = 0\n",
    "for batch in tqdm(dl):\n",
    "    # preprocessing\n",
    "    for key in preprocessing:\n",
    "        batch[key] = preprocessing[key](batch[key])\n",
    "    count += 1\n",
    "print(count)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing from disk and compressed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"used_inputs\": [\"rgb_prep_zip\", \"measurements\"], \n",
    "        \"used_measurements\": [\"speed\", \"steer\", \"throttle\"],\n",
    "        \"seq_len\": 1\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = CARLADataset(root_dir=path_data, config=config)\n",
    "dl = DataLoader(dataset=dataset, batch_size=16, num_workers=0, sampler=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 217/217 [00:02<00:00, 81.08it/s]\n"
     ]
    }
   ],
   "source": [
    "df_meta = dataset.df_meta_data\n",
    "for idx in tqdm(range(len(df_meta))):\n",
    "    path_compressed = os.path.join(df_meta.iloc[idx][0], \"rgb_prep_zip\", df_meta.iloc[idx][1])\n",
    "    path_decompressed = os.path.splitext(path_compressed)[0]\n",
    "    extract_file(path_compressed)\n",
    "    img_torch = torch.load(path_decompressed)\n",
    "    os.remove(path_decompressed)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing from disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 217/217 [00:00<00:00, 685.09it/s]\n"
     ]
    }
   ],
   "source": [
    "config = {\"used_inputs\": [\"rgb_prep\", \"measurements\"], \n",
    "        \"used_measurements\": [\"speed\", \"steer\", \"throttle\"],\n",
    "        \"seq_len\": 1\n",
    "        }\n",
    "\n",
    "dataset = CARLADataset(root_dir=path_data, config=config)\n",
    "dl = DataLoader(dataset=dataset, batch_size=16, num_workers=0)\n",
    "\n",
    "df_meta = dataset.df_meta_data\n",
    "for idx in tqdm(range(len(df_meta))):\n",
    "    path = os.path.join(df_meta.iloc[idx][0], \"rgb_prep\", df_meta.iloc[idx][1])\n",
    "    img_torch = torch.load(path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare winning approaches (npz and npy) according to file size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_stats = dataset.get_statistics() # alt 55.93"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rgb_in_GB</th>\n",
       "      <th>measurements_in_GB</th>\n",
       "      <th>driving_time</th>\n",
       "      <th>%_of_entire_data</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>55.93</td>\n",
       "      <td>1.14</td>\n",
       "      <td>1 day, 11:57:13</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   rgb_in_GB  measurements_in_GB     driving_time  %_of_entire_data\n",
       "0      55.93                1.14  1 day, 11:57:13             100.0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Macs Finder apparently calcs 1GB as 10**9 Bytes --> 55.93 + 1.14 = 57.07\n",
    "ds_stats "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "rgb_entire_size = ds_stats[\"rgb_in_GB\"].item() \n",
    "# rgb_entire_size = 5\n",
    "\n",
    "rgb_entire_size_mb = rgb_entire_size * 1000\n",
    "# rgb_size_frame = rgb_entire_size_mb / len(df_meta_data)\n",
    "npy_size = 0.416\n",
    "npz_size = 0.380\n",
    "predicted_size_npy = npy_size * len(df_meta_data) / 1000\n",
    "predicted_size_npz = npz_size * len(df_meta_data) / 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "107.688256"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_size_npy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outdated functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compress_file(file_path, dest_path):\n",
    "  path_pwd = os.getcwd()\n",
    "  # Extract the directory and file name from the file path\n",
    "  directory, file_name = os.path.split(file_path)\n",
    "  os.chdir(directory)\n",
    "  # Compress the file using shutil.make_archive\n",
    "  shutil.make_archive(file_name, 'zip', directory, file_name) # directory, file_name\n",
    "  os.chdir(path_pwd)\n",
    "  old_file = os.path.join(directory, file_name + '.zip')\n",
    "  # shutil.copy(old_file, dest_path)\n",
    "  # os.remove(old_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_file(file_path):\n",
    "    with ZipFile(path_source, 'r') as zip:\n",
    "        file_name = os.path.splitext(path_source)[0].split(os.sep)[-1]\n",
    "        zip.printdir()\n",
    "        # zip.extract(file_name, path_destination)\n",
    "        zip.extractall() # path=path_destination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_file(file_path):\n",
    "    path_pwd = os.getcwd()\n",
    "    directory, file_name = os.path.split(file_path)\n",
    "    os.chdir(directory)\n",
    "    shutil.unpack_archive(file_name, directory, \"zip\")\n",
    "    os.chdir(path_pwd)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "carla",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "bc80d0638afb8ec7c43f4b834002a598fcddbd6e8bf5db40ad8cba47e68e6a97"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
